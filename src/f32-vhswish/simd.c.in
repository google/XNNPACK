// Copyright 2025 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$BATCH_TILES = tuple(int(bt) for bt in BATCH_TILES.split(","))
$SIMD_SIZE = BATCH_TILES[0]
#include <assert.h>

#include "src/xnnpack/simd/f32-${ARCH}.h"

#include "src/xnnpack/common.h"
#include "src/xnnpack/vunary.h"

$for BATCH_TILE in BATCH_TILES:
  $assert BATCH_TILE % SIMD_SIZE == 0
  $assert BATCH_TILE >= SIMD_SIZE
  $SIMD_TILE = BATCH_TILE // SIMD_SIZE

  void xnn_f32_vhswish_ukernel__${ARCH}_u${BATCH_TILE}(
      size_t batch,
      const float* input,
      float* output,
      const struct xnn_f32_default_params params[restrict XNN_MIN_ELEMENTS(1)])
  {
    assert(batch != 0);
    assert(batch % sizeof(float) == 0);
    assert(input != NULL);
    assert(output != NULL);

    XNN_SIMD_CONST_F32(vsixth, 0x1.555556p-3f);
    XNN_SIMD_CONST_F32(vhalf, 0.5f);
    XNN_SIMD_CONST_F32(vone, 1.0f);
    XNN_SIMD_CONST_F32(vzero, 0.0f);

    $if SIMD_TILE > 1:
      for (; batch >= ${BATCH_TILE} * sizeof(float); batch -= ${BATCH_TILE} * sizeof(float)) {
        $for N in range(SIMD_TILE):
          const xnn_simd_f32_t vx${N} = xnn_loadu_f32(input + ${N} * xnn_simd_size_f32);
        input += ${BATCH_TILE};

        $for N in range(SIMD_TILE):
          xnn_simd_f32_t vacc${N} = xnn_fmadd_f32(vx${N}, vsixth, vhalf);

        $for N in range(SIMD_TILE):
          vacc${N} = xnn_max_f32(vacc${N}, vzero);

        $for N in range(SIMD_TILE):
          vacc${N} = xnn_min_f32(vacc${N}, vone);

        $for N in range(SIMD_TILE):
          vacc${N} = xnn_mul_f32(vacc${N}, vx${N});

        $for N in range(SIMD_TILE):
          xnn_storeu_f32(output + ${N} * xnn_simd_size_f32, vacc${N});
        output += ${BATCH_TILE};
      }
    for (; batch >= xnn_simd_bytes_f32; batch -= xnn_simd_bytes_f32) {
      const xnn_simd_f32_t vx = xnn_loadu_f32(input);
      input += xnn_simd_size_f32;
      xnn_simd_f32_t vacc = xnn_fmadd_f32(vx, vsixth, vhalf);
      vacc = xnn_max_f32(vacc, vzero);
      vacc = xnn_min_f32(vacc, vone);
      vacc = xnn_mul_f32(vacc, vx);
      xnn_storeu_f32(output, vacc);
      output += xnn_simd_size_f32;
    }
    if XNN_UNLIKELY(batch != 0) {
      const xnn_simd_f32_t vx = xnn_load_tail_f32(input, batch >> XNN_LOG2_SIZEOF_FLOAT);
      xnn_simd_f32_t vacc = xnn_fmadd_f32(vx, vsixth, vhalf);
      vacc = xnn_max_f32(vacc, vzero);
      vacc = xnn_min_f32(vacc, vone);
      vacc = xnn_mul_f32(vacc, vx);
      xnn_store_tail_f32(output, vacc, batch >> XNN_LOG2_SIZEOF_FLOAT);
    }
  }
