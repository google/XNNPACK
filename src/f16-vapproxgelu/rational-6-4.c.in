// Copyright 2025 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$ABC = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
$BATCH_TILES = tuple(int(bt) for bt in BATCH_TILES.split(","))
$SIMD_SIZE = BATCH_TILES[0]
#include <assert.h>
#include <stddef.h>

#include "src/xnnpack/simd/f16-${ARCH}.h"

#include "src/xnnpack/common.h"
#include "src/xnnpack/math.h"
#include "src/xnnpack/microparams.h"
#include "src/xnnpack/vunary.h"

$for BATCH_TILE in BATCH_TILES:
  $assert BATCH_TILE % SIMD_SIZE == 0
  $assert BATCH_TILE >= SIMD_SIZE
  $SIMD_TILE = BATCH_TILE // SIMD_SIZE

  void xnn_f16_vapproxgelu_ukernel__${ARCH}_rational_6_4_div_u${BATCH_TILE}(
      size_t batch,
      const xnn_float16* input,
      xnn_float16* output,
      const struct xnn_f16_default_params unused_params[restrict XNN_MIN_ELEMENTS(1)])
  {
    assert(batch != 0);
    assert(batch % sizeof(xnn_float16) == 0);
    assert(input != NULL);
    assert(output != NULL);
    assert(xnn_simd_size_f16 == ${SIMD_SIZE});

    // Cap the inputs to this value as `erf(x/sqrt(2))` will always be `+/-1.0f`
    // beyond this point. This value is chosen as the first floating point
    // number as of which the interpolation returns +/-1.0f.
  #if XNN_SIMD_HAS_NATIVE_FMA
    XNN_SIMD_CONST_F16_FROM_FLOAT(vmax_abs_x, 3.26367188e+00f);
  #else
    XNN_SIMD_CONST_F16_FROM_FLOAT(vmax_abs_x, 3.26953125e+00f);
  #endif  // XNN_SIMD_HAS_NATIVE_FMA

    // The monomial coefficients of the numerator polynomial (odd).
    XNN_SIMD_CONST_F16_FROM_FLOAT(valpha_1, 7.9763704538e-01f);
    XNN_SIMD_CONST_F16_FROM_FLOAT(valpha_3, 1.4811584353e-01f);
    XNN_SIMD_CONST_F16_FROM_FLOAT(valpha_5, 4.3507730588e-03f);

    // The monomial coefficients of the denominator polynomial (even).
    // XNN_SIMD_CONST_F16_FROM_FLOAT(vbeta_0, 1.0e+00f);
    XNN_SIMD_CONST_F16_FROM_FLOAT(vbeta_2, 3.5130375624e-01f);
    XNN_SIMD_CONST_F16_FROM_FLOAT(vbeta_4, 4.0821321309e-02f);

    XNN_SIMD_CONST_F16_FROM_FLOAT(vone, 1.0f);
    XNN_SIMD_CONST_F16_FROM_FLOAT(vhalf, 0.5f);

    $if SIMD_TILE > 1:
      for (; batch >= ${BATCH_TILE} * sizeof(xnn_float16); batch -= ${BATCH_TILE} * sizeof(xnn_float16)) {
        const xnn_simd_f16_t vx_orig_${ABC[0]} = xnn_loadu_f16(input);
        $for N in range(1, SIMD_TILE):
          const xnn_simd_f16_t vx_orig_${ABC[N]} = xnn_loadu_f16(input + ${N} * xnn_simd_size_f16);
        input += ${BATCH_TILE};

        // Clamp the inputs to the interpolation range.
        $for N in range(SIMD_TILE):
          xnn_simd_f16_t vx_${ABC[N]} = xnn_min_f16(vmax_abs_x, vx_orig_${ABC[N]});
        $for N in range(SIMD_TILE):
          vx_${ABC[N]} = xnn_max_f16(xnn_neg_f16(vmax_abs_x), vx_${ABC[N]});

        // Since the polynomials are odd/even, we need x^2.
        $for N in range(SIMD_TILE):
          const xnn_simd_f16_t vx2_${ABC[N]} = xnn_mul_f16(vx_${ABC[N]}, vx_${ABC[N]});

        // Evaluate the numerator polynomial p.
        $for N in range(SIMD_TILE):
          xnn_simd_f16_t vp_${ABC[N]} = xnn_fmadd_f16(vx2_${ABC[N]}, valpha_5, valpha_3);
        $for N in range(SIMD_TILE):
          vp_${ABC[N]} = xnn_fmadd_f16(vx2_${ABC[N]}, vp_${ABC[N]}, valpha_1);
        $for N in range(SIMD_TILE):
          vp_${ABC[N]} = xnn_mul_f16(vx_${ABC[N]}, vp_${ABC[N]});

        // Evaluate the denominator polynomial q.
        $for N in range(SIMD_TILE):
          xnn_simd_f16_t vq_${ABC[N]} = xnn_fmadd_f16(vx2_${ABC[N]}, vbeta_4, vbeta_2);
        $for N in range(SIMD_TILE):
          vq_${ABC[N]} = xnn_fmadd_f16(vx2_${ABC[N]}, vq_${ABC[N]}, vone);

        // Divide the numerator by the denominator.
        $for N in range(SIMD_TILE):
          const xnn_simd_f16_t verf_${ABC[N]} = xnn_div_f16(vp_${ABC[N]}, vq_${ABC[N]});

        // Add one to the rational interpolant, and multiply by 0.5 times the
        // original input.
        $for N in range(SIMD_TILE):
          const xnn_simd_f16_t vy_${ABC[N]} = xnn_mul_f16(xnn_mul_f16(vx_orig_${ABC[N]}, vhalf),
                                              xnn_add_f16(verf_${ABC[N]}, vone));

        xnn_storeu_f16(output, vy_${ABC[0]});
        $for N in range(1, SIMD_TILE):
          xnn_storeu_f16(output + ${N} * xnn_simd_size_f16, vy_${ABC[N]});
        output += ${BATCH_TILE};
      }
    for (; batch >= xnn_simd_bytes_f16; batch -= xnn_simd_bytes_f16) {
      const xnn_simd_f16_t vx_orig = xnn_loadu_f16(input);
      input += xnn_simd_size_f16;

      // Clamp the inputs to the interpolation range.
      xnn_simd_f16_t vx = xnn_min_f16(vmax_abs_x, vx_orig);
      vx = xnn_max_f16(xnn_neg_f16(vmax_abs_x), vx);

      // Since the polynomials are odd/even, we need x^2.
      const xnn_simd_f16_t vx2 = xnn_mul_f16(vx, vx);

      // Evaluate the numerator polynomial p.
      xnn_simd_f16_t vp = xnn_fmadd_f16(vx2, valpha_5, valpha_3);
      vp = xnn_fmadd_f16(vx2, vp, valpha_1);
      vp = xnn_mul_f16(vx, vp);

      // Evaluate the denominator polynomial q.
      xnn_simd_f16_t vq = xnn_fmadd_f16(vx2, vbeta_4, vbeta_2);
      vq = xnn_fmadd_f16(vx2, vq, vone);

      // Divide the numerator by the denominator and add one
      const xnn_simd_f16_t verf =  xnn_div_f16(vp, vq);

      // Add one to the rational interpolant, and multiply by 0.5 times the
      // original input.
      const xnn_simd_f16_t vy = xnn_mul_f16(xnn_mul_f16(vx_orig, vhalf),
                                            xnn_add_f16(verf, vone));

      xnn_storeu_f16(output, vy);
      output += xnn_simd_size_f16;
    }
    $if SIMD_SIZE > 1:
      if XNN_UNLIKELY(batch != 0) {
        xnn_simd_f16_t vx_orig = xnn_load_tail_f16(input, batch >> XNN_LOG2_SIZEOF_HALF);

      // See above for comments.
      xnn_simd_f16_t vx = xnn_min_f16(vmax_abs_x, vx_orig);
      vx = xnn_max_f16(xnn_neg_f16(vmax_abs_x), vx);
      const xnn_simd_f16_t vx2 = xnn_mul_f16(vx, vx);
      xnn_simd_f16_t vp = xnn_fmadd_f16(vx2, valpha_5, valpha_3);
      vp = xnn_fmadd_f16(vx2, vp, valpha_1);
      vp = xnn_mul_f16(vx, vp);
      xnn_simd_f16_t vq = xnn_fmadd_f16(vx2, vbeta_4, vbeta_2);
      vq = xnn_fmadd_f16(vx2, vq, vone);
      const xnn_simd_f16_t verf =  xnn_div_f16(vp, vq);
      const xnn_simd_f16_t vy = xnn_mul_f16(xnn_mul_f16(vx_orig, vhalf),
                                            xnn_add_f16(verf, vone));

        xnn_store_tail_f16(output, vy, batch >> XNN_LOG2_SIZEOF_HALF);
      }
  }
