// Copyright 2025 RVVPL and Lobachevsky State University of Nizhny Novgorod
// Code adapted from https://github.com/rvvpl/rvvmf
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert LMUL in [1, 2, 4, 8]
#include <assert.h>
#include <math.h>

#include <riscv_vector.h>

#include "src/xnnpack/common.h"
#include "src/xnnpack/intrinsics-polyfill.h"
#include "src/xnnpack/vunary.h"


#include <stdint.h>
#include <float.h>
#include <math.h>

/* ---------- UTILS ---------- */

#ifndef forceinline 
    #if defined(__clang__) || defined(__GNUC__) || defined(__GNUG__)
        #define forceinline __attribute__((always_inline)) inline
    #else
        #define forceinline inline
    #endif
#endif

#define RVVMF_EXP_AS_FP32(x) (*(float*)(&(x)))

forceinline vfloat32m${LMUL}_t calc_polynom_deg_2_f32m${LMUL}(vfloat32m${LMUL}_t x, float a0, float a1, float a2, size_t vl)
{ 
    return __riscv_vfmadd_vv_f32m${LMUL}(x, __riscv_vfmadd_vf_f32m${LMUL}(x, a2,
        __riscv_vfmv_v_f_f32m${LMUL}(a1, vl), vl), __riscv_vfmv_v_f_f32m${LMUL}(a0, vl), vl);
}

forceinline void fast_2_sum_vv_f32m${LMUL}(vfloat32m${LMUL}_t a, vfloat32m${LMUL}_t b, vfloat32m${LMUL}_t* sh, vfloat32m${LMUL}_t* sl, size_t vl)
{
    *sh = __riscv_vfadd_vv_f32m${LMUL}(a, b, vl);
    *sl = __riscv_vfsub_vv_f32m${LMUL}(b, __riscv_vfsub_vv_f32m${LMUL}(*sh, a, vl), vl);
}
    
forceinline void fast_2_sum_fv_f32m${LMUL}(float a, vfloat32m${LMUL}_t b, vfloat32m${LMUL}_t* sh, vfloat32m${LMUL}_t* sl, size_t vl)
{
    *sh = __riscv_vfadd_vf_f32m${LMUL}(b, a, vl);
    *sl = __riscv_vfsub_vv_f32m${LMUL}(b, __riscv_vfsub_vf_f32m${LMUL}(*sh, a, vl), vl);
}
        
forceinline void mul21_vv_f32m${LMUL}(vfloat32m${LMUL}_t ah, vfloat32m${LMUL}_t al, vfloat32m${LMUL}_t bh, vfloat32m${LMUL}_t bl, vfloat32m${LMUL}_t* rh, size_t vl)
{
    vfloat32m${LMUL}_t zh, zl;
    zh = __riscv_vfmul_vv_f32m${LMUL}(ah, bh, vl);
    zl = __riscv_vfmsub_vv_f32m${LMUL}(ah, bh, zh, vl);
    zl = __riscv_vfadd_vv_f32m${LMUL}(zl, __riscv_vfmadd_vv_f32m${LMUL}(ah, bl, __riscv_vfmul_vv_f32m${LMUL}(al, bh, vl), vl), vl);
    *rh = __riscv_vfadd_vv_f32m${LMUL}(zh, zl, vl);
}

forceinline void fma12_vv_f32m${LMUL}(vfloat32m${LMUL}_t ah, vfloat32m${LMUL}_t bh, vfloat32m${LMUL}_t ch, vfloat32m${LMUL}_t* zh, vfloat32m${LMUL}_t* zl, size_t vl)
{ 
    *zh = __riscv_vfmadd_vv_f32m${LMUL}(ah, bh, ch, vl);
    *zl = __riscv_vfmadd_vv_f32m${LMUL}(ah, bh, __riscv_vfsub_vv_f32m${LMUL}(ch, *zh, vl), vl);
}

forceinline void fma12_vf_f32m${LMUL}(vfloat32m${LMUL}_t ah, float bh, vfloat32m${LMUL}_t ch, vfloat32m${LMUL}_t* zh, vfloat32m${LMUL}_t* zl, size_t vl)
{ 
    *zh = __riscv_vfmadd_vf_f32m${LMUL}(ah, bh, ch, vl);
    *zl = __riscv_vfmadd_vf_f32m${LMUL}(ah, bh, __riscv_vfsub_vv_f32m${LMUL}(ch, *zh, vl), vl);
}

/* ---------- DATA ---------- */
#if ${LMUL} == 1

uint32_t ZERO_F32 = 0x0;        // 0.0f
uint32_t ONE_F32  = 0x3f800000;  // 1.0f

uint32_t EXP_EXPM1_OVERFLOW_THRESHOLD_F32 = 0x42b17217;  // 0x1.62e42ep6f
uint32_t EXP_SUBNORMAL_THRESHOLD_F32      = 0xc2aeac4f;       // -0x1.5d589ep6f
uint32_t EXP_ZERO_THRESHOLD_F32           = 0xc2cff1b4;            // -0x1.9fe368p6f
uint32_t EXP_UNDERFLOW_VALUE_F32          = 0x0;                  // 0.0f

size_t  TABLE_SIZE_DEG_F32 = 4;
uint32_t MASK_FI_BIT_F32 = 0x0000000f;
uint32_t MASK_HI_BIT_F32 = 0x00001fff;
uint32_t MAGIC_CONST_1_F32 = 0x4b400000;  // 12582912.0f
uint32_t INV_LOG2_2K_F32 = 0x41b8aa3b;    // 0x1.715476p4f
uint32_t M_LOG2_2K_H_F32 = 0xbd317000;    // -0x1.62ep-5f
uint32_t M_LOG2_2K_L_F32 = 0xb605fdf4;    // -0x1.0bfbe8p-19f
uint32_t M_LOG2_2K_LL_F32 = 0xa9e7bcd6;   // -0x1.cf79acp-44f

uint32_t RVVMF_EXP_LOOK_UP_TABLE_HIGH_F32[16] = {
    0x3f800000, 0x3f85aac3, 0x3f8b95c2, 0x3f91c3d3,
    0x3f9837f0, 0x3f9ef532, 0x3fa5fed7, 0x3fad583f,
    0x3fb504f3, 0x3fbd08a4, 0x3fc5672a, 0x3fce248c,
    0x3fd744fd, 0x3fe0ccdf, 0x3feac0c7, 0x3ff5257d
};
uint32_t RVVMF_EXP_LOOK_UP_TABLE_LOW_F32[16] = {
    0x0, 0x334f9891, 0xb260aba1, 0x33675624,
    0x33231b71, 0x33412342, 0xb${int(32/LMUL)}c9d5e, 0xb22deaf6,
    0x32cfe77a, 0xb3414fe8, 0x320aa837, 0x3228fc24,
    0xb2d4a58a, 0xb21eab59, 0xb24116de, 0x32292436
};

uint32_t EXP_POL_COEFF_2_F32 = 0x3f000000;  // 0x1p-1f
uint32_t EXP_POL_COEFF_3_F32 = 0x3e2aab6f;  // 0x1.5556dep-3f
uint32_t EXP_POL_COEFF_4_F32 = 0x3d2aab4b;  // 0x1.555696p-5f
#else

extern uint32_t ZERO_F32;
extern uint32_t ONE_F32 ;
extern uint32_t EXP_EXPM1_OVERFLOW_THRESHOLD_F32;
extern uint32_t EXP_SUBNORMAL_THRESHOLD_F32     ;
extern uint32_t EXP_ZERO_THRESHOLD_F32          ;
extern uint32_t EXP_UNDERFLOW_VALUE_F32         ;

extern size_t  TABLE_SIZE_DEG_F32;
extern uint32_t MASK_FI_BIT_F32;
extern uint32_t MASK_HI_BIT_F32;
extern uint32_t MAGIC_CONST_1_F32;  // 12582912.0f
extern uint32_t INV_LOG2_2K_F32;    // 0x1.715476p4f
extern uint32_t M_LOG2_2K_H_F32;    // -0x1.62ep-5f
extern uint32_t M_LOG2_2K_L_F32;    // -0x1.0bfbe8p-19f
extern uint32_t M_LOG2_2K_LL_F32;   // -0x1.cf79acp-44f

extern uint32_t RVVMF_EXP_LOOK_UP_TABLE_HIGH_F32[16];
extern  uint32_t RVVMF_EXP_LOOK_UP_TABLE_LOW_F32[16];

extern uint32_t EXP_POL_COEFF_2_F32;  // 0x1p-1f
extern uint32_t EXP_POL_COEFF_3_F32;  // 0x1.5556dep-3f
extern uint32_t EXP_POL_COEFF_4_F32;  // 0x1.555696p-5f


#endif

/* ---------- EXP IMPLEMENTATION ---------- */

forceinline void check_special_cases_f32m${LMUL}(vfloat32m${LMUL}_t* x, vfloat32m${LMUL}_t* special, vbool${int(32/LMUL)}_t* specialMask,
    float overflowThreshold, size_t vl)
{
    uint32_t pinf = 0x7f800000;    
    vbool${int(32/LMUL)}_t mask;
    // check +inf
    *specialMask = __riscv_vmfeq_vf_f32m${LMUL}_b${int(32/LMUL)}(*x, RVVMF_EXP_AS_FP32(pinf), vl);
    *special = __riscv_vfmerge_vfm_f32m${LMUL}(*x, RVVMF_EXP_AS_FP32(pinf), *specialMask, vl);
    // check overflow
    mask = __riscv_vmand_mm_b${int(32/LMUL)}(__riscv_vmfgt_vf_f32m${LMUL}_b${int(32/LMUL)}(*x, overflowThreshold, vl),
        __riscv_vmflt_vf_f32m${LMUL}_b${int(32/LMUL)}(*x, RVVMF_EXP_AS_FP32(pinf), vl), vl);
    *special = __riscv_vfmerge_vfm_f32m${LMUL}(*special, RVVMF_EXP_AS_FP32(pinf), mask, vl);
    *specialMask = __riscv_vmor_mm_b${int(32/LMUL)}(*specialMask, mask, vl);  
    /* if (__riscv_vcpop_m_b${int(32/LMUL)}(mask, vl))
        volatile double exception = DBL_MAX*2.0; */  // FE_OVERFLOW
    
    // NaNs, overflow, -inf -- automatically
    *x = __riscv_vfmerge_vfm_f32m${LMUL}(*x, RVVMF_EXP_AS_FP32(ZERO_F32), *specialMask, vl);
}

forceinline void do_exp_argument_reduction_hl_f32m${LMUL}(vfloat32m${LMUL}_t x,
    vfloat32m${LMUL}_t* yh, vfloat32m${LMUL}_t* yl, vuint32m${LMUL}_t* ei, vuint32m${LMUL}_t* fi, size_t vl)
{
    vfloat32m${LMUL}_t vmagicConst1 = __riscv_vfmv_v_f_f32m${LMUL}(RVVMF_EXP_AS_FP32(MAGIC_CONST_1_F32), vl);
    vfloat32m${LMUL}_t h = __riscv_vfmadd_vf_f32m${LMUL}(x, RVVMF_EXP_AS_FP32(INV_LOG2_2K_F32), vmagicConst1, vl);
    vuint32m${LMUL}_t hi = __riscv_vand_vx_u32m${LMUL}(__riscv_vreinterpret_v_f32m${LMUL}_u32m${LMUL}(h), MASK_HI_BIT_F32, vl);
    *fi = __riscv_vand_vx_u32m${LMUL}(hi, MASK_FI_BIT_F32, vl);
    *ei = __riscv_vsrl_vx_u32m${LMUL}(hi, TABLE_SIZE_DEG_F32, vl);
    h = __riscv_vfsub_vv_f32m${LMUL}(h, vmagicConst1, vl);
    fma12_vf_f32m${LMUL}(h, RVVMF_EXP_AS_FP32(M_LOG2_2K_L_F32),
        __riscv_vfmadd_vf_f32m${LMUL}(h, RVVMF_EXP_AS_FP32(M_LOG2_2K_H_F32), x, vl), yh, yl, vl);
    *yl = __riscv_vfmadd_vf_f32m${LMUL}(h, RVVMF_EXP_AS_FP32(M_LOG2_2K_LL_F32), *yl, vl);
    fast_2_sum_vv_f32m${LMUL}(*yh, *yl, yh, yl, vl);
}

forceinline void get_table_values_hl_f32m${LMUL}(
    vuint32m${LMUL}_t* index, vfloat32m${LMUL}_t* th, vfloat32m${LMUL}_t* tl, size_t vl)
{
    *index = __riscv_vmul_vx_u32m${LMUL}(*index, (uint32_t)sizeof(float), vl);
    *th = __riscv_vloxei32_v_f32m${LMUL}((float*)RVVMF_EXP_LOOK_UP_TABLE_HIGH_F32, *index, vl);
    *tl = __riscv_vloxei32_v_f32m${LMUL}((float*)RVVMF_EXP_LOOK_UP_TABLE_LOW_F32, *index, vl);
}

forceinline void calculate_exp_polynom_hl_f32m${LMUL}(vfloat32m${LMUL}_t yh, vfloat32m${LMUL}_t yl, vfloat32m${LMUL}_t* ph, vfloat32m${LMUL}_t* pl, size_t vl)
{
    vfloat32m${LMUL}_t sqryh = __riscv_vfmul_vv_f32m${LMUL}(yh, yh, vl);
    vfloat32m${LMUL}_t r = calc_polynom_deg_2_f32m${LMUL}(yh, RVVMF_EXP_AS_FP32(EXP_POL_COEFF_2_F32),
        RVVMF_EXP_AS_FP32(EXP_POL_COEFF_3_F32), RVVMF_EXP_AS_FP32(EXP_POL_COEFF_4_F32), vl);
    fma12_vv_f32m${LMUL}(sqryh, r, yh, ph, pl, vl);
    *pl = __riscv_vfadd_vv_f32m${LMUL}(*pl, yl, vl);
}

forceinline void update_exponent_f32m${LMUL}(vuint32m${LMUL}_t ei, vfloat32m${LMUL}_t* res, size_t vl)
{
    *res = __riscv_vreinterpret_v_u32m${LMUL}_f32m${LMUL}(__riscv_vadd_vv_u32m${LMUL}(
        __riscv_vreinterpret_v_f32m${LMUL}_u32m${LMUL}(*res), __riscv_vsll_vx_u32m${LMUL}(ei, (size_t)23, vl), vl));
}

forceinline void update_exponent_with_subnormal_f32m${LMUL}(float subnormalThreshold, vfloat32m${LMUL}_t x,
    vuint32m${LMUL}_t ei, vfloat32m${LMUL}_t* res, size_t vl)
{
#ifndef __FAST_MATH__
    uint32_t ninf = 0xff800000;
#if ${LMUL} == 1
    vbool${int(32/LMUL)}_t subnormalMask = __riscv_vmand_mm_b${int(32/LMUL)}(__riscv_vmfgt_vf_f32m1_b${int(32/LMUL)}(x, RVVMF_EXP_AS_FP32(ninf), vl),
        __riscv_vmflt_vf_f32m${LMUL}_b${int(32/LMUL)}(x, subnormalThreshold, vl), vl);
#endif
#if ${LMUL} == 2
    vbool${int(32/LMUL)}_t subnormalMask = __riscv_vmand_mm_b16(__riscv_vmfgt_vf_f32m2_b16(x, RVVMF_EXP_AS_FP32(ninf), vl),
        __riscv_vmflt_vf_f32m${LMUL}_b16(x, subnormalThreshold, vl), vl);
#endif
#if ${LMUL} == 4
    vbool${int(32/LMUL)}_t subnormalMask = __riscv_vmand_mm_b8(__riscv_vmfgt_vf_f32m4_b8(x, RVVMF_EXP_AS_FP32(ninf), vl),
        __riscv_vmflt_vf_f32m4_b8(x, subnormalThreshold, vl), vl);
#endif
#if ${LMUL} == 8
    vbool${int(32/LMUL)}_t subnormalMask = __riscv_vmand_mm_b4(__riscv_vmfgt_vf_f32m8_b4(x, RVVMF_EXP_AS_FP32(ninf), vl),
        __riscv_vmflt_vf_f32m8_b4(x, subnormalThreshold, vl), vl);
#endif
    /* if (__riscv_vcpop_m_b${int(32/LMUL)}(subnormalMask, vl))
        volatile double exception = nextafter(DBL_MIN/(double((uint64_t)1 << 52)), 0.0) */ // FE_UNDERFLOW
    
    vfloat32m${LMUL}_t subnormalRes;
    vuint32m${LMUL}_t shiftNum = __riscv_vreinterpret_v_i32m${LMUL}_u32m${LMUL}(__riscv_vneg_v_i32m${LMUL}(__riscv_vreinterpret_v_u32m${LMUL}_i32m${LMUL}(ei), vl));
    shiftNum = __riscv_vadd_vx_u32m${LMUL}(__riscv_vand_vx_u32m${LMUL}(shiftNum, (uint32_t)0x000001ff, vl), (uint32_t)1, vl);
    shiftNum = __riscv_vsll_vx_u32m${LMUL}(shiftNum, (size_t)23, vl);
    subnormalRes = __riscv_vfadd_vv_f32m${LMUL}(*res, __riscv_vreinterpret_v_u32m${LMUL}_f32m${LMUL}(shiftNum), vl);
    subnormalRes = __riscv_vreinterpret_v_u32m${LMUL}_f32m${LMUL}(__riscv_vand_vx_u32m${LMUL}(
        __riscv_vreinterpret_v_f32m${LMUL}_u32m${LMUL}(subnormalRes), (uint32_t)0x807fffff, vl));
#endif

    update_exponent_f32m${LMUL}(ei, res, vl);
    
#ifndef __FAST_MATH__
    *res = __riscv_vmerge_vvm_f32m${LMUL}(*res, subnormalRes, subnormalMask, vl);
#endif   
}

forceinline void reconstruct_exp_hl_hl_f32m${LMUL}(vfloat32m${LMUL}_t x, vuint32m${LMUL}_t ei, vfloat32m${LMUL}_t th, vfloat32m${LMUL}_t tl,
    vfloat32m${LMUL}_t pm${LMUL}h, vfloat32m${LMUL}_t pm${LMUL}l, vfloat32m${LMUL}_t* res, float subnormalThreshold, size_t vl)
{
    vfloat32m${LMUL}_t sh, sl;
    fast_2_sum_fv_f32m${LMUL}(RVVMF_EXP_AS_FP32(ONE_F32), pm${LMUL}h, &sh, &sl, vl);
    sl = __riscv_vfadd_vv_f32m${LMUL}(sl, pm${LMUL}l, vl);
    mul21_vv_f32m${LMUL}(th, tl, sh, sl, res, vl);
    update_exponent_with_subnormal_f32m${LMUL}(subnormalThreshold, x, ei, res, vl);
}

forceinline void update_underflow_f32m${LMUL}(vfloat32m${LMUL}_t x, vfloat32m${LMUL}_t* res,
    float underflowThreshold, float underflowValue, size_t vl)
{
    vbool${int(32/LMUL)}_t underflowMask = __riscv_vmflt_vf_f32m${LMUL}_b${int(32/LMUL)}(x, underflowThreshold, vl);
    *res = __riscv_vfmerge_vfm_f32m${LMUL}(*res, underflowValue, underflowMask, vl);
}

vfloat32m${LMUL}_t __riscv_vexp_f32m${LMUL}(vfloat32m${LMUL}_t x, size_t avl)
{
    size_t vl = __riscv_vsetvl_e32m${LMUL}(avl);

    vfloat32m${LMUL}_t res, yh, yl, th, tl, pm${LMUL}h, pm${LMUL}l;
    vuint32m${LMUL}_t ei, fi;
    
#ifndef __FAST_MATH__
    float zeroThreshold = RVVMF_EXP_AS_FP32(EXP_ZERO_THRESHOLD_F32);
    vfloat32m${LMUL}_t special;
    vbool${int(32/LMUL)}_t specialMask;
    check_special_cases_f32m${LMUL}(&x, &special, &specialMask, RVVMF_EXP_AS_FP32(EXP_EXPM1_OVERFLOW_THRESHOLD_F32), vl);
#else
    float zeroThreshold = RVVMF_EXP_AS_FP32(EXP_SUBNORMAL_THRESHOLD_F32);    
#endif
    
    do_exp_argument_reduction_hl_f32m${LMUL}(x, &yh, &yl, &ei, &fi, vl);
    get_table_values_hl_f32m${LMUL}(&fi, &th, &tl, vl);
    calculate_exp_polynom_hl_f32m${LMUL}(yh, yl, &pm${LMUL}h, &pm${LMUL}l, vl);
    reconstruct_exp_hl_hl_f32m${LMUL}(x, ei, th, tl, pm${LMUL}h, pm${LMUL}l, &res, RVVMF_EXP_AS_FP32(EXP_SUBNORMAL_THRESHOLD_F32), vl);
    update_underflow_f32m${LMUL}(x, &res, zeroThreshold, RVVMF_EXP_AS_FP32(EXP_UNDERFLOW_VALUE_F32), vl);

#ifndef __FAST_MATH__
    res = __riscv_vmerge_vvm_f32m${LMUL}(res, special, specialMask, vl);
#endif

    return res;
}


void xnn_f32_vexp_ukernel__rvv_exp_u${LMUL}v(
    size_t batch,
    const float* input,
    float* output,
    const struct xnn_f32_default_params* params)
{
  assert(batch != 0);
  assert(batch % sizeof(float) == 0);
  assert(input != NULL);
  assert(output != NULL);

  batch >>= XNN_LOG2_SIZEOF_FLOAT;
  do {
    const size_t n = __riscv_vsetvl_e32m${LMUL}(batch);
    vfloat32m${LMUL}_t vx = __riscv_vle32_v_f32m${LMUL}(input, n);
    input += n;
    vfloat32m${LMUL}_t vacc = __riscv_vexp_f32m${LMUL}(vx, n);
    __riscv_vse32_v_f32m${LMUL}(output, vacc, n);
    output += n;

    batch -= n;
  } while (batch != 0);
}
