// Copyright (C) 2024 Intel Corporation
//  
// Redistribution and use in source and binary forms, with or without modification,
// are permitted provided that the following conditions are met:
//  
// 1. Redistributions of source code must retain the above copyright notice,
//    this list of conditions and the following disclaimer.
// 2. Redistributions in binary form must reproduce the above copyright notice,
//    this list of conditions and the following disclaimer in the documentation
//    and/or other materials provided with the distribution.
// 3. Neither the name of the copyright holder nor the names of its contributors
//    may be used to endorse or promote products derived from this software
//    without specific prior written permission.
//  
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
// ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS
// BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,
// OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
// OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
// OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
// WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
// OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
// EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
//  
//  
// SPDX-License-Identifier: BSD-3-Clause

$assert DATATYPE in ["QS8", "QU8"]
$assert BATCH_TILE >= 8
$assert BATCH_TILE == 8 or BATCH_TILE % 16 == 0
$SIMD_TILE = BATCH_TILE // 16

#include <assert.h>
#include <immintrin.h>
#include <emmintrin.h>
#include "src/xnnpack/intrinsics-polyfill.h"
#include "src/xnnpack/math.h"
#include "src/xnnpack/vbinary.h"

$XINT8_T = {"QS8": "int8_t", "QU8": "uint8_t"}[DATATYPE]
$_MM256_CVTEPX8_EPI32 = {"QS8": "_mm256_cvtepi8_epi32", "QU8": "_mm256_cvtepu8_epi32"}[DATATYPE]
$_MM_PACKXS_EPI16 = {"QS8": "_mm_packs_epi16", "QU8": "_mm_packus_epi16"}[DATATYPE]
void xnn_${DATATYPE.lower()}_vprelu_ukernel__avx2_u${BATCH_TILE}(
    size_t batch,
    const ${XINT8_T}* input_a,
    const ${XINT8_T}* input_b,
    ${XINT8_T}* output,
    const union xnn_qs8_vprelu_scalar_params* restrict params) XNN_OOB_READS
{
  assert(batch != 0);
  assert(batch % sizeof(${XINT8_T}) == 0);
  assert(input_a != NULL);
  assert(input_b != NULL);
  assert(output != NULL);

  const __m256i vinput_zero_point = _mm256_set1_epi32(params->scalar.input_zero_point);
  const __m256i vslope_zero_point = _mm256_set1_epi32(params->scalar.slope_zero_point);
  const __m256 vpositive_multiplier = _mm256_set1_ps(params->scalar.positive_multiplier);
  const __m256 vnegative_multiplier = _mm256_set1_ps(params->scalar.negative_multiplier);
  const __m256 voutput_min_less_zero_point = _mm256_set1_ps((int32_t) params->scalar.output_min - (int32_t) params->scalar.output_zero_point);
  const __m256 voutput_max_less_zero_point = _mm256_set1_ps((int32_t) params->scalar.output_max - (int32_t) params->scalar.output_zero_point);
  const __m256 vmagic_bias = _mm256_set1_ps(12582912.0f);
  const __m256i vmagic_bias_less_output_zero_point = _mm256_set1_epi32(INT32_C(0x4B400000) - (int32_t)params->scalar.output_zero_point);

  $if BATCH_TILE > 8:
    for (; batch >= ${BATCH_TILE} * sizeof(${XINT8_T}); batch -= ${BATCH_TILE} * sizeof(${XINT8_T})) {
      __m256i va0 = ${_MM256_CVTEPX8_EPI32}(_mm_loadu_si64((const __m128i*) input_a));
      __m256i vb0 = ${_MM256_CVTEPX8_EPI32}(_mm_loadu_si64((const __m128i*) input_b));
      
      $for N in range(1, 2*SIMD_TILE):
        __m256i va${N} = ${_MM256_CVTEPX8_EPI32}(_mm_loadu_si64((const __m128i*) (input_a + ${N * 8})));
        __m256i vb${N} = ${_MM256_CVTEPX8_EPI32}(_mm_loadu_si64((const __m128i*) (input_b + ${N * 8}))); 
      input_a += ${BATCH_TILE};
      input_b += ${BATCH_TILE};

      $for N in range(2*SIMD_TILE):
        __m256i va${N}_sub = _mm256_sub_epi32(va${N}, vinput_zero_point);
        __m256i vb${N}_sub = _mm256_sub_epi32(vb${N}, vslope_zero_point);
        __m256i vcompare${N} = _mm256_cmpgt_epi32(_mm256_setzero_si256(), va${N}_sub);
        __m256i vacc${N} = _mm256_blendv_epi8(va${N}_sub, _mm256_mullo_epi32(va${N}_sub, vb${N}_sub), vcompare${N});

      $for N in range(2*SIMD_TILE):
        __m256 vscale${N} = _mm256_blendv_ps(vpositive_multiplier, vnegative_multiplier, _mm256_castsi256_ps(vcompare${N}));
        __m256 vfpacc${N} = _mm256_mul_ps(_mm256_cvtepi32_ps(vacc${N}), vscale${N});

      $for N in range(2*SIMD_TILE):
        __m256 vfpacc_clamped${N} = _mm256_min_ps(_mm256_max_ps(vfpacc${N}, voutput_min_less_zero_point), voutput_max_less_zero_point);
        __m256 vfpacc_biased${N} = _mm256_add_ps(vfpacc_clamped${N}, vmagic_bias);
        __m256i vout${N} = _mm256_sub_epi32(_mm256_castps_si256(vfpacc_biased${N}), vmagic_bias_less_output_zero_point);

      $for N in range(2*SIMD_TILE):  
        const __m128i vout_low${N} = _mm256_castsi256_si128(vout${N});
        const __m128i vout_high${N} = _mm256_extracti128_si256(vout${N}, 1);
        const __m128i vout_packed16${N} = _mm_packs_epi32(vout_low${N}, vout_high${N});
        __m128i vout_final${N} = ${_MM_PACKXS_EPI16}(vout_packed16${N}, vout_packed16${N});

      _mm_storeu_si64((__m128i*)(output), vout_final0);

      $for N in range(1, 2*SIMD_TILE):
        _mm_storeu_si64((__m128i*)(output + ${N*8}), vout_final${N});

      output += ${BATCH_TILE};
    }

  for (; batch >= 8 * sizeof(${XINT8_T}); batch -= 8 * sizeof(${XINT8_T})) {
    __m256i va = ${_MM256_CVTEPX8_EPI32}(_mm_loadu_si64((const __m128i*) input_a));
    __m256i vb = ${_MM256_CVTEPX8_EPI32}(_mm_loadu_si64((const __m128i*) input_b));
    __m256i va_sub = _mm256_sub_epi32(va, vinput_zero_point);
    __m256i vb_sub = _mm256_sub_epi32(vb, vslope_zero_point);
    __m256i vcompare = _mm256_cmpgt_epi32(_mm256_setzero_si256(), va_sub);
    __m256i vacc = _mm256_blendv_epi8(va_sub, _mm256_mullo_epi32(va_sub, vb_sub), vcompare);
    __m256 vscale = _mm256_blendv_ps(vpositive_multiplier, vnegative_multiplier, _mm256_castsi256_ps(vcompare));
    __m256 vfpacc = _mm256_mul_ps(_mm256_cvtepi32_ps(vacc), vscale);
    __m256 vfpacc_clamped = _mm256_min_ps(_mm256_max_ps(vfpacc, voutput_min_less_zero_point), voutput_max_less_zero_point);
    __m256 vfpacc_biased = _mm256_add_ps(vfpacc_clamped, vmagic_bias);
    __m256i vout = _mm256_sub_epi32(_mm256_castps_si256(vfpacc_biased), vmagic_bias_less_output_zero_point);
    input_a+=8;
    input_b+=8;
    const __m128i vout_low = _mm256_castsi256_si128(vout);
    const __m128i vout_high = _mm256_extracti128_si256(vout, 1);
    const __m128i vout_packed16 = _mm_packs_epi32(vout_low, vout_high);
    __m128i vout_final = ${_MM_PACKXS_EPI16}(vout_packed16, vout_packed16);
    _mm_storeu_si64((__m128i*) output, vout_final);
    output+=8;
  }

  if XNN_UNLIKELY(batch != 0) {
    assert(batch >= 1 * sizeof(${XINT8_T}));
    assert(batch <= 7 * sizeof(${XINT8_T}));

    const __m256i va = ${_MM256_CVTEPX8_EPI32}(_mm_loadu_si128((const __m128i*) input_a));
    const __m256i vb = ${_MM256_CVTEPX8_EPI32}(_mm_loadu_si128((const __m128i*) input_b));
    const __m256i va_sub = _mm256_sub_epi32(va, vinput_zero_point);
    const __m256i vb_sub = _mm256_sub_epi32(vb, vslope_zero_point);
    const __m256i vcompare = _mm256_cmpgt_epi32(_mm256_setzero_si256(), va_sub);
    const __m256i vacc = _mm256_blendv_epi8(va_sub, _mm256_mullo_epi32(va_sub, vb_sub), vcompare);
    const __m256 vscale = _mm256_blendv_ps(vpositive_multiplier, vnegative_multiplier, _mm256_castsi256_ps(vcompare));
    const __m256 vfpacc = _mm256_mul_ps(_mm256_cvtepi32_ps(vacc), vscale);
    const __m256 vfpacc_clamped = _mm256_min_ps(_mm256_max_ps(vfpacc, voutput_min_less_zero_point), voutput_max_less_zero_point);
    const __m256 vfpacc_biased = _mm256_add_ps(vfpacc_clamped, vmagic_bias);
    const __m256i vout = _mm256_sub_epi32(_mm256_castps_si256(vfpacc_biased), vmagic_bias_less_output_zero_point);
    const __m128i vout_low = _mm256_castsi256_si128(vout);
    const __m128i vout_high = _mm256_extracti128_si256(vout, 1);
    const __m128i vout_packed16 = _mm_packs_epi32(vout_low, vout_high);
    __m128i vout_final = ${_MM_PACKXS_EPI16}(vout_packed16, vout_packed16);
   
    if (batch & (4 * sizeof(${XINT8_T}))) {
      _mm_storeu_si32(output, vout_final);
      vout_final = _mm_srli_epi64(vout_final, 32);
      output += 4;
    }

    if (batch & (2 * sizeof(${XINT8_T}))) {
     _mm_storeu_si16(output, vout_final);
      vout_final = _mm_srli_epi32(vout_final, 16);
      output += 2;
    }
    if (batch & (1 * sizeof(${XINT8_T}))) {
      *output = (${XINT8_T}) _mm_extract_epi8(vout_final, 0);
    }
  }
}
