// Copyright 2025 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert(SIMD_SIZE > 0)
#include <assert.h>

#include <immintrin.h>

#include "src/xnnpack/common.h"
#include "src/xnnpack/intrinsics-polyfill.h"
#include "src/xnnpack/microparams.h"

static XNN_INLINE void xnn_store_tail_f16(uint16_t* o, __m128i vh, size_t c) {
  assert(c > 0);
  assert(c < 8);
  if (c & 4) {
    _mm_storel_epi64((__m128i*) o, vh);
    vh = _mm_unpackhi_epi64(vh, vh);
    o += 4;
  }
  if (c & 2) {
    _mm_storeu_si32(o, vh);
    vh = _mm_srli_epi64(vh, 32);
    o += 2;
  }
  if (c & 1) {
    *o = (uint16_t) _mm_extract_epi16(vh, 0);
  }
}

static XNN_INLINE __m128i xnn_load_tail_safe_f16(const uint16_t* i, size_t c) {
  assert(c > 0);
  assert(c < 8);

  XNN_ALIGN(16) uint16_t padded[8];
  uint16_t* dst = padded;
  switch (c) {
  case 7: *dst++ = *i++;
  case 6: *dst++ = *i++;
  case 5: *dst++ = *i++;
  case 4: *dst++ = *i++;
  case 3: *dst++ = *i++;
  case 2: *dst++ = *i++;
  default: *dst++ = *i++;
  }
  return _mm_load_si128((const __m128i*) padded);
}

void xnn_f16_avgpool_minmax_ukernel_9p__f16c_u${SIMD_SIZE}(
    size_t output_pixels,
    size_t kernel_elements,
    size_t channels,
    const xnn_float16** input,
    size_t input_offset,
    size_t input_pixel_stride,
    const xnn_float16* zero,
    const xnn_float16* multiplier,
    xnn_float16* output,
    size_t input_increment,
    size_t output_increment,
    const struct xnn_f16_scaleminmax_params* restrict params)
{
  assert(output_pixels != 0);
  assert(channels != 0);

  const __m256 vmin = _mm256_cvtph_ps(_mm_set1_epi16(*(const uint16_t*) &params->scalar.min));
  const __m256 vmax = _mm256_cvtph_ps(_mm_set1_epi16(*(const uint16_t*) &params->scalar.max));
  XNN_FORCE_REALIZATION(vmin);
  XNN_FORCE_REALIZATION(vmax);

  __m256 vscale = _mm256_cvtph_ps(_mm_set1_epi16(*(const uint16_t*) &params->scalar.scale));

  do {
    // Start with the previous output as the zero buffer.
    const uint16_t* prev_output = (const uint16_t*) zero;

    const xnn_float16** i = input;

    // Passes 0 - n-1: load the output, add 9 inputs.
    size_t k = kernel_elements;
    for (; k > 9; k -= 9) {
      $for K in range(9):
        const uint16_t* i${K} = (const uint16_t*) *i++;
        assert(i${K} != NULL);
        if XNN_UNPREDICTABLE(i${K} != (const uint16_t*) zero) {
          i${K} = (const uint16_t*) ((uintptr_t) i${K} + input_offset);
        }

      uint16_t* o = (uint16_t*) output;
      size_t c = channels;
      for (; c >= ${SIMD_SIZE}; c -= ${SIMD_SIZE}) {
        $for K in range(9):
          const __m256 vi${K} = _mm256_cvtph_ps(_mm_loadu_si128((const __m128i*) i${K})); i${K} += ${SIMD_SIZE};
        const __m256 vprev = _mm256_cvtph_ps(_mm_loadu_si128((const __m128i*) prev_output)); prev_output += ${SIMD_SIZE};

        const __m256 vsum018 = _mm256_add_ps(_mm256_add_ps(vi0, vi1), vi8);
        const __m256 vsum23 = _mm256_add_ps(vi2, vi3);
        const __m256 vsum45 = _mm256_add_ps(vi4, vi5);
        const __m256 vsum67 = _mm256_add_ps(vi6, vi7);

        const __m256 vsum2345 = _mm256_add_ps(vsum23, vsum45);
        const __m256 vsum01678 = _mm256_add_ps(vsum018, vsum67);
        const __m256 vsum012345678 = _mm256_add_ps(vsum2345, vsum01678);

        const __m256 vacc = _mm256_add_ps(vprev, vsum012345678);

        _mm_storeu_si128((__m128i*) o, _mm256_cvtps_ph(vacc, _MM_FROUND_TO_NEAREST_INT)); o += ${SIMD_SIZE};
      }
      $if SIMD_SIZE > 1:
        if (c > 0) {
          $for K in range(9):
            const __m256 vi${K} = _mm256_cvtph_ps(_mm_loadu_si128((const __m128i*) i${K}));
          const __m256 vprev = _mm256_cvtph_ps(xnn_load_tail_safe_f16((const uint16_t*) prev_output, c));

          const __m256 vsum018 = _mm256_add_ps(_mm256_add_ps(vi0, vi1), vi8);
          const __m256 vsum23 = _mm256_add_ps(vi2, vi3);
          const __m256 vsum45 = _mm256_add_ps(vi4, vi5);
          const __m256 vsum67 = _mm256_add_ps(vi6, vi7);

          const __m256 vsum2345 = _mm256_add_ps(vsum23, vsum45);
          const __m256 vsum01678 = _mm256_add_ps(vsum018, vsum67);
          const __m256 vsum012345678 = _mm256_add_ps(vsum2345, vsum01678);

          const __m256 vacc = _mm256_add_ps(vprev, vsum012345678);

          xnn_store_tail_f16(o, _mm256_cvtps_ph(vacc, _MM_FROUND_TO_NEAREST_INT), c); o += c;
        }

      // Subsequent passes read from the previous output.
      prev_output = (const uint16_t*) output;
    }

    // Final pass: load the output, add remaining kernel elements, apply scaling/min/max
    $for K in range(9):
      const uint16_t* i${K} = (const uint16_t*) (${K} < k ? *i++ : zero);
      assert(i${K} != NULL);
      if XNN_UNPREDICTABLE(i${K} != (const uint16_t*) zero) {
        i${K} = (const uint16_t*) ((uintptr_t) i${K} + input_offset);
      }

    if (multiplier) {
      vscale = _mm256_cvtph_ps(_mm_set1_epi16(*(const uint16_t*) (multiplier++)));
    }
    uint16_t* o = (uint16_t*) output;
    size_t c = channels;
    for (; c >= ${SIMD_SIZE}; c -= ${SIMD_SIZE}) {
      $for K in range(9):
        const __m256 vi${K} = _mm256_cvtph_ps(_mm_loadu_si128((const __m128i*) i${K})); i${K} += ${SIMD_SIZE};
      const __m256 vprev = _mm256_cvtph_ps(_mm_loadu_si128((const __m128i*) prev_output)); prev_output += ${SIMD_SIZE};

      const __m256 vsum018 = _mm256_add_ps(_mm256_add_ps(vi0, vi1), vi8);
      const __m256 vsum23 = _mm256_add_ps(vi2, vi3);
      const __m256 vsum45 = _mm256_add_ps(vi4, vi5);
      const __m256 vsum67 = _mm256_add_ps(vi6, vi7);

      const __m256 vsum2345 = _mm256_add_ps(vsum23, vsum45);
      const __m256 vsum01678 = _mm256_add_ps(vsum018, vsum67);
      const __m256 vsum012345678 = _mm256_add_ps(vsum2345, vsum01678);

      __m256 vacc = _mm256_add_ps(vprev, vsum012345678);

      vacc = _mm256_mul_ps(vacc, vscale);
      vacc = _mm256_max_ps(vacc, vmin);
      vacc = _mm256_min_ps(vacc, vmax);

      _mm_storeu_si128((__m128i*) o, _mm256_cvtps_ph(vacc, _MM_FROUND_TO_NEAREST_INT)); o += ${SIMD_SIZE};
    }
    $if SIMD_SIZE > 1:
      if (c > 0) {
        $for K in range(9):
          const __m256 vi${K} = _mm256_cvtph_ps(_mm_loadu_si128((const __m128i*) i${K}));
        const __m256 vprev = _mm256_cvtph_ps(xnn_load_tail_safe_f16((const uint16_t*) prev_output, c));

        const __m256 vsum018 = _mm256_add_ps(_mm256_add_ps(vi0, vi1), vi8);
        const __m256 vsum23 = _mm256_add_ps(vi2, vi3);
        const __m256 vsum45 = _mm256_add_ps(vi4, vi5);
        const __m256 vsum67 = _mm256_add_ps(vi6, vi7);

        const __m256 vsum2345 = _mm256_add_ps(vsum23, vsum45);
        const __m256 vsum01678 = _mm256_add_ps(vsum018, vsum67);
        const __m256 vsum012345678 = _mm256_add_ps(vsum2345, vsum01678);

        __m256 vacc = _mm256_add_ps(vprev, vsum012345678);

        vacc = _mm256_mul_ps(vacc, vscale);
        vacc = _mm256_max_ps(vacc, vmin);
        vacc = _mm256_min_ps(vacc, vmax);

        xnn_store_tail_f16((uint16_t*) o, _mm256_cvtps_ph(vacc, _MM_FROUND_TO_NEAREST_INT), c); o += c;
      }

    input = (const xnn_float16**) ((uintptr_t) input + input_increment);
    input_offset += input_pixel_stride;
    output = (xnn_float16*) ((uintptr_t) output + output_increment);
  } while (--output_pixels != 0);
}
