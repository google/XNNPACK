// Copyright 2023 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$ABC = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
$assert NR % 8 == 0
$assert 8 <= NR <= 16
$assert REQUANTIZATION in ["FP32", "RNDNU"] or not REQUANTIZATION
$assert DATATYPE in ["QC8", "QS8", "QD8", "QC4"]
$assert DATATYPE != "QC8" or REQUANTIZATION == "FP32"
$assert not DATATYPE in ["QD8", "QC4"] or not REQUANTIZATION
#include <assert.h>

#include <arm_neon.h>

#include <xnnpack/gemm.h>
$if REQUANTIZATION == "FP32":
  #include <xnnpack/intrinsics-polyfill.h>
#include <xnnpack/math.h>


$DATATYPE_SPEC = {"QC8": "qs8_qc8w", "QS8": "qs8", "QD8": "qd8_f32_qc8w", "QC4": "qd8_f32_qc4w"}[DATATYPE]
$REQUANTIZATION_SPEC = "_" + REQUANTIZATION.lower() if REQUANTIZATION else ""
$PARAMS_STRUCT = REQUANTIZATION.lower() + "_" + ("neonv8" if REQUANTIZATION == "FP32" else "neon")
$PARAMS_UNION = {"QC8": "xnn_qs8_qc8w_conv_minmax_params", "QS8": "xnn_qs8_conv_minmax_params", "QD8": "xnn_f32_minmax_params", "QC4": "xnn_f32_qc4w_minmax_params"}[DATATYPE]
$OUT_T = "float" if DATATYPE in ["QC4", "QD8"] else "int8_t"
void xnn_${DATATYPE_SPEC}_gemm_minmax${REQUANTIZATION_SPEC}_ukernel_${MR}x${NR}c8__neoni8mm(
    size_t mr,
    size_t nc,
    size_t kc,
    const int8_t* restrict a,
    size_t a_stride,
    const void* restrict w,
    ${OUT_T}* restrict c,
    size_t cm_stride,
    size_t cn_stride,
    $if DATATYPE in ["QD8", "QC4"]:
      const union ${PARAMS_UNION} params[restrict XNN_MIN_ELEMENTS(1)],
      const struct xnn_qd8_quantization_params quantization_params[restrict XNN_MIN_ELEMENTS(1)]) XNN_OOB_READS
    $else:
      const union ${PARAMS_UNION} params[restrict XNN_MIN_ELEMENTS(1)]) XNN_OOB_READS
{
  assert(mr != 0);
  assert(mr <= ${MR});
  assert(nc != 0);
  assert(kc != 0);
  assert(kc % sizeof(int8_t) == 0);
  assert(a != NULL);
  assert(w != NULL);
  assert(c != NULL);

  kc = round_up_po2(kc, 8 * sizeof(int8_t));
  const int8_t* a0 = a;
  ${OUT_T}* c0 = c;
  $for M in range(1, MR):
    const int8_t* a${M} = (const int8_t*) ((uintptr_t) a${M-1} + a_stride);
    ${OUT_T}* c${M} = (${OUT_T}*) ((uintptr_t) c${M-1} + cm_stride);
    $if M % 2 == 0:
      if XNN_UNPREDICTABLE(mr <= ${M}) {
        a${M} = a${M-1};
        c${M} = c${M-1};
      }
    $elif M + 1 == MR:
      if XNN_UNPREDICTABLE(mr != ${M+1}) {
        a${M} = a${M-1};
        c${M} = c${M-1};
      }
    $else:
      if XNN_UNPREDICTABLE(mr < ${M+1}) {
        a${M} = a${M-1};
        c${M} = c${M-1};
      }
  $if DATATYPE == "QC4":
    const int8x16_t vmask = vmovq_n_s8(INT8_C(0xF0));

  // Loop over groups of ${NR} columns.
  do {
    // Initialize accumulators with bias. ${NR} bias values are loaded from the
    // weight matrix, at the start of the group of ${NR} columns.
    $if DATATYPE in ["QD8", "QC4"]:
      $for M in range(0, MR, 2):
        $if M + 1 < MR:
          const int32x4_t vinput_zero_point${ABC[M:M+2]} = vld1q_s32(&quantization_params[${M}].zero_point);
        $else:
          const int32x4_t vinput_zero_point${ABC[M:M+2]} = vld1q_dup_s32(&quantization_params[${M}].zero_point);
        $for N in range(0, NR, 4):
          $if M == 0:
            const int32x4_t vksum${ABC[N:N+4]} = vld1q_s32(w); w = (const int32_t*) w + 4;
          $if M + 1 < MR:
            const int32x4_t vksumzp${M}x${ABC[N:N+4]} = vmulq_lane_s32(vksum${ABC[N:N+4]}, vget_low_s32(vinput_zero_point${ABC[M:M+2]}), 0);
            const int32x4_t vksumzp${M+1}x${ABC[N:N+4]} = vmulq_lane_s32(vksum${ABC[N:N+4]}, vget_high_s32(vinput_zero_point${ABC[M:M+2]}), 0);
          $else:
            const int32x4_t vksumzp${M}x${ABC[N:N+4]} = vmulq_s32(vksum${ABC[N:N+4]}, vinput_zero_point${ABC[M:M+2]});

      #if XNN_ARCH_ARM64
        $for M in range(0, MR, 2):
          $for N in range(0, NR, 4):
            $if M + 1 < MR:
              int32x4_t vacc${M}${M+1}x${ABC[N:N+2]} = vreinterpretq_s32_u64(vtrn1q_u64(vreinterpretq_u64_s32(vksumzp${M}x${ABC[N:N+4]}), vreinterpretq_u64_s32(vksumzp${M+1}x${ABC[N:N+4]})));
              int32x4_t vacc${M}${M+1}x${ABC[N+2:N+4]} = vreinterpretq_s32_u64(vtrn2q_u64(vreinterpretq_u64_s32(vksumzp${M}x${ABC[N:N+4]}), vreinterpretq_u64_s32(vksumzp${M+1}x${ABC[N:N+4]})));
            $else:
              int32x4_t vacc${M}${M+1}x${ABC[N:N+2]} = vreinterpretq_s32_u64(vtrn1q_u64(vreinterpretq_u64_s32(vksumzp${M}x${ABC[N:N+4]}), vreinterpretq_u64_s32(vksumzp${M}x${ABC[N:N+4]})));
              int32x4_t vacc${M}${M+1}x${ABC[N+2:N+4]} = vreinterpretq_s32_u64(vtrn2q_u64(vreinterpretq_u64_s32(vksumzp${M}x${ABC[N:N+4]}), vreinterpretq_u64_s32(vksumzp${M}x${ABC[N:N+4]})));
      #else
        $for M in range(0, MR, 2):
          $for N in range(0, NR, 4):
            $if M + 1 < MR:
              int32x4_t vacc${M}${M+1}x${ABC[N:N+2]} = vcombine_s32(vget_low_s32(vksumzp${M}x${ABC[N:N+4]}), vget_low_s32(vksumzp${M+1}x${ABC[N:N+4]}));
              int32x4_t vacc${M}${M+1}x${ABC[N+2:N+4]} = vcombine_s32(vget_high_s32(vksumzp${M}x${ABC[N:N+4]}), vget_high_s32(vksumzp${M+1}x${ABC[N:N+4]}));
            $else:
              int32x4_t vacc${M}${M+1}x${ABC[N:N+2]} = vcombine_s32(vget_low_s32(vksumzp${M}x${ABC[N:N+4]}), vget_low_s32(vksumzp${M}x${ABC[N:N+4]}));
              int32x4_t vacc${M}${M+1}x${ABC[N+2:N+4]} = vcombine_s32(vget_high_s32(vksumzp${M}x${ABC[N:N+4]}), vget_high_s32(vksumzp${M}x${ABC[N:N+4]}));
      #endif
    $else:
      #if XNN_ARCH_ARM64
        $for N in range(0, NR, 4):
          const uint64x2x2_t vbias01x${ABC[N:N+4]} = vld2q_dup_u64(w); w = (const int32_t*) w + 4;
      #else
        $for N in range(0, NR, 4):
          uint64x2x2_t vbias01x${ABC[N:N+4]};
          vbias01x${ABC[N:N+4]}.val[0] = vld1q_dup_u64(w); w = (const int32_t*) w + 2;
          vbias01x${ABC[N:N+4]}.val[1] = vld1q_dup_u64(w); w = (const int32_t*) w + 2;
      #endif
      $for M in range(0, MR, 2):
        $for N in range(0, NR, 4):
          int32x4_t vacc${M}${M+1}x${ABC[N:N+2]} = vreinterpretq_s32_u64(vbias01x${ABC[N:N+4]}.val[0]);
          int32x4_t vacc${M}${M+1}x${ABC[N+2:N+4]} = vreinterpretq_s32_u64(vbias01x${ABC[N:N+4]}.val[1]);

    // Inner accumulation loop along the ${NR} columns.
    size_t k = kc;
    // 2x partial unrolled loop to load 8 bytes at a time.

    $for M in range(0, MR, 2):
      uint64x2x2_t va${M}${M+1}x${ABC[0:16]};
      va${M}${M+1}x${ABC[0:16]}.val[0] = vdupq_n_u64(0);
      va${M}${M+1}x${ABC[0:16]}.val[1] = vdupq_n_u64(0);

    while (k >= 16 * sizeof(int8_t)) {
      // Load a ${MR}x16 block of activations.
      #if XNN_ARCH_ARM64
        $for M in range(0, MR, 2):
          va${M}${M+1}x${ABC[0:16]} = vld2q_lane_u64((const void*) a${M}, va${M}${M+1}x${ABC[0:16]}, 0); a${M} += 16;
        $for M in range(0, MR, 2):
          $if M + 1 < MR:
            va${M}${M+1}x${ABC[0:16]} = vld2q_lane_u64((const void*) a${M+1}, va${M}${M+1}x${ABC[0:16]}, 1); a${M+1} += 16;
      #else
        $for M in range(0, MR, 2):
          va${M}${M+1}x${ABC[0:16]}.val[0] = vld1q_lane_u64((const void*) a${M}, va${M}${M+1}x${ABC[0:16]}.val[0], 0); a${M} += 8;
          va${M}${M+1}x${ABC[0:16]}.val[1] = vld1q_lane_u64((const void*) a${M}, va${M}${M+1}x${ABC[0:16]}.val[1], 0); a${M} += 8;
        $for M in range(0, MR, 2):
          $if M + 1 < MR:
            va${M}${M+1}x${ABC[0:16]}.val[0] = vld1q_lane_u64((const void*) a${M+1}, va${M}${M+1}x${ABC[0:16]}.val[0], 1); a${M+1} += 8;
            va${M}${M+1}x${ABC[0:16]}.val[1] = vld1q_lane_u64((const void*) a${M+1}, va${M}${M+1}x${ABC[0:16]}.val[1], 1); a${M+1} += 8;
      #endif

      $for M in range(0, MR, 2):
        const int8x16_t va${M}${M+1}x${ABC[0:8]} = vreinterpretq_s8_u64(va${M}${M+1}x${ABC[0:16]}.val[0]);
        const int8x16_t va${M}${M+1}x${ABC[8:16]} = vreinterpretq_s8_u64(va${M}${M+1}x${ABC[0:16]}.val[1]);

      // Load a 16x${NR} block of weights.
      $if DATATYPE == "QC4":
        $for N in range(0, NR, 4):
          const int8x16_t vb${ABC[N:N+4]}x${ABC[0:8]} = vld1q_s8(w); w = (const int8_t*) w + 16;
        $for N in range(0, NR, 4):
          const int8x16_t vb${ABC[N:N+4]}x${ABC[8:16]} = vld1q_s8(w); w = (const int8_t*) w + 16;
        $for K in range(0, 16, 8):
          $for N in range(0, NR, 4):
            const int8x16_t vxb${ABC[N:N+4]}x${ABC[K:K+8:2]} = vshlq_n_s8(vb${ABC[N:N+4]}x${ABC[K:K+8]}, 4);
            const int8x16_t vxb${ABC[N:N+4]}x${ABC[K+1:K+8:2]} = vandq_s8(vb${ABC[N:N+4]}x${ABC[K:K+8]}, vmask);
        $for N in range(0, NR, 4):
          const int8x16x2_t vxb${ABC[N:N+4]}x${ABC[0:8]} = vzipq_s8(vxb${ABC[N:N+4]}x${ABC[0:8:2]}, vxb${ABC[N:N+4]}x${ABC[1:8:2]});
          const int8x16x2_t vxb${ABC[N:N+4]}x${ABC[8:16]} = vzipq_s8(vxb${ABC[N:N+4]}x${ABC[8:16:2]}, vxb${ABC[N:N+4]}x${ABC[9:16:2]});

        // Multiply-accumulate: ${MR}x8 * 8x${NR} --> ${MR}x${NR}.
        $for M in range(0, MR, 2):
          $for N in range(0, NR, 4):
            vacc${M}${M+1}x${ABC[N:N+2]} = vmmlaq_s32(vacc${M}${M+1}x${ABC[N:N+2]}, va${M}${M+1}x${ABC[0:8]}, vxb${ABC[N:N+4]}x${ABC[0:8]}.val[0]);
            vacc${M}${M+1}x${ABC[N+2:N+4]} = vmmlaq_s32(vacc${M}${M+1}x${ABC[N+2:N+4]}, va${M}${M+1}x${ABC[0:8]}, vxb${ABC[N:N+4]}x${ABC[0:8]}.val[1]);
        $for M in range(0, MR, 2):
          $for N in range(0, NR, 4):
            vacc${M}${M+1}x${ABC[N:N+2]} = vmmlaq_s32(vacc${M}${M+1}x${ABC[N:N+2]}, va${M}${M+1}x${ABC[8:16]}, vxb${ABC[N:N+4]}x${ABC[8:16]}.val[0]);
            vacc${M}${M+1}x${ABC[N+2:N+4]} = vmmlaq_s32(vacc${M}${M+1}x${ABC[N+2:N+4]}, va${M}${M+1}x${ABC[8:16]}, vxb${ABC[N:N+4]}x${ABC[8:16]}.val[1]);
      $else:
        $for N in range(0, NR, 2):
          const int8x16_t vb${ABC[N:N+2]}x${ABC[0:8]} = vld1q_s8(w); w = (const int8_t*) w + 16;
        $for N in range(0, NR, 2):
          const int8x16_t vb${ABC[N:N+2]}x${ABC[8:16]} = vld1q_s8(w); w = (const int8_t*) w + 16;

        // Multiply-accumulate: ${MR}x8 * 8x${NR} --> ${MR}x${NR}.
        $for M in range(0, MR, 2):
          $for N in range(0, NR, 2):
            vacc${M}${M+1}x${ABC[N:N+2]} = vmmlaq_s32(vacc${M}${M+1}x${ABC[N:N+2]}, va${M}${M+1}x${ABC[0:8]}, vb${ABC[N:N+2]}x${ABC[0:8]});
        $for M in range(0, MR, 2):
          $for N in range(0, NR, 2):
            vacc${M}${M+1}x${ABC[N:N+2]} = vmmlaq_s32(vacc${M}${M+1}x${ABC[N:N+2]}, va${M}${M+1}x${ABC[8:16]}, vb${ABC[N:N+2]}x${ABC[8:16]});

      k -= 16 * sizeof(int8_t);
    }
    // Handle up to 8 final positions of `k`
    if XNN_UNLIKELY(k != 0) {
      // Load a ${MR}x8 block of activations.
      $for M in range(0, MR, 2):
        uint64x2_t va${M}${M+1}x${ABC[0:8]} = vld1q_dup_u64((const void*) a${M}); a${M} += 8;
      $for M in range(0, MR, 2):
        $if M + 1 < MR:
          va${M}${M+1}x${ABC[0:8]} = vld1q_lane_u64((const void*) a${M+1}, va${M}${M+1}x${ABC[0:8]}, 1); a${M+1} += 8;

      // Load a 16x${NR} block of weights.
      $if DATATYPE == "QC4":
        $for N in range(0, NR, 4):
          const int8x16_t vb${ABC[N:N+4]}x${ABC[0:8]} = vld1q_s8(w); w = (const int8_t*) w + 16;
        $for N in range(0, NR, 4):
          const int8x16_t vxb${ABC[N:N+4]}x${ABC[0:8:2]} = vshlq_n_s8(vb${ABC[N:N+4]}x${ABC[0:8]}, 4);
          const int8x16_t vxb${ABC[N:N+4]}x${ABC[1:8:2]} = vandq_s8(vb${ABC[N:N+4]}x${ABC[0:8]}, vmask);
        $for N in range(0, NR, 4):
          const int8x16x2_t vxb${ABC[N:N+4]}x${ABC[0:8]} = vzipq_s8(vxb${ABC[N:N+4]}x${ABC[0:8:2]}, vxb${ABC[N:N+4]}x${ABC[1:8:2]});
        // Multiply-accumulate: ${MR}x4 * 4x${NR} --> ${MR}x${NR}.
        $for M in range(0, MR, 2):
          $for N in range(0, NR, 4):
            vacc${M}${M+1}x${ABC[N:N+2]} = vmmlaq_s32(vacc${M}${M+1}x${ABC[N:N+2]}, vreinterpretq_s8_u64(va${M}${M+1}x${ABC[0:8]}), vxb${ABC[N:N+4]}x${ABC[0:8]}.val[0]);
            vacc${M}${M+1}x${ABC[N+2:N+4]} = vmmlaq_s32(vacc${M}${M+1}x${ABC[N+2:N+4]}, vreinterpretq_s8_u64(va${M}${M+1}x${ABC[0:8]}), vxb${ABC[N:N+4]}x${ABC[0:8]}.val[1]);
      $else:
        $for N in range(0, NR, 2):
          const int8x16_t vb${ABC[N:N+2]}x${ABC[0:8]} = vld1q_s8(w); w = (const int8_t*) w + 16;

        // Multiply-accumulate: ${MR}x4 * 4x${NR} --> ${MR}x${NR}.
        $for M in range(0, MR, 2):
          $for N in range(0, NR, 2):
            vacc${M}${M+1}x${ABC[N:N+2]} = vmmlaq_s32(vacc${M}${M+1}x${ABC[N:N+2]}, vreinterpretq_s8_u64(va${M}${M+1}x${ABC[0:8]}), vb${ABC[N:N+2]}x${ABC[0:8]});
    }

    #if XNN_ARCH_ARM64
      $for M in range(0, MR, 2):
        $for N in range(0, NR, 4):
          int32x4_t vacc${M}x${ABC[N:N+4]} = vreinterpretq_s32_u64(vtrn1q_u64(vreinterpretq_u64_s32(vacc${M}${M+1}x${ABC[N:N+2]}), vreinterpretq_u64_s32(vacc${M}${M+1}x${ABC[N+2:N+4]})));
          $if M + 1 < MR:
            int32x4_t vacc${M+1}x${ABC[N:N+4]} = vreinterpretq_s32_u64(vtrn2q_u64(vreinterpretq_u64_s32(vacc${M}${M+1}x${ABC[N:N+2]}), vreinterpretq_u64_s32(vacc${M}${M+1}x${ABC[N+2:N+4]})));
    #else
      $for M in range(0, MR, 2):
        $for N in range(0, NR, 4):
          int32x4_t vacc${M}x${ABC[N:N+4]} = vcombine_s32(vget_low_s32(vacc${M}${M+1}x${ABC[N:N+2]}), vget_low_s32(vacc${M}${M+1}x${ABC[N+2:N+4]}));
          $if M + 1 < MR:
            int32x4_t vacc${M+1}x${ABC[N:N+4]} = vcombine_s32(vget_high_s32(vacc${M}${M+1}x${ABC[N:N+2]}), vget_high_s32(vacc${M}${M+1}x${ABC[N+2:N+4]}));
    #endif
    $if DATATYPE == "QC4":
      $for M in range(0, MR):
        $for N in range(0, NR, 4):
          float32x4_t vout${M}x${ABC[N:N+4]} = vcvtq_n_f32_s32(vacc${M}x${ABC[N:N+4]}, 4);
    $elif DATATYPE == "QD8":
      $for M in range(0, MR):
        $for N in range(0, NR, 4):
          float32x4_t vout${M}x${ABC[N:N+4]} = vcvtq_f32_s32(vacc${M}x${ABC[N:N+4]});

    $if DATATYPE in ["QD8", "QC4"]:
      $for M in range(0, MR, 2):
        $if M + 1 == MR:
          const float32x4_t vinput_scale${M} = vld1q_dup_f32(&quantization_params[${M}].inv_scale);
          $for N in range(0, NR, 4):
            vout${M}x${ABC[N:N+4]} = vmulq_f32(vout${M}x${ABC[N:N+4]}, vinput_scale${M});
        $else:
          const float32x4_t vinput_scale${ABC[M:M+2]} = vreinterpretq_f32_s32(vld1q_s32(&quantization_params[${M}].zero_point));
          $for N in range(0, NR, 4):
            vout${M}x${ABC[N:N+4]} = vmulq_lane_f32(vout${M}x${ABC[N:N+4]}, vget_low_f32(vinput_scale${ABC[M:M+2]}), 1);
            vout${M+1}x${ABC[N:N+4]} = vmulq_lane_f32(vout${M+1}x${ABC[N:N+4]}, vget_high_f32(vinput_scale${ABC[M:M+2]}), 1);

      $for N in range(0, NR, 4):
        const float32x4_t vfilter_output_scale${ABC[N:N+4]} = vld1q_f32(w); w = (const float*) w + 4;

      #if XNN_ARCH_ARM64
        $for N in range(0, NR, 4):
          const float32x4_t vbias${ABC[N:N+4]} = vld1q_f32(w); w = (const float*) w + 4;
          $for M in range(MR):
            vout${M}x${ABC[N:N+4]} = vfmaq_f32(vbias${ABC[N:N+4]}, vout${M}x${ABC[N:N+4]}, vfilter_output_scale${ABC[N:N+4]});
      #else
        $for N in range(0, NR, 4):
          const float32x4_t vbias${ABC[N:N+4]} = vld1q_f32(w); w = (const float*) w + 4;
          $for M in range(MR):
            vout${M}x${ABC[N:N+4]} = vmlaq_f32(vbias${ABC[N:N+4]}, vout${M}x${ABC[N:N+4]}, vfilter_output_scale${ABC[N:N+4]});
      #endif

      const float32x4_t voutput_min = vld1q_dup_f32(&params->scalar.min);
      $for M in range(0, MR):
        $for N in range(0, NR, 4):
          vout${M}x${ABC[N:N+4]} = vmaxq_f32(vout${M}x${ABC[N:N+4]}, voutput_min);

      const float32x4_t voutput_max = vld1q_dup_f32(&params->scalar.max);
      $for M in range(0, MR):
        $for N in range(0, NR, 4):
          vout${M}x${ABC[N:N+4]} = vminq_f32(vout${M}x${ABC[N:N+4]}, voutput_max);

      if XNN_LIKELY(nc >= ${NR}) {
        $for M in reversed(range(MR)):
          vst1q_f32(c${M}, vout${M}x${ABC[0:4]});
          $for N in range(4, NR, 4):
            vst1q_f32(c${M} + ${N}, vout${M}x${ABC[N:N+4]});

        $for M in range(MR):
          a${M} = (const int8_t*) ((uintptr_t) a${M} - kc);

        $for M in range(MR):
          c${M} = (float*) ((uintptr_t) c${M} + cn_stride);

        nc -= ${NR};
      } else {
       $for LOG2N in reversed(range(NR.bit_length())):
          $if NR != 1 << LOG2N:
            $if LOG2N == 1:
              $for M in reversed(range(MR)):
                float32x2_t vout${M}x${ABC[N:N+2]} = vget_low_f32(vout${M}x${ABC[N:N+4]});
            if (nc & ${1 << LOG2N}) {
              $if LOG2N > 1:
                $for N in range(0, 1 << LOG2N, 4):
                  $for M in reversed(range(MR)):
                    vst1q_f32(c${M}, vout${M}x${ABC[N:N+4]}); c${M} += 4;
                    vout${M}x${ABC[N:N+4]} = vout${M}x${ABC[N+(1 << LOG2N):N+(1 << LOG2N)+4]};
              $elif LOG2N == 1:
                $for M in reversed(range(MR)):
                  vst1_f32(c${M}, vout${M}x${ABC[N:N+2]}); c${M} += 2;
                $for M in reversed(range(MR)):
                  vout${M}x${ABC[N:N+2]} = vget_high_f32(vout${M}x${ABC[N:N+4]});
              $elif LOG2N == 0:
                $for M in reversed(range(MR)):
                  vst1_lane_f32(c${M}, vout${M}x${ABC[N:N+2]}, 0);
            }
        nc = 0;
      }
    $else:
      $if REQUANTIZATION == "RNDNU":
        const int32x4_t vright_pre_shift = vld1q_dup_s32(&params->${PARAMS_STRUCT}.right_pre_shift);
        const int32x4_t vmultiplier = vld1q_dup_s32(&params->${PARAMS_STRUCT}.multiplier);
        const int32x4_t vright_post_shift = vld1q_dup_s32(&params->${PARAMS_STRUCT}.right_post_shift);

        $for M in range(MR):
          $for N in range(0, NR, 4):
            vacc${M}x${ABC[N:N+4]} = vqshlq_s32(vacc${M}x${ABC[N:N+4]}, vright_pre_shift);

        $for M in range(MR):
          $for N in range(0, NR, 4):
            vacc${M}x${ABC[N:N+4]} = vqdmulhq_s32(vacc${M}x${ABC[N:N+4]}, vmultiplier);

        $for M in range(MR):
          $for N in range(0, NR, 4):
            vacc${M}x${ABC[N:N+4]} = vrshlq_s32(vacc${M}x${ABC[N:N+4]}, vright_post_shift);
      $elif REQUANTIZATION == "FP32":
        $for M in range(MR):
          $for N in range(0, NR, 4):
            float32x4_t vfpacc${M}x${ABC[N:N+4]} = vcvtq_f32_s32(vacc${M}x${ABC[N:N+4]});

        $if DATATYPE == "QC8":
          $for N in range(0, NR, 4):
            const float32x4_t vscale${ABC[N:N+4]} = vld1q_f32((const float*) w); w = (const float*) w + 4;
            $for M in range(MR):
              vfpacc${M}x${ABC[N:N+4]} = vmulq_f32(vfpacc${M}x${ABC[N:N+4]}, vscale${ABC[N:N+4]});
        $else:
          const float32x4_t vscale = vld1q_dup_f32(&params->${PARAMS_STRUCT}.scale);
          $for M in range(MR):
            $for N in range(0, NR, 4):
              vfpacc${M}x${ABC[N:N+4]} = vmulq_f32(vfpacc${M}x${ABC[N:N+4]}, vscale);

        $for M in range(MR):
          $for N in range(0, NR, 4):
            vacc${M}x${ABC[N:N+4]} = vcvtnq_s32_f32(vfpacc${M}x${ABC[N:N+4]});

      const int16x8_t voutput_zero_point = vld1q_dup_s16(&params->${PARAMS_STRUCT}.output_zero_point);
      #if XNN_ARCH_ARM64
        $for M in range(MR):
          $for N in range(0, NR, 8):
            const int16x8_t vacc${M}x${ABC[N:N+8]} = vqaddq_s16(vqmovn_high_s32(vqmovn_s32(vacc${M}x${ABC[N:N+4]}), vacc${M}x${ABC[N+4:N+8]}), voutput_zero_point);

        $for M in range(MR):
          $for N in range(0, NR, 16):
            $if N + 8 < NR:
              int8x16_t vout${M}x${ABC[N:N+16]} = vqmovn_high_s16(vqmovn_s16(vacc${M}x${ABC[N:N+8]}), vacc${M}x${ABC[N+8:N+16]});
            $elif M % 2 == 1:
              int8x16_t vout${M-1}x${ABC[N:N+8]}_${M}x${ABC[N:N+8]} = vqmovn_high_s16(vqmovn_s16(vacc${M-1}x${ABC[N:N+8]}), vacc${M}x${ABC[N:N+8]});
            $elif M + 1 == MR:
              int8x8_t vout${M}x${ABC[N:N+8]} = vqmovn_s16(vacc${M}x${ABC[N:N+8]});
      #else
        $for M in range(MR):
          $for N in range(0, NR, 8):
            const int16x8_t vacc${M}x${ABC[N:N+8]} = vqaddq_s16(vcombine_s16(vqmovn_s32(vacc${M}x${ABC[N:N+4]}), vqmovn_s32(vacc${M}x${ABC[N+4:N+8]})), voutput_zero_point);

        $for M in range(MR):
          $for N in range(0, NR, 16):
            $if N + 8 < NR:
              int8x16_t vout${M}x${ABC[N:N+16]} = vcombine_s8(vqmovn_s16(vacc${M}x${ABC[N:N+8]}), vqmovn_s16(vacc${M}x${ABC[N+8:N+16]}));
            $elif M % 2 == 1:
              int8x16_t vout${M-1}x${ABC[N:N+8]}_${M}x${ABC[N:N+8]} = vcombine_s8(vqmovn_s16(vacc${M-1}x${ABC[N:N+8]}), vqmovn_s16(vacc${M}x${ABC[N:N+8]}));
            $elif M + 1 == MR:
              int8x8_t vout${M}x${ABC[N:N+8]} = vqmovn_s16(vacc${M}x${ABC[N:N+8]});
      #endif
      $if NR == 8 and MR == 1:
        const int8x8_t voutput_min = vld1_dup_s8(&params->${PARAMS_STRUCT}.output_min);
        const int8x8_t voutput_max = vld1_dup_s8(&params->${PARAMS_STRUCT}.output_max);
      $else:
        const int8x16_t voutput_min = vld1q_dup_s8(&params->${PARAMS_STRUCT}.output_min);
        const int8x16_t voutput_max = vld1q_dup_s8(&params->${PARAMS_STRUCT}.output_max);

      $for M in range(MR):
        $for N in range(0, NR, 16):
          $if N + 8 < NR:
            vout${M}x${ABC[N:N+16]} = vmaxq_s8(vout${M}x${ABC[N:N+16]}, voutput_min);
          $elif M % 2 == 1:
            vout${M-1}x${ABC[N:N+8]}_${M}x${ABC[N:N+8]} = vmaxq_s8(vout${M-1}x${ABC[N:N+8]}_${M}x${ABC[N:N+8]}, voutput_min);
          $elif M + 1 == MR:
            $if NR == 8 and MR == 1:
              vout${M}x${ABC[N:N+8]} = vmax_s8(vout${M}x${ABC[N:N+8]}, voutput_min);
            $else:
              vout${M}x${ABC[N:N+8]} = vmax_s8(vout${M}x${ABC[N:N+8]}, vget_low_s8(voutput_min));

      $for M in range(MR):
        $for N in range(0, NR, 16):
          $if N + 8 < NR:
            vout${M}x${ABC[N:N+16]} = vminq_s8(vout${M}x${ABC[N:N+16]}, voutput_max);
          $elif M % 2 == 1:
            vout${M-1}x${ABC[N:N+8]}_${M}x${ABC[N:N+8]} = vminq_s8(vout${M-1}x${ABC[N:N+8]}_${M}x${ABC[N:N+8]}, voutput_max);
          $elif M + 1 == MR:
            $if NR == 8 and MR == 1:
              vout${M}x${ABC[N:N+8]} = vmin_s8(vout${M}x${ABC[N:N+8]}, voutput_max);
            $else:
              vout${M}x${ABC[N:N+8]} = vmin_s8(vout${M}x${ABC[N:N+8]}, vget_low_s8(voutput_max));

      if (nc >= ${NR}) {
        // Main case where there the ${NR} columns fit in the destination.
        $for M in range(MR):
          $for N in range(0, NR, 16):
            $if N + 8 < NR:
              vst1q_s8(c${M} + ${N}, vout${M}x${ABC[N:N+16]});
            $elif M % 2 == 1:
              vst1_s8(c${M-1} + ${N}, vget_low_s8(vout${M-1}x${ABC[N:N+8]}_${M}x${ABC[N:N+8]}));
              vst1_s8(c${M} + ${N}, vget_high_s8(vout${M-1}x${ABC[N:N+8]}_${M}x${ABC[N:N+8]}));
            $elif M + 1 == MR:
              vst1_s8(c${M} + ${N}, vout${M}x${ABC[N:N+8]});

        // Advance to the next ${NR} columns.
        $for M in range(MR):
          c${M} = (int8_t*) ((uintptr_t) c${M} + cn_stride);

        $for M in range(MR):
          a${M} = (const int8_t*) ((uintptr_t) a${M} - kc);

        nc -= ${NR};
      } else {
        // Final case where not all of the ${NR} columns fit in the destination.
        $if NR == 16:
          $for M in range(MR):
            $if M % 2 == 1:
              int8x16_t vout${M-1}x01234567_${M}x01234567 = vcombine_s8(vget_low_s8(vout${M-1}x0123456789ABCDEF), vget_low_s8(vout${M}x0123456789ABCDEF));
            $elif M + 1 == MR:
              int8x8_t vout${M}x01234567 = vget_low_s8(vout${M}x0123456789ABCDEF);
          if (nc & 8) {
            $for M in range(MR):
              $if M % 2 == 1:
                vst1_s8(c${M-1}, vget_low_s8(vout${M-1}x01234567_${M}x01234567)); c${M-1} += 8;
                vst1_s8(c${M}, vget_high_s8(vout${M-1}x01234567_${M}x01234567)); c${M} += 8;
              $elif M + 1 == MR:
                vst1_s8(c${M}, vout${M}x01234567); c${M} += 8;
            $for M in range(MR):
              $if M % 2 == 1:
                vout${M-1}x01234567_${M}x01234567 = vcombine_s8(vget_high_s8(vout${M-1}x0123456789ABCDEF), vget_high_s8(vout${M}x0123456789ABCDEF));
              $elif M + 1 == MR:
                vout${M}x01234567 = vget_high_s8(vout${M}x0123456789ABCDEF);
          }
        if (nc & 4) {
          $for M in range(MR):
            $if M % 2 == 1:
              vst1q_lane_u32((void*) c${M-1}, vreinterpretq_u32_s8(vout${M-1}x01234567_${M}x01234567), 0); c${M-1} += 4;
              vst1q_lane_u32((void*) c${M}, vreinterpretq_u32_s8(vout${M-1}x01234567_${M}x01234567), 2); c${M} += 4;
            $elif M + 1 == MR:
              vst1_lane_u32((void*) c${M}, vreinterpret_u32_s8(vout${M}x01234567), 0); c${M} += 4;
          $for M in range(MR):
            $if M % 2 == 1:
              vout${M-1}x01234567_${M}x01234567 = vextq_s8(vout${M-1}x01234567_${M}x01234567, vout${M-1}x01234567_${M}x01234567, 4);
            $elif M + 1 == MR:
              vout${M}x01234567 = vext_s8(vout${M}x01234567, vout${M}x01234567, 4);
        }
        if (nc & 2) {
          $for M in range(MR):
            $if M % 2 == 1:
              vst1q_lane_u16((void*) c${M-1}, vreinterpretq_u16_s8(vout${M-1}x01234567_${M}x01234567), 0); c${M-1} += 2;
              vst1q_lane_u16((void*) c${M}, vreinterpretq_u16_s8(vout${M-1}x01234567_${M}x01234567), 4); c${M} += 2;
            $elif M + 1 == MR:
              vst1_lane_u16((void*) c${M}, vreinterpret_u16_s8(vout${M}x01234567), 0); c${M} += 2;
          $for M in range(MR):
            $if M % 2 == 1:
              vout${M-1}x01234567_${M}x01234567 = vextq_s8(vout${M-1}x01234567_${M}x01234567, vout${M-1}x01234567_${M}x01234567, 2);
            $elif M + 1 == MR:
              vout${M}x01234567 = vext_s8(vout${M}x01234567, vout${M}x01234567, 2);
        }
        if (nc & 1) {
          $for M in range(MR):
            $if M % 2 == 1:
              vst1q_lane_s8(c${M-1}, vout${M-1}x01234567_${M}x01234567, 0);
              vst1q_lane_s8(c${M}, vout${M-1}x01234567_${M}x01234567, 8);
            $elif M + 1 == MR:
              vst1_lane_s8(c${M}, vout${M}x01234567, 0);
        }

        nc = 0;
      }
  } while (nc != 0);
}
