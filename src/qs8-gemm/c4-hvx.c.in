// Copyright 2025 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert REQUANTIZATION == "FP32" or not REQUANTIZATION
$assert REQUANTIZATION in ["FP32", "RNDNU"] or not REQUANTIZATION
$assert DATATYPE in ["QC8", "QS8_QC4"]
$assert DATATYPE != "QC8" or REQUANTIZATION == "FP32"
$assert not DATATYPE in ["QD8", "QC4"] or not REQUANTIZATION
#include <assert.h>
#include <math.h>  // for lrintf
#include <stdio.h>  // for printf

#include <hexagon_types.h>
#include <hexagon_protos.h>
#include <hvx_hexagon_protos.h>

#include "src/xnnpack/gemm.h"
#include "src/xnnpack/intrinsics-polyfill.h"  // for Q6_V_vstu_variable
#include "src/xnnpack/math.h"
#include "src/xnnpack/unaligned.h"

$DATATYPE_SPEC = {"QC8": "qs8_qc8w", "QS8_QC4": "qs8_qc4w", "QD8_F16" : "qd8_f16_qc8w", "QD8_F32": "qd8_f32_qc8w", "QC4_F16": "qd8_f16_qc4w", "QC4_F32": "qd8_f32_qc4w"}[DATATYPE]
$REQUANTIZATION_SPEC = "_" + REQUANTIZATION.lower() if REQUANTIZATION else ""
$PARAMS_STRUCT = REQUANTIZATION.lower() + "_scalar" if REQUANTIZATION else "scalar"
$PARAMS_TYPE = {"QC8": "union xnn_qs8_qc8w_conv_minmax_params", "QS8_QC4": "union xnn_qs8_qc8w_conv_minmax_params", "QD8_F16": "struct xnn_f16_minmax_params", "QD8_F32": "struct xnn_f32_minmax_params", "QC4_F16": "struct xnn_f16_qc4w_minmax_params", "QC4_F32": "struct xnn_f32_qc4w_minmax_params"}[DATATYPE]
$OUT_T = {"QC8": "int8_t", "QS8_QC4": "int8_t", "QD8_F16": "xnn_float16", "QD8_F32": "float", "QC4_F16": "xnn_float16", "QC4_F32": "float"}[DATATYPE]

// multiply vacc by vscale and return result as int
// vacc is vector of int32
// vscale is vector of floats
// return is vector of int
#if __HVX_ARCH__ >= 73
static XNN_INLINE HVX_Vector rescale_fp32(HVX_Vector vacc, HVX_Vector vscale)
{
  const HVX_Vector vaccf = Q6_Vsf_equals_Vw(vacc);
  const HVX_Vector vscaledqf = Q6_Vqf32_vmpy_VsfVsf(vaccf, vscale);

  // Create a vector of `0.5f` with the same sign as the entries of `a`.
  const HVX_Vector vhalf = Q6_V_vsplat_R(float_as_uint32(0.5f));
  const HVX_Vector vsign_mask = Q6_V_vsplat_R(0x80000000);
  const HVX_Vector vsigned_half = Q6_V_vor_VV(Q6_V_vand_VV(vaccf, vsign_mask), vhalf);
  const HVX_Vector vresult = Q6_Vw_equals_Vsf(Q6_Vsf_equals_Vqf32(Q6_Vqf32_vadd_Vqf32Vsf(vscaledqf, vsigned_half)));
  return vresult;
}
#else
static HVX_Vector rescale_fp32(HVX_Vector vacc, HVX_Vector vscale)
{
  XNN_ALIGN(128) int32_t vacc_buffer[32];
  XNN_ALIGN(128) float vscale_buffer[32];

  *((HVX_Vector *)&vacc_buffer) = vacc;
  *((HVX_Vector *)&vscale_buffer) = vscale;

  for (int i = 0; i < 32; ++i) {
    vacc_buffer[i] = (int32_t)lrintf((float)vacc_buffer[i] * vscale_buffer[i]);
  }
  return *(HVX_Vector *)&vacc_buffer;
}
#endif  // __HVX_ARCH__ >= 73

void xnn_${DATATYPE_SPEC}_gemm_minmax${REQUANTIZATION_SPEC}_ukernel_${MR}x${NR}c4__hvx${"_prfm" if PREFETCH else ""}(
    size_t mr,
    size_t nc,
    size_t kc,
    const int8_t* restrict a,
    size_t a_stride,
    const void* restrict w,
    ${OUT_T}* restrict c,
    size_t cm_stride,
    size_t cn_stride,
    $if DATATYPE in ["QD8_F16", "QD8_F32", "QC4_F16", "QC4_F32"]:
      const ${PARAMS_TYPE}* restrict params,
      const struct xnn_qd8_quantization_params* restrict quantization_params) XNN_OOB_READS
    $else:
      const ${PARAMS_TYPE}* restrict params) XNN_OOB_READS
{
  assert(mr != 0);
  assert(mr <= ${MR});
  assert(nc != 0);
  assert(kc != 0);
  assert(kc % sizeof(int8_t) == 0);
  assert(a != NULL);
  assert(w != NULL);
  assert(c != NULL);

  kc = round_up_po2(kc, 4 * sizeof(int8_t));
  const int8_t* a0 = a;
  $if DATATYPE in ["QD8_F16", "QC4_F16"]:
    uint16_t* c0 = (uint16_t*) c;
  $else:
    ${OUT_T}* c0 = c;
  $for M in range(1, MR):
    const int8_t* a${M} = (const int8_t*) ((uintptr_t) a${M-1} + a_stride);
    $if DATATYPE in ["QD8_F16", "QC4_F16"]:
      uint16_t* c${M} = (uint16_t*) ((uintptr_t) c${M-1} + cm_stride);
    $else:
      ${OUT_T}* c${M} = (${OUT_T}*) ((uintptr_t) c${M-1} + cm_stride);
    $if M % 2 == 0:
      if XNN_UNPREDICTABLE(mr <= ${M}) {
        a${M} = a${M-1};
        c${M} = c${M-1};
      }
    $elif M + 1 == MR:
      if XNN_UNPREDICTABLE(mr != ${M+1}) {
        a${M} = a${M-1};
        c${M} = c${M-1};
      }
    $else:
      if XNN_UNPREDICTABLE(mr < ${M+1}) {
        a${M} = a${M-1};
        c${M} = c${M-1};
      }

  // TODO: Use log when fixed
  {
    static int warning_unaligned = 0;
    if ((a_stride & (sizeof(int32_t) - 1)) != 0 && warning_unaligned == 0) {
      printf("HEXAGON GEMM a_stride unaligned.");
      warning_unaligned = 1;
    }
    static int warning_a_unaligned = 0;
    if ((((intptr_t) a) & (sizeof(int32_t) - 1)) != 0 && warning_a_unaligned == 0) {
      printf("HEXAGON GEMM a unaligned.");
      warning_a_unaligned = 1;
    }
    fflush(stdout);
  }

  const HVX_Vector voutput_zero_point = Q6_Vh_vsplat_R(params->${PARAMS_STRUCT}.output_zero_point);
  const HVX_Vector voutput_min = Q6_Vb_vsplat_R(params->${PARAMS_STRUCT}.output_min);
  const HVX_Vector voutput_max = Q6_Vb_vsplat_R(params->${PARAMS_STRUCT}.output_max);
  $if DATATYPE in ["QC4_F16", "QC4_F32", "QS8_QC4"]:
    const HVX_Vector vmask = Q6_Vb_vsplat_R(0xF0);
  $if PREFETCH:
    const uint32_t prefetch_size = HEXAGON_V64_CREATE_H(0, 0, kc * ${NR}, 1);
  do {
    $if PREFETCH:
      Q6_l2fetch_AR((void*) w, prefetch_size);
    $for N in range(0, NR, 32):
      HVX_Vector vacc0x${N//32} = *((HVX_Vector *) w); w = (const int8_t*) w + 128;
    $for M in range(1, MR):
      $for N in range(0, NR, 32):
        HVX_Vector vacc${M}x${N//32} = vacc0x${N//32};

    size_t k = kc;
    if (((((intptr_t) a) | a_stride) & (sizeof(int32_t) - 1)) != 0) {
      for (; k >= 8 * sizeof(int8_t); k -= 8 * sizeof(int8_t)) {
        $for M in range(MR):
          const HVX_Vector va${M}x0123 = Q6_V_vsplat_R(unaligned_load_s32(a${M}));
          const HVX_Vector va${M}x4567 = Q6_V_vsplat_R(unaligned_load_s32(a${M}+4)); a${M} += 8;

        $if DATATYPE in ["QC4_F16", "QC4_F32", "QS8_QC4"]:
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x01234567 = *((HVX_Vector *) w); w = (const int8_t*) w + 128;
          $for N in range(0, NR, 32):
            const HVX_Vector vbs${N//32}x0123 = Q6_Vw_vasl_VwR(vb${N//32}x01234567, 4);
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x4567 = Q6_V_vand_VV(vb${N//32}x01234567, vmask);
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x0123 = Q6_V_vand_VV(vbs${N//32}x0123, vmask);
        $else:
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x0123 = *((HVX_Vector *) w); w = (const int8_t*) w + 128;
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x4567 = *((HVX_Vector *) w); w = (const int8_t*) w + 128;

        $for M in range(MR):
          $for N in range(0, NR, 32):
            vacc${M}x${N//32} = Q6_Vw_vrmpyacc_VwVbVb(vacc${M}x${N//32}, va${M}x0123, vb${N//32}x0123);
        $for M in range(MR):
          $for N in range(0, NR, 32):
            vacc${M}x${N//32} = Q6_Vw_vrmpyacc_VwVbVb(vacc${M}x${N//32}, va${M}x4567, vb${N//32}x4567);
      }
    } else {
      for (; k >= 8 * sizeof(int8_t); k -= 8 * sizeof(int8_t)) {
        $for M in range(MR):
          const HVX_Vector va${M}x0123 = Q6_V_vsplat_R(*((const int32_t*)a${M}));
          const HVX_Vector va${M}x4567 = Q6_V_vsplat_R(*((const int32_t*)a${M}+4)); a${M} += 8;

        $if DATATYPE in ["QC4_F16", "QC4_F32", "QS8_QC4"]:
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x01234567 = *((HVX_Vector *) w); w = (const int8_t*) w + 128;
          $for N in range(0, NR, 32):
            const HVX_Vector vbs${N//32}x0123 = Q6_Vw_vasl_VwR(vb${N//32}x01234567, 4);
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x4567 = Q6_V_vand_VV(vb${N//32}x01234567, vmask);
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x0123 = Q6_V_vand_VV(vbs${N//32}x0123, vmask);
        $else:
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x0123 = *((HVX_Vector *) w); w = (const int8_t*) w + 128;
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x4567 = *((HVX_Vector *) w); w = (const int8_t*) w + 128;

        $for M in range(MR):
          $for N in range(0, NR, 32):
            vacc${M}x${N//32} = Q6_Vw_vrmpyacc_VwVbVb(vacc${M}x${N//32}, va${M}x0123, vb${N//32}x0123);
        $for M in range(MR):
          $for N in range(0, NR, 32):
            vacc${M}x${N//32} = Q6_Vw_vrmpyacc_VwVbVb(vacc${M}x${N//32}, va${M}x4567, vb${N//32}x4567);
      }
    }
    if (k != 0) {
      $for M in range(MR):
        const HVX_Vector va${M}x0123 = Q6_V_vsplat_R(unaligned_load_s32(a${M})); a${M} += 4;

      $if DATATYPE in ["QC4_F16", "QC4_F32", "QS8_QC4"]:
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x01234567 = *((HVX_Vector *) w); w = (const int8_t*) w + 128;
          $for N in range(0, NR, 32):
            const HVX_Vector vbs${N//32}x0123 = Q6_Vw_vasl_VwR(vb${N//32}x01234567, 4);
          $for N in range(0, NR, 32):
            const HVX_Vector vb${N//32}x0123 = Q6_V_vand_VV(vbs${N//32}x0123, vmask);
      $else:
        $for N in range(0, NR, 32):
          const HVX_Vector vb${N//32}x0123 = *((HVX_Vector *) w); w = (const int8_t*) w + 128;

      $for M in range(MR):
        $for N in range(0, NR, 32):
          vacc${M}x${N//32} = Q6_Vw_vrmpyacc_VwVbVb(vacc${M}x${N//32}, va${M}x0123, vb${N//32}x0123);
    }

    $if DATATYPE in ["QC4_F16", "QC4_F32", "QS8_QC4"]:
      $for M in range(MR):
        $for N in range(0, NR, 32):
          vacc${M}x${N//32} = Q6_Vw_vasr_VwR(vacc${M}x${N//32}, 4);

    $for N in range(0, NR, 32):
      const HVX_Vector vscale${N//32} = *((HVX_Vector *) w); w = (const int8_t*) w + 128;
      $for M in range(MR):
        vacc${M}x${N//32} = rescale_fp32(vacc${M}x${N//32}, vscale${N//32});

    $for M in range(MR):
      $for N in range(0, NR, 64):
        $if N + 32 < NR:
          HVX_Vector vout${M}x${N//64} = Q6_Vh_vpack_VwVw_sat(vacc${M}x${N//32+1}, vacc${M}x${N//32});
        $else:
          HVX_Vector vout${M}x${N//64} = Q6_Vh_vpack_VwVw_sat(vacc${M}x${N//32}, vacc${M}x${N//32});

    $for M in range(MR):
      $for N in range(0, NR, 64):
        vout${M}x${N//64} = Q6_Vh_vadd_VhVh_sat(vout${M}x${N//64}, voutput_zero_point);

    $for M in range(MR):
      $if NR > 64:
        HVX_Vector vout${M} = Q6_Vb_vpack_VhVh_sat(vout${M}x1, vout${M}x0);
      $else:
        HVX_Vector vout${M} = Q6_Vb_vpack_VhVh_sat(vout${M}x0, vout${M}x0);

    $for M in range(MR):
      vout${M} = Q6_Vb_vmax_VbVb(vout${M}, voutput_min);

    $for M in range(MR):
      vout${M} = Q6_Vb_vmin_VbVb(vout${M}, voutput_max);

    if XNN_LIKELY(nc >= ${NR}) {
      $for M in range(MR):
        $if NR == 128:
          *((HVX_UVector *)c${M}) = vout${M};
        $else:
          Q6_V_vstu_variable(c${M}, ${NR}, vout${M});
        c${M} = (${OUT_T}*) ((uintptr_t) c${M} + cn_stride);
        a${M} = (const int8_t*) ((uintptr_t) a${M} - kc);

      nc -= ${NR};
    } else {
      // Prepare mask for valid 8-bit elements (depends on nc).
      $for M in range(MR):
        Q6_V_vstu_variable(c${M}, nc, vout${M});
      nc = 0;
    }
  } while (nc != 0);
}