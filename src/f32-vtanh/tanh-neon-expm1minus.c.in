// Copyright 2023 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert (LOG2LUT, P, H) in [(3, 4, 3), (0, 6, 5)]
$assert not PS or (P, H) == (4, 3)
$assert DIV in ["DIV", "NR2RECPS", "NR1RECPS1FMA", "NR2FMA"]
$LUT = 1 << LOG2LUT

#include <assert.h>
#include <stddef.h>
#include <stdint.h>
#include <math.h>

#include <arm_neon.h>

#include "xnnpack/common.h"
#include "xnnpack/math.h"
#include "xnnpack/vunary.h"
#include "xnnpack/microparams.h"

$if LOG2LUT == 3:
  extern XNN_INTERNAL const uint32_t xnn_table_exp2minus_k_over_${LUT}[${LUT}];

$VMLAQ_F32 = "vfmaq_f32" if FMA else "vmlaq_f32"
$VMLSQ_F32 = "vfmsq_f32" if FMA else "vmlsq_f32"
$LUT_SUFFIX = "" if LOG2LUT == 0 else "lut%d_" % LUT
$POLY_SUFFIX = "p%dh%d%s" % (P, H, "ps" if PS else "ts")
$DIV_SUFFIX = DIV.lower()
$ISA = "neonfma" if DIV == "DIV" else "neonfma" if FMA else "neon"
$PARAMS_STRUCT = "neon_expm1minus_rr1_" + LUT_SUFFIX + ("p%dh%d" % (P, H))
$ISA_PREFIX = "aarch64_" if DIV == "DIV" else ""
$ABC = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"

void xnn_f32_vtanh_ukernel__${ISA_PREFIX}${ISA}_expm1minus_rr1_${LUT_SUFFIX}${POLY_SUFFIX}_${DIV_SUFFIX}_u${BATCH_TILE}(
    size_t batch,
    const float* input,
    float* output,
    const union xnn_f32_tanh_params params[restrict XNN_MIN_ELEMENTS(1)]) XNN_OOB_READS
{
  const float32x4_t vsat_cutoff = vld1q_dup_f32(&params->${PARAMS_STRUCT}.sat_cutoff);
  const float32x4_t vminus_log2e = vld1q_dup_f32(&params->${PARAMS_STRUCT}.minus_log2e);

  const float32x4_t vmagic_bias = vld1q_dup_f32(&params->${PARAMS_STRUCT}.magic_bias);

  $if LOG2LUT != 0:
    const uint64x2_t vindex_mask = vreinterpretq_u64_u32(vmovq_n_u32(UINT32_C(0x${"%X" % (LUT-1)})));
  const float32x4_t vln2 = vld1q_dup_f32(&params->${PARAMS_STRUCT}.ln2);

  $for i in reversed(range(2, P+1)):
    const float32x4_t vc${i} = vld1q_dup_f32(&params->${PARAMS_STRUCT}.c${i});

  const float32x4_t vone = vmovq_n_f32(1.0f);
  const float32x4_t vtwo = vmovq_n_f32(2.0f);

  const uint32x4_t vsign_mask = vmovq_n_u32(UINT32_C(0x80000000));

  $if BATCH_TILE > 4:
    for (; batch >= ${BATCH_TILE} * sizeof(float); batch -= ${BATCH_TILE} * sizeof(float)) {
      $for N in range(0, BATCH_TILE, 4):
        const float32x4_t vx${ABC[N:N+4]} = vld1q_f32(input); input += 4;

      $for N in range(0, BATCH_TILE, 4):
        float32x4_t vz${ABC[N:N+4]} = vabsq_f32(vx${ABC[N:N+4]});
      $for N in range(0, BATCH_TILE, 4):
        vz${ABC[N:N+4]} = vminq_f32(vz${ABC[N:N+4]}, vsat_cutoff);

      $for N in range(0, BATCH_TILE, 4):
        float32x4_t vn${ABC[N:N+4]} = ${VMLAQ_F32}(vmagic_bias, vz${ABC[N:N+4]}, vminus_log2e);

      $if LOG2LUT == 0:
        $for N in range(0, BATCH_TILE, 4):
          const float32x4_t vs${ABC[N:N+4]} = vreinterpretq_f32_s32(vshlq_n_s32(vreinterpretq_s32_f32(vn${ABC[N:N+4]}), 23));
      $else:
        $for N in range(0, BATCH_TILE, 4):
          const uint32x4_t ve${ABC[N:N+4]} = vshlq_n_u32(vreinterpretq_u32_f32(vn${ABC[N:N+4]}), ${23-LOG2LUT});
          const uint64x2_t vidx${ABC[N:N+4]} = vandq_u64(vreinterpretq_u64_f32(vn${ABC[N:N+4]}), vindex_mask);
        $for N in range(0, BATCH_TILE, 4):
          const uint64_t vidx${ABC[N:N+2]} = vgetq_lane_u64(vidx${ABC[N:N+4]}, 0);
        $for N in range(0, BATCH_TILE, 4):
          uint32x2_t vl${ABC[N:N+2]} = vld1_dup_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N:N+2]}]);
          const uint64_t vidx${ABC[N+2:N+4]} = vgetq_lane_u64(vidx${ABC[N:N+4]}, 1);
        $for N in range(0, BATCH_TILE, 4):
          uint32x2_t vl${ABC[N+2:N+4]} = vld1_dup_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+2:N+4]}]);
        $for N in range(0, BATCH_TILE, 4):
          vl${ABC[N:N+2]} = vld1_lane_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N:N+2]} >> 32)], vl${ABC[N:N+2]}, 1);
          vl${ABC[N+2:N+4]} = vld1_lane_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N+2:N+4]} >> 32)], vl${ABC[N+2:N+4]}, 1);
        $for N in range(0, BATCH_TILE, 4):
          const uint32x4_t vl${ABC[N:N+4]} = vcombine_u32(vl${ABC[N:N+2]}, vl${ABC[N+2:N+4]});
        $for N in range(0, BATCH_TILE, 4):
          const float32x4_t vs${ABC[N:N+4]} = vreinterpretq_f32_u32(vaddq_u32(vl${ABC[N:N+4]}, ve${ABC[N:N+4]}));

      $for N in range(0, BATCH_TILE, 4):
        vn${ABC[N:N+4]} = vsubq_f32(vn${ABC[N:N+4]}, vmagic_bias);

      $for N in range(0, BATCH_TILE, 4):
        const float32x4_t vt${ABC[N:N+4]} = ${VMLAQ_F32}(vz${ABC[N:N+4]}, vn${ABC[N:N+4]}, vln2);

      $for N in range(0, BATCH_TILE, 4):
        float32x4_t vp${ABC[N:N+4]} = ${VMLAQ_F32}(vc${P-1}, vc${P}, vt${ABC[N:N+4]});
      $for N in range(0, BATCH_TILE, 4):
        $for i in reversed(range(2, P-1)):
          vp${ABC[N:N+4]} = ${VMLAQ_F32}(vc${i}, vp${ABC[N:N+4]}, vt${ABC[N:N+4]});
      $for N in range(0, BATCH_TILE, 4):
        vp${ABC[N:N+4]} = ${VMLSQ_F32}(vtwo, vp${ABC[N:N+4]}, vt${ABC[N:N+4]});

      $for N in range(0, BATCH_TILE, 4):
        const float32x4_t vts${ABC[N:N+4]} = vmulq_f32(vt${ABC[N:N+4]}, vs${ABC[N:N+4]});
        const float32x4_t vsmo${ABC[N:N+4]} = vsubq_f32(vs${ABC[N:N+4]}, vone);
      $for N in range(0, BATCH_TILE, 4):
        const float32x4_t vemo${ABC[N:N+4]} = ${VMLSQ_F32}(vsmo${ABC[N:N+4]}, vp${ABC[N:N+4]}, vts${ABC[N:N+4]});

      $for N in range(0, BATCH_TILE, 4):
        const float32x4_t vepo${ABC[N:N+4]} = vaddq_f32(vemo${ABC[N:N+4]}, vtwo);

      $if DIV == "DIV":
        $for N in range(0, BATCH_TILE, 4):
          float32x4_t vy${ABC[N:N+4]} = vdivq_f32(vemo${ABC[N:N+4]}, vepo${ABC[N:N+4]});
      $else:
        $for N in range(0, BATCH_TILE, 4):
          float32x4_t vrepo${ABC[N:N+4]} = vrecpeq_f32(vepo${ABC[N:N+4]});
        $if DIV in ["NR1RECPS1FMA", "NR2RECPS"]:
          $for N in range(0, BATCH_TILE, 4):
            float32x4_t verepo${ABC[N:N+4]} = vrecpsq_f32(vrepo${ABC[N:N+4]}, vepo${ABC[N:N+4]});
          $for N in range(0, BATCH_TILE, 4):
            vrepo${ABC[N:N+4]} = vmulq_f32(vrepo${ABC[N:N+4]}, verepo${ABC[N:N+4]});
        $else:
          $for N in range(0, BATCH_TILE, 4):
            float32x4_t verepo${ABC[N:N+4]} = vfmsq_f32(vone, vrepo${ABC[N:N+4]}, vepo${ABC[N:N+4]});
          $for N in range(0, BATCH_TILE, 4):
            vrepo${ABC[N:N+4]} = vfmaq_f32(vrepo${ABC[N:N+4]}, vrepo${ABC[N:N+4]}, verepo${ABC[N:N+4]});
        $if DIV == "NR2RECPS":
          $for N in range(0, BATCH_TILE, 4):
            verepo${ABC[N:N+4]} = vrecpsq_f32(vrepo${ABC[N:N+4]}, vepo${ABC[N:N+4]});
          $for N in range(0, BATCH_TILE, 4):
            vrepo${ABC[N:N+4]} = vmulq_f32(vrepo${ABC[N:N+4]}, verepo${ABC[N:N+4]});
        $else:
          $for N in range(0, BATCH_TILE, 4):
            verepo${ABC[N:N+4]} = vfmsq_f32(vone, vrepo${ABC[N:N+4]}, vepo${ABC[N:N+4]});
          $for N in range(0, BATCH_TILE, 4):
            vrepo${ABC[N:N+4]} = vfmaq_f32(vrepo${ABC[N:N+4]}, vrepo${ABC[N:N+4]}, verepo${ABC[N:N+4]});

        $for N in range(0, BATCH_TILE, 4):
          float32x4_t vy${ABC[N:N+4]} = vmulq_f32(vemo${ABC[N:N+4]}, vrepo${ABC[N:N+4]});

      $for N in range(0, BATCH_TILE, 4):
        vy${ABC[N:N+4]} = vbslq_f32(vsign_mask, vx${ABC[N:N+4]}, vy${ABC[N:N+4]});

      $for N in range(0, BATCH_TILE, 4):
        vst1q_f32(output, vy${ABC[N:N+4]}); output += 4;
    }

  for (; batch >= 4 * sizeof(float); batch -= 4 * sizeof(float)) {
    const float32x4_t vx = vld1q_f32(input); input += 4;

    float32x4_t vz = vabsq_f32(vx);
    vz = vminq_f32(vz, vsat_cutoff);

    float32x4_t vn = ${VMLAQ_F32}(vmagic_bias, vz, vminus_log2e);

    $if LOG2LUT == 0:
      const float32x4_t vs = vreinterpretq_f32_s32(vshlq_n_s32(vreinterpretq_s32_f32(vn), 23));
    $else:
      const uint32x4_t ve = vshlq_n_u32(vreinterpretq_u32_f32(vn), ${23-3});
      const uint64x2_t vidx = vandq_u64(vreinterpretq_u64_f32(vn), vindex_mask);
      const uint64_t vidx_lo = vgetq_lane_u64(vidx, 0);
      const uint64_t vidx_hi = vgetq_lane_u64(vidx, 1);
      uint32x2_t vl_lo = vld1_dup_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_lo]);
      uint32x2_t vl_hi = vld1_dup_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_hi]);
      vl_lo = vld1_lane_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_lo >> 32)], vl_lo, 1);
      vl_hi = vld1_lane_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_hi >> 32)], vl_hi, 1);
      const uint32x4_t vl = vcombine_u32(vl_lo, vl_hi);
      const float32x4_t vs = vreinterpretq_f32_u32(vaddq_u32(vl, ve));

    vn = vsubq_f32(vn, vmagic_bias);

    const float32x4_t vt = ${VMLAQ_F32}(vz, vn, vln2);

    float32x4_t vp = ${VMLAQ_F32}(vc${P-1}, vc${P}, vt);
    $for i in reversed(range(2, P-1)):
      vp = ${VMLAQ_F32}(vc${i}, vp, vt);
    vp = ${VMLSQ_F32}(vtwo, vp, vt);

    const float32x4_t vts = vmulq_f32(vt, vs);
    const float32x4_t vsmo = vsubq_f32(vs, vone);
    const float32x4_t vemo = ${VMLSQ_F32}(vsmo, vp, vts);

    const float32x4_t vepo = vaddq_f32(vemo, vtwo);

    $if DIV == "DIV":
      float32x4_t vy = vdivq_f32(vemo, vepo);
    $else:
      float32x4_t vrepo = vrecpeq_f32(vepo);
      $if DIV in ["NR1RECPS1FMA", "NR2RECPS"]:
        float32x4_t verepo = vrecpsq_f32(vrepo, vepo);
        vrepo = vmulq_f32(vrepo, verepo);
      $else:
        float32x4_t verepo = vfmsq_f32(vone, vrepo, vepo);
        vrepo = vfmaq_f32(vrepo, vrepo, verepo);
      $if DIV == "NR2RECPS":
        verepo = vrecpsq_f32(vrepo, vepo);
        vrepo = vmulq_f32(vrepo, verepo);
      $else:
        verepo = vfmsq_f32(vone, vrepo, vepo);
        vrepo = vfmaq_f32(vrepo, vrepo, verepo);

      float32x4_t vy = vmulq_f32(vemo, vrepo);

    vy = vbslq_f32(vsign_mask, vx, vy);
    vst1q_f32(output, vy); output += 4;
  }
  if XNN_UNLIKELY(batch != 0) {
    const float32x4_t vx = vld1q_f32(input);

    float32x4_t vz = vabsq_f32(vx);
    vz = vminq_f32(vz, vsat_cutoff);

    float32x4_t vn = ${VMLAQ_F32}(vmagic_bias, vz, vminus_log2e);

    $if LOG2LUT == 0:
      const float32x4_t vs = vreinterpretq_f32_s32(vshlq_n_s32(vreinterpretq_s32_f32(vn), 23));
    $else:
      const uint32x4_t ve = vshlq_n_u32(vreinterpretq_u32_f32(vn), ${23-3});
      const uint64x2_t vidx = vandq_u64(vreinterpretq_u64_f32(vn), vindex_mask);
      const uint64_t vidx_lo = vgetq_lane_u64(vidx, 0);
      const uint64_t vidx_hi = vgetq_lane_u64(vidx, 1);
      uint32x2_t vl_lo = vld1_dup_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_lo]);
      uint32x2_t vl_hi = vld1_dup_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_hi]);
      vl_lo = vld1_lane_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_lo >> 32)], vl_lo, 1);
      vl_hi = vld1_lane_u32(&xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_hi >> 32)], vl_hi, 1);
      const uint32x4_t vl = vcombine_u32(vl_lo, vl_hi);
      const float32x4_t vs = vreinterpretq_f32_u32(vaddq_u32(vl, ve));

    vn = vsubq_f32(vn, vmagic_bias);

    const float32x4_t vt = ${VMLAQ_F32}(vz, vn, vln2);

    float32x4_t vp = ${VMLAQ_F32}(vc${P-1}, vc${P}, vt);
    $for i in reversed(range(2, P-1)):
      vp = ${VMLAQ_F32}(vc${i}, vp, vt);
    vp = ${VMLSQ_F32}(vtwo, vp, vt);

    const float32x4_t vts = vmulq_f32(vt, vs);
    const float32x4_t vsmo = vsubq_f32(vs, vone);
    const float32x4_t vemo = ${VMLSQ_F32}(vsmo, vp, vts);

    const float32x4_t vepo = vaddq_f32(vemo, vtwo);

    $if DIV == "DIV":
      float32x4_t vy = vdivq_f32(vemo, vepo);
    $else:
      float32x4_t vrepo = vrecpeq_f32(vepo);
      $if DIV in ["NR1RECPS1FMA", "NR2RECPS"]:
        float32x4_t verepo = vrecpsq_f32(vrepo, vepo);
        vrepo = vmulq_f32(vrepo, verepo);
      $else:
        float32x4_t verepo = vfmsq_f32(vone, vrepo, vepo);
        vrepo = vfmaq_f32(vrepo, vrepo, verepo);
      $if DIV == "NR2RECPS":
        verepo = vrecpsq_f32(vrepo, vepo);
        vrepo = vmulq_f32(vrepo, verepo);
      $else:
        verepo = vfmsq_f32(vone, vrepo, vepo);
        vrepo = vfmaq_f32(vrepo, vrepo, verepo);

      float32x4_t vy = vmulq_f32(vemo, vrepo);

    vy = vbslq_f32(vsign_mask, vx, vy);

    float32x2_t vy_low = vget_low_f32(vy);

    if (batch & (2 * sizeof(float))) {
      vst1_f32(output, vy_low); output += 2;
      vy_low = vget_high_f32(vy);
    }
    if (batch & (1 * sizeof(float))) {
      vst1_lane_f32(output, vy_low, 0);
    }
  }
}
