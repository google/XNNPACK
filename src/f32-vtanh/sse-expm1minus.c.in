// Copyright 2023 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert (LOG2LUT, P, H) in [(0, 6, 5), (3, 4, 3)]
$assert not PS or (P, H) == (4, 3)
$assert DIV in ["DIV", "NR1", "NR2"]
$assert SAT in ["MINMAX", "SELECT"]
$assert BATCH_TILE % 4 == 0
$assert BATCH_TILE >= 4
$ABC = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
$LUT = 1 << LOG2LUT
#include <assert.h>
#include <stddef.h>
#include <stdint.h>

$if SSE == 4:
  #include <smmintrin.h>
$else:
  #include <emmintrin.h>

#include "xnnpack/common.h"
#include "xnnpack/microparams.h"
#include "xnnpack/vunary.h"

$if LOG2LUT != 0:

  // Table of exp2(k / ${LUT}) values decremented (as integer) by (k << ${23-LOG2LUT}), k = 0..${LUT-1}
  extern XNN_INTERNAL const uint32_t xnn_table_exp2minus_k_over_${LUT}[${LUT}];

$LUT_SUFFIX = "" if LOG2LUT == 0 else "lut%d_" % LUT
$POLY_SUFFIX = "p%dh%d%s" % (P, H, "ps" if PS else "ts")
$DIV_SUFFIX = DIV.lower()
$PARAMS_STRUCT = "sse_expm1minus_rr1_" + LUT_SUFFIX + ("p%dh%d" % (P, H))
$ISA = {2: "sse2", 4: "sse41"}[SSE]
void xnn_f32_vtanh_ukernel__${ISA}_expm1minus_rr1_${LUT_SUFFIX}${POLY_SUFFIX}_${DIV_SUFFIX}_u${BATCH_TILE}(
    size_t batch,
    const float* input,
    float* output,
    const union xnn_f32_tanh_params params[restrict XNN_MIN_ELEMENTS(1)]) XNN_OOB_READS
{
  assert(batch != 0);
  assert(batch % sizeof(float) == 0);
  assert(input != NULL);
  assert(output != NULL);

  const __m128 vsign_mask = _mm_load_ps(params->${PARAMS_STRUCT}.sign_mask);
  const __m128 vsat_cutoff = _mm_load_ps(params->${PARAMS_STRUCT}.sat_cutoff);
  const __m128 vlog2e = _mm_load_ps(params->${PARAMS_STRUCT}.log2e);
  const __m128 vmagic_bias = _mm_load_ps(params->${PARAMS_STRUCT}.magic_bias);
  $if LOG2LUT != 0:
    const __m128i vindex_mask = _mm_load_si128((const __m128i*) params->${PARAMS_STRUCT}.index_mask);
  const __m128 vminus_ln2 = _mm_load_ps(params->${PARAMS_STRUCT}.minus_ln2);
  $for i in reversed(range(2, P+1)):
    const __m128 vc${i} = _mm_load_ps(params->${PARAMS_STRUCT}.c${i});
  const __m128 vminus_two = _mm_load_ps(params->${PARAMS_STRUCT}.minus_two);
  const __m128 vminus_one = _mm_load_ps(params->${PARAMS_STRUCT}.minus_one);

  $if BATCH_TILE > 4:
    for (; batch >= ${BATCH_TILE} * sizeof(float); batch -= ${BATCH_TILE} * sizeof(float)) {
      const __m128 vx${ABC[0:4]} = _mm_loadu_ps(input);
      $for N in range(4, BATCH_TILE, 4):
        const __m128 vx${ABC[N:N+4]} = _mm_loadu_ps(input + ${N});
      input += ${BATCH_TILE};

      $for N in range(0, BATCH_TILE, 4):
        $if SAT == "MINMAX":
          __m128 vz${ABC[N:N+4]} = _mm_or_ps(vx${ABC[N:N+4]}, vsign_mask);
        $elif SAT == "SELECT":
          const __m128 vz${ABC[N:N+4]} = _mm_or_ps(vx${ABC[N:N+4]}, vsign_mask);

      $for N in range(0, BATCH_TILE, 4):
        const __m128 vinvsignx${ABC[N:N+4]} = _mm_xor_ps(vx${ABC[N:N+4]}, vz${ABC[N:N+4]});

      $for N in range(0, BATCH_TILE, 4):
        $if SAT == "MINMAX":
          vz${ABC[N:N+4]} = _mm_max_ps(vsat_cutoff, vz${ABC[N:N+4]});
        $elif SAT == "SELECT":
          const __m128 vm${ABC[N:N+4]} = _mm_cmple_ps(vz${ABC[N:N+4]}, vsat_cutoff);

      $for N in range(0, BATCH_TILE, 4):
        __m128 vn${ABC[N:N+4]} = _mm_add_ps(_mm_mul_ps(vz${ABC[N:N+4]}, vlog2e), vmagic_bias);

      $if LOG2LUT == 0:
        $for N in range(0, BATCH_TILE, 4):
          const __m128 vs${ABC[N:N+4]} = _mm_castsi128_ps(_mm_slli_epi32(_mm_castps_si128(vn${ABC[N:N+4]}), 23));
      $else:
        $for N in range(0, BATCH_TILE, 4):
          const __m128i ve${ABC[N:N+4]} = _mm_slli_epi32(_mm_castps_si128(vn${ABC[N:N+4]}), ${23-LOG2LUT});

        #if XNN_ARCH_X86_64
          $for N in range(0, BATCH_TILE, 4):
            __m128i vidx${ABC[N:N+4]} = _mm_and_si128(_mm_castps_si128(vn${ABC[N:N+4]}), vindex_mask);

          $for N in range(0, BATCH_TILE, 4):
            const uint64_t vidx${ABC[N:N+2]} = (uint64_t) _mm_cvtsi128_si64(vidx${ABC[N:N+4]});
            vidx${ABC[N:N+4]} = _mm_unpackhi_epi64(vidx${ABC[N:N+4]}, vidx${ABC[N:N+4]});

          $for N in range(0, BATCH_TILE, 4):
            const uint64_t vidx${ABC[N+2:N+4]} = (uint64_t) _mm_cvtsi128_si64(vidx${ABC[N:N+4]});

          $if SSE == 4:
            $for N in range(0, BATCH_TILE, 4):
              __m128i vl${ABC[N:N+4]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N:N+2]}]);
              vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N:N+2]} >> 32)], 1);
              vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+2:N+4]}], 2);
              vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N+2:N+4]} >> 32)], 3);
          $else:
            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N:N+2]}]);
              const __m128i vl${ABC[N+1]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N:N+2]} >> 32)]);

            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N:N+2]} = _mm_unpacklo_epi32(vl${ABC[N]}, vl${ABC[N+1]});

            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N+2]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+2:N+4]}]);
              const __m128i vl${ABC[N+3]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N+2:N+4]} >> 32)]);

            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N+2:N+4]} = _mm_unpacklo_epi32(vl${ABC[N+2]}, vl${ABC[N+3]});
        #else
          $for N in range(0, BATCH_TILE, 4):
            const __m128i vidx${ABC[N:N+4]} = _mm_and_si128(_mm_castps_si128(vn${ABC[N:N+4]}), vindex_mask);

          $for N in range(0, BATCH_TILE, 4):
            const uint32_t vidx${ABC[N]} = (uint32_t) _mm_cvtsi128_si32(vidx${ABC[N:N+4]});

          $if SSE == 4:
            $for N in range(0, BATCH_TILE, 4):
              __m128i vl${ABC[N:N+4]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx${ABC[N]}]);
          $else:
            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx${ABC[N]}]);

          $for N in range(0, BATCH_TILE, 4):
            const uint32_t vidx${ABC[N+1]} = (uint32_t) _mm_extract_epi16(vidx${ABC[N:N+4]}, 2);

          $if SSE == 4:
            $for N in range(0, BATCH_TILE, 4):
              vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[vidx${ABC[N+1]}], 1);
          $else:
            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N+1]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx${ABC[N+1]}]);

            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N:N+2]} = _mm_unpacklo_epi32(vl${ABC[N]}, vl${ABC[N+1]});

          $for N in range(0, BATCH_TILE, 4):
            const uint32_t vidx${ABC[N+2]} = (uint32_t) _mm_extract_epi16(vidx${ABC[N:N+4]}, 4);

          $if SSE == 4:
            $for N in range(0, BATCH_TILE, 4):
              vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[vidx${ABC[N+2]}], 2);
          $else:
            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N+2]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx${ABC[N+2]}]);

          $for N in range(0, BATCH_TILE, 4):
            const uint32_t vidx${ABC[N+3]} = (uint32_t) _mm_extract_epi16(vidx${ABC[N:N+4]}, 6);

          $if SSE == 4:
            $for N in range(0, BATCH_TILE, 4):
              vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[vidx${ABC[N+3]}], 3);
          $else:
            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N+3]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx${ABC[N+3]}]);

            $for N in range(0, BATCH_TILE, 4):
              const __m128i vl${ABC[N+2:N+4]} = _mm_unpacklo_epi32(vl${ABC[N+2]}, vl${ABC[N+3]});
        #endif
        $if SSE != 4:
          $for N in range(0, BATCH_TILE, 4):
            const __m128i vl${ABC[N:N+4]} = _mm_unpacklo_epi64(vl${ABC[N:N+2]}, vl${ABC[N+2:N+4]});

        $for N in range(0, BATCH_TILE, 4):
          const __m128 vs${ABC[N:N+4]} = _mm_castsi128_ps(_mm_add_epi32(vl${ABC[N:N+4]}, ve${ABC[N:N+4]}));

      $for N in range(0, BATCH_TILE, 4):
        vn${ABC[N:N+4]} = _mm_sub_ps(vn${ABC[N:N+4]}, vmagic_bias);

      $for N in range(0, BATCH_TILE, 4):
        const __m128 vt${ABC[N:N+4]} = _mm_add_ps(_mm_mul_ps(vn${ABC[N:N+4]}, vminus_ln2), vz${ABC[N:N+4]});

      $for N in range(0, BATCH_TILE, 4):
        __m128 vp${ABC[N:N+4]} = _mm_add_ps(_mm_mul_ps(vc${P}, vt${ABC[N:N+4]}), vc${P-1});
      $for i in reversed(range(2, P - 1)):
        $for N in range(0, BATCH_TILE, 4):
          vp${ABC[N:N+4]} = _mm_add_ps(_mm_mul_ps(vp${ABC[N:N+4]}, vt${ABC[N:N+4]}), vc${i});
      $for N in range(0, BATCH_TILE, 4):
        vp${ABC[N:N+4]} = _mm_sub_ps(_mm_mul_ps(vp${ABC[N:N+4]}, vt${ABC[N:N+4]}), vminus_two);

      $for N in range(0, BATCH_TILE, 4):
        const __m128 vts${ABC[N:N+4]} = _mm_mul_ps(vt${ABC[N:N+4]}, vs${ABC[N:N+4]});
        const __m128 vsmo${ABC[N:N+4]} = _mm_add_ps(vs${ABC[N:N+4]}, vminus_one);
      $for N in range(0, BATCH_TILE, 4):
        const __m128 vemo${ABC[N:N+4]} = _mm_add_ps(_mm_mul_ps(vp${ABC[N:N+4]}, vts${ABC[N:N+4]}), vsmo${ABC[N:N+4]});

      $for N in range(0, BATCH_TILE, 4):
        $if DIV == "DIV":
          const __m128 vepo${ABC[N:N+4]} = _mm_sub_ps(vemo${ABC[N:N+4]}, vminus_two);
        $else:
          const __m128 vepo${ABC[N:N+4]} = _mm_sub_ps(vminus_two, vemo${ABC[N:N+4]});

      $if DIV == "DIV":
        $for N in range(0, BATCH_TILE, 4):
          __m128 vy${ABC[N:N+4]} = _mm_div_ps(vemo${ABC[N:N+4]}, vepo${ABC[N:N+4]});
      $else:
        $for N in range(0, BATCH_TILE, 4):
          __m128 vrepo${ABC[N:N+4]} = _mm_rcp_ps(vepo${ABC[N:N+4]});
        $for N in range(0, BATCH_TILE, 4):
          vrepo${ABC[N:N+4]} = _mm_mul_ps(vrepo${ABC[N:N+4]}, _mm_add_ps(_mm_mul_ps(vrepo${ABC[N:N+4]}, vepo${ABC[N:N+4]}), vminus_two));
        $if DIV == "NR2":
          $for N in range(0, BATCH_TILE, 4):
            vrepo${ABC[N:N+4]} = _mm_mul_ps(vrepo${ABC[N:N+4]}, _mm_sub_ps(_mm_mul_ps(vrepo${ABC[N:N+4]}, vepo${ABC[N:N+4]}), vminus_two));

        $for N in range(0, BATCH_TILE, 4):
          __m128 vy${ABC[N:N+4]} = _mm_mul_ps(vemo${ABC[N:N+4]}, vrepo${ABC[N:N+4]});

      $if SAT == "SELECT":
        $for N in range(0, BATCH_TILE, 4):
          $if SSE == 4:
            vy${ABC[N:N+4]} = _mm_blendv_ps(vy${ABC[N:N+4]}, vminus_one, vm${ABC[N:N+4]});
          $else:
            vy${ABC[N:N+4]} = _mm_or_ps(_mm_andnot_ps(vm${ABC[N:N+4]}, vy${ABC[N:N+4]}), _mm_and_ps(vminus_one, vm${ABC[N:N+4]}));

      $for N in range(0, BATCH_TILE, 4):
        vy${ABC[N:N+4]} = _mm_xor_ps(vy${ABC[N:N+4]}, vinvsignx${ABC[N:N+4]});

      _mm_storeu_ps(output, vy${ABC[0:4]});
      $for N in range(4, BATCH_TILE, 4):
        _mm_storeu_ps(output + ${N}, vy${ABC[N:N+4]});
      output += ${BATCH_TILE};
    }
  for (; batch >= 4 * sizeof(float); batch -= 4 * sizeof(float)) {
    const __m128 vx = _mm_loadu_ps(input);
    input += 4;

    $if SAT == "MINMAX":
      __m128 vz = _mm_or_ps(vx, vsign_mask);
    $elif SAT == "SELECT":
      const __m128 vz = _mm_or_ps(vx, vsign_mask);

    const __m128 vinvsignx = _mm_xor_ps(vx, vz);

    $if SAT == "MINMAX":
      vz = _mm_max_ps(vsat_cutoff, vz);
    $elif SAT == "SELECT":
      const __m128 vm = _mm_cmple_ps(vz, vsat_cutoff);

    __m128 vn = _mm_add_ps(_mm_mul_ps(vz, vlog2e), vmagic_bias);

    $if LOG2LUT == 0:
      const __m128 vs = _mm_castsi128_ps(_mm_slli_epi32(_mm_castps_si128(vn), 23));
    $else:
      const __m128i ve = _mm_slli_epi32(_mm_castps_si128(vn), ${23-LOG2LUT});

      #if XNN_ARCH_X86_64
        __m128i vidx = _mm_and_si128(_mm_castps_si128(vn), vindex_mask);
        const uint64_t vidx_lo = (uint64_t) _mm_cvtsi128_si64(vidx);
        vidx = _mm_unpackhi_epi64(vidx, vidx);
        const uint64_t vidx_hi = (uint64_t) _mm_cvtsi128_si64(vidx);
        $if SSE == 4:
          __m128i vl = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_lo]);
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_lo >> 32)], 1);
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_hi], 2);
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_hi >> 32)], 3);
        $else:
          const __m128i vl0 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_lo]);
          const __m128i vl1 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_lo >> 32)]);
          const __m128i vl_lo = _mm_unpacklo_epi32(vl0, vl1);
          const __m128i vl2 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_hi]);
          const __m128i vl3 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_hi >> 32)]);
          const __m128i vl_hi = _mm_unpacklo_epi32(vl2, vl3);
      #else
        const __m128i vidx = _mm_and_si128(_mm_castps_si128(vn), vindex_mask);
        const uint32_t vidx0 = (uint32_t) _mm_cvtsi128_si32(vidx);
        $if SSE == 4:
          __m128i vl = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx0]);
        $else:
          const __m128i vl0 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx0]);
        const uint32_t vidx1 = (uint32_t) _mm_extract_epi16(vidx, 2);
        $if SSE == 4:
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[vidx1], 1);
        $else:
          const __m128i vl1 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx1]);
          const __m128i vl_lo = _mm_unpacklo_epi32(vl0, vl1);
        const uint32_t vidx2 = (uint32_t) _mm_extract_epi16(vidx, 4);
        $if SSE == 4:
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[vidx2], 2);
        $else:
          const __m128i vl2 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx2]);
        const uint32_t vidx3 = (uint32_t) _mm_extract_epi16(vidx, 6);
        $if SSE == 4:
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[vidx3], 3);
        $else:
          const __m128i vl3 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx3]);
          const __m128i vl_hi = _mm_unpacklo_epi32(vl2, vl3);
      #endif
      $if SSE != 4:
        const __m128i vl = _mm_unpacklo_epi64(vl_lo, vl_hi);

      const __m128 vs = _mm_castsi128_ps(_mm_add_epi32(vl, ve));

    vn = _mm_sub_ps(vn, vmagic_bias);

    const __m128 vt = _mm_add_ps(_mm_mul_ps(vn, vminus_ln2), vz);

    __m128 vp = _mm_add_ps(_mm_mul_ps(vc${P}, vt), vc${P-1});
    $for i in reversed(range(2, P - 1)):
      vp = _mm_add_ps(_mm_mul_ps(vp, vt), vc${i});
    vp = _mm_sub_ps(_mm_mul_ps(vp, vt), vminus_two);

    const __m128 vts = _mm_mul_ps(vt, vs);
    const __m128 vsmo = _mm_add_ps(vs, vminus_one);
    const __m128 vemo = _mm_add_ps(_mm_mul_ps(vp, vts), vsmo);

    $if DIV == "DIV":
      const __m128 vepo = _mm_sub_ps(vemo, vminus_two);
    $else:
      const __m128 vepo = _mm_sub_ps(vminus_two, vemo);

    $if DIV == "DIV":
      __m128 vy = _mm_div_ps(vemo, vepo);
    $else:
      __m128 vrepo = _mm_rcp_ps(vepo);
      vrepo = _mm_mul_ps(vrepo, _mm_add_ps(_mm_mul_ps(vrepo, vepo), vminus_two));
      $if DIV == "NR2":
        vrepo = _mm_mul_ps(vrepo, _mm_sub_ps(_mm_mul_ps(vrepo, vepo), vminus_two));

      __m128 vy = _mm_mul_ps(vemo, vrepo);

    $if SAT == "SELECT":
      $if SSE == 4:
        vy = _mm_blendv_ps(vy, vminus_one, vm);
      $else:
        vy = _mm_or_ps(_mm_andnot_ps(vm, vy), _mm_and_ps(vminus_one, vm));

    vy = _mm_xor_ps(vy, vinvsignx);

    _mm_storeu_ps(output, vy);
    output += 4;
  }
  if XNN_UNLIKELY(batch != 0) {
    const __m128 vx = _mm_loadu_ps(input);

    $if SAT == "MINMAX":
      __m128 vz = _mm_or_ps(vx, vsign_mask);
    $elif SAT == "SELECT":
      const __m128 vz = _mm_or_ps(vx, vsign_mask);

    const __m128 vinvsignx = _mm_xor_ps(vx, vz);

    $if SAT == "MINMAX":
      vz = _mm_max_ps(vsat_cutoff, vz);
    $elif SAT == "SELECT":
      const __m128 vm = _mm_cmple_ps(vz, vsat_cutoff);

    __m128 vn = _mm_add_ps(_mm_mul_ps(vz, vlog2e), vmagic_bias);

    $if LOG2LUT == 0:
      const __m128 vs = _mm_castsi128_ps(_mm_slli_epi32(_mm_castps_si128(vn), 23));
    $else:
      const __m128i ve = _mm_slli_epi32(_mm_castps_si128(vn), ${23-LOG2LUT});

      #if XNN_ARCH_X86_64
        __m128i vidx = _mm_and_si128(_mm_castps_si128(vn), vindex_mask);
        const uint64_t vidx_lo = (uint64_t) _mm_cvtsi128_si64(vidx);
        vidx = _mm_unpackhi_epi64(vidx, vidx);
        const uint64_t vidx_hi = (uint64_t) _mm_cvtsi128_si64(vidx);
        $if SSE == 4:
          __m128i vl = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_lo]);
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_lo >> 32)], 1);
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_hi], 2);
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_hi >> 32)], 3);
        $else:
          const __m128i vl0 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_lo]);
          const __m128i vl1 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_lo >> 32)]);
          const __m128i vl_lo = _mm_unpacklo_epi32(vl0, vl1);
          const __m128i vl2 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx_hi]);
          const __m128i vl3 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx_hi >> 32)]);
          const __m128i vl_hi = _mm_unpacklo_epi32(vl2, vl3);
      #else
        const __m128i vidx = _mm_and_si128(_mm_castps_si128(vn), vindex_mask);
        const uint32_t vidx0 = (uint32_t) _mm_cvtsi128_si32(vidx);
        $if SSE == 4:
          __m128i vl = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx0]);
        $else:
          const __m128i vl0 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx0]);
        const uint32_t vidx1 = (uint32_t) _mm_extract_epi16(vidx, 2);
        $if SSE == 4:
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[vidx1], 1);
        $else:
          const __m128i vl1 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx1]);
          const __m128i vl_lo = _mm_unpacklo_epi32(vl0, vl1);
        const uint32_t vidx2 = (uint32_t) _mm_extract_epi16(vidx, 4);
        $if SSE == 4:
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[vidx2], 2);
        $else:
          const __m128i vl2 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx2]);
        const uint32_t vidx3 = (uint32_t) _mm_extract_epi16(vidx, 6);
        $if SSE == 4:
          vl = _mm_insert_epi32(vl, (int) xnn_table_exp2minus_k_over_${LUT}[vidx3], 3);
        $else:
          const __m128i vl3 = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[vidx3]);
          const __m128i vl_hi = _mm_unpacklo_epi32(vl2, vl3);
      #endif
      $if SSE != 4:
        const __m128i vl = _mm_unpacklo_epi64(vl_lo, vl_hi);

      const __m128 vs = _mm_castsi128_ps(_mm_add_epi32(vl, ve));

    vn = _mm_sub_ps(vn, vmagic_bias);

    const __m128 vt = _mm_add_ps(_mm_mul_ps(vn, vminus_ln2), vz);

    __m128 vp = _mm_add_ps(_mm_mul_ps(vc${P}, vt), vc${P-1});
    $for i in reversed(range(2, P - 1)):
      vp = _mm_add_ps(_mm_mul_ps(vp, vt), vc${i});
    vp = _mm_sub_ps(_mm_mul_ps(vp, vt), vminus_two);

    const __m128 vts = _mm_mul_ps(vt, vs);
    const __m128 vsmo = _mm_add_ps(vs, vminus_one);
    const __m128 vemo = _mm_add_ps(_mm_mul_ps(vp, vts), vsmo);

    $if DIV == "DIV":
      const __m128 vepo = _mm_sub_ps(vemo, vminus_two);
    $else:
      const __m128 vepo = _mm_sub_ps(vminus_two, vemo);

    $if DIV == "DIV":
      __m128 vy = _mm_div_ps(vemo, vepo);
    $else:
      __m128 vrepo = _mm_rcp_ps(vepo);
      vrepo = _mm_mul_ps(vrepo, _mm_add_ps(_mm_mul_ps(vrepo, vepo), vminus_two));
      $if DIV == "NR2":
        vrepo = _mm_mul_ps(vrepo, _mm_sub_ps(_mm_mul_ps(vrepo, vepo), vminus_two));

      __m128 vy = _mm_mul_ps(vemo, vrepo);

    $if SAT == "SELECT":
      $if SSE == 4:
        vy = _mm_blendv_ps(vy, vminus_one, vm);
      $else:
        vy = _mm_or_ps(_mm_andnot_ps(vm, vy), _mm_and_ps(vminus_one, vm));

    vy = _mm_xor_ps(vy, vinvsignx);

    if (batch & (2 * sizeof(float))) {
      _mm_storel_pi((__m64*) output, vy);
      vy = _mm_movehl_ps(vy, vy);
      output += 2;
    }
    if (batch & (1 * sizeof(float))) {
      _mm_store_ss(output, vy);
    }
  }
}
