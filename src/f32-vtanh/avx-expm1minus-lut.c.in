// Copyright 2023 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert (LOG2LUT, P, H) == (3, 4, 3)
$assert DIV in ["DIV", "NR1ADJ"]
$assert BATCH_TILE % 8 == 0
$assert BATCH_TILE >= 8
$ABC = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
$LUT = 1 << LOG2LUT
#include <assert.h>
#include <stddef.h>
#include <stdint.h>

#include <immintrin.h>

#include "xnnpack/common.h"
#include "xnnpack/microparams.h"
#include "xnnpack/vunary.h"

// Table of exp2(k / ${LUT}) values decremented (as integer) by (k << ${23-LOG2LUT}), k = 0..${LUT-1}
extern XNN_INTERNAL const uint32_t xnn_table_exp2minus_k_over_${LUT}[${LUT}];

$LUT_SUFFIX = "" if LOG2LUT == 0 else "lut%d_" % LUT
$POLY_SUFFIX = "p%dh%dts" % (P, H)
$DIV_SUFFIX = DIV.lower()
$PARAMS_STRUCT = "avx_expm1minus_rr1_" + LUT_SUFFIX + ("p%dh%d" % (P, H))
$ISA = "fma3" if FMA else "avx"
void xnn_f32_vtanh_ukernel__${ISA}_expm1minus_rr1_${LUT_SUFFIX}${POLY_SUFFIX}_${DIV_SUFFIX}_u${BATCH_TILE}(
    size_t batch,
    const float* input,
    float* output,
    const union xnn_f32_tanh_params params[restrict XNN_MIN_ELEMENTS(1)]) XNN_OOB_READS
{
  assert(batch != 0);
  assert(batch % sizeof(float) == 0);
  assert(input != NULL);
  assert(output != NULL);

  const __m256 vsign_mask = _mm256_load_ps(params->${PARAMS_STRUCT}.sign_mask);
  const __m256 vsat_cutoff = _mm256_load_ps(params->${PARAMS_STRUCT}.sat_cutoff);
  const __m256 vlog2e = _mm256_load_ps(params->${PARAMS_STRUCT}.log2e);
  const __m256 vmagic_bias = _mm256_load_ps(params->${PARAMS_STRUCT}.magic_bias);
  const __m128i vindex_mask = _mm_load_si128((const __m128i*) params->${PARAMS_STRUCT}.index_mask);
  const __m256 vminus_ln2 = _mm256_load_ps(params->${PARAMS_STRUCT}.minus_ln2);
  const __m256 vc4 = _mm256_load_ps(params->${PARAMS_STRUCT}.c4);
  const __m256 vc3 = _mm256_load_ps(params->${PARAMS_STRUCT}.c3);
  const __m256 vc2 = _mm256_load_ps(params->${PARAMS_STRUCT}.c2);
  const __m256 vtwo = _mm256_load_ps(params->${PARAMS_STRUCT}.two);
  const __m256 vminus_one = _mm256_load_ps(params->${PARAMS_STRUCT}.minus_one);

  $if BATCH_TILE > 8:
    for (; batch >= ${BATCH_TILE} * sizeof(float); batch -= ${BATCH_TILE} * sizeof(float)) {
      const __m256 vx${ABC[0:8]} = _mm256_loadu_ps(input);
      $for N in range(8, BATCH_TILE, 8):
        const __m256 vx${ABC[N:N+8]} = _mm256_loadu_ps(input + ${N});
      input += ${BATCH_TILE};

      $for N in range(0, BATCH_TILE, 8):
        __m256 vz${ABC[N:N+8]} = _mm256_or_ps(vx${ABC[N:N+8]}, vsign_mask);

      $for N in range(0, BATCH_TILE, 8):
        const __m256 vinvsignx${ABC[N:N+8]} = _mm256_xor_ps(vx${ABC[N:N+8]}, vz${ABC[N:N+8]});
        vz${ABC[N:N+8]} = _mm256_max_ps(vsat_cutoff, vz${ABC[N:N+8]});

      $for N in range(0, BATCH_TILE, 8):
        $if FMA == 3:
          __m256 vn${ABC[N:N+8]} = _mm256_fmadd_ps(vz${ABC[N:N+8]}, vlog2e, vmagic_bias);
        $else:
          __m256 vn${ABC[N:N+8]} = _mm256_add_ps(_mm256_mul_ps(vz${ABC[N:N+8]}, vlog2e), vmagic_bias);

      $for N in range(0, BATCH_TILE, 8):
        const __m128 vn${ABC[N+4:N+8]} = _mm256_extractf128_ps(vn${ABC[N:N+8]}, 1);
        const __m128i ve${ABC[N:N+4]} = _mm_slli_epi32(_mm_castps_si128(_mm256_castps256_ps128(vn${ABC[N:N+8]})), ${23-LOG2LUT});

      $for N in range(0, BATCH_TILE, 8):
        const __m128i ve${ABC[N+4:N+8]} = _mm_slli_epi32(_mm_castps_si128(vn${ABC[N+4:N+8]}), ${23-LOG2LUT});
        const __m128i vidx${ABC[N:N+4]} = _mm_and_si128(_mm_castps_si128(_mm256_castps256_ps128(vn${ABC[N:N+8]})), vindex_mask);
        const __m128i vidx${ABC[N+4:N+8]} = _mm_and_si128(_mm_castps_si128(vn${ABC[N+4:N+8]}), vindex_mask);

      #if XNN_ARCH_X86_64
        $for N in range(0, BATCH_TILE, 8):
          const uint64_t vidx${ABC[N:N+2]} = (uint64_t) _mm_cvtsi128_si64(vidx${ABC[N:N+4]});
          const uint64_t vidx${ABC[N+4:N+6]} = (uint64_t) _mm_cvtsi128_si64(vidx${ABC[N+4:N+8]});

        $for N in range(0, BATCH_TILE, 8):
          __m128i vl${ABC[N:N+4]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N:N+2]}]);
          __m128i vl${ABC[N+4:N+8]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+4:N+6]}]);

        $for N in range(0, BATCH_TILE, 8):
          vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N:N+2]} >> 32)], 1);
          vl${ABC[N+4:N+8]} = _mm_insert_epi32(vl${ABC[N+4:N+8]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N+4:N+6]} >> 32)], 1);
          const uint64_t vidx${ABC[N+2:N+4]} = (uint64_t) _mm_extract_epi64(vidx${ABC[N:N+4]}, 1);
          const uint64_t vidx${ABC[N+6:N+8]} = (uint64_t) _mm_extract_epi64(vidx${ABC[N+4:N+8]}, 1);

        $for N in range(0, BATCH_TILE, 8):
          vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+2:N+4]}], 2);
          vl${ABC[N+4:N+8]} = _mm_insert_epi32(vl${ABC[N+4:N+8]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+6:N+8]}], 2);

        $for N in range(0, BATCH_TILE, 8):
          vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N+2:N+4]} >> 32)], 3);
          vl${ABC[N+4:N+8]} = _mm_insert_epi32(vl${ABC[N+4:N+8]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx${ABC[N+6:N+8]} >> 32)], 3);
      #else
        $for N in range(0, BATCH_TILE, 8):
          const uint32_t vidx${ABC[N]} = (uint32_t) _mm_cvtsi128_si32(vidx${ABC[N:N+4]});
          const uint32_t vidx${ABC[N+4]} = (uint32_t) _mm_cvtsi128_si32(vidx${ABC[N+4:N+8]});

        $for N in range(0, BATCH_TILE, 8):
          __m128i vl${ABC[N:N+4]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N]}]);
          __m128i vl${ABC[N+4:N+8]} = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+4]}]);
          const uint32_t vidx${ABC[N+1]} = (uint32_t) _mm_extract_epi32(vidx${ABC[N:N+4]}, 1);
          const uint32_t vidx${ABC[N+5]} = (uint32_t) _mm_extract_epi32(vidx${ABC[N+4:N+8]}, 1);

        $for N in range(0, BATCH_TILE, 8):
          vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+1]}], 1);
          vl${ABC[N+4:N+8]} = _mm_insert_epi32(vl${ABC[N+4:N+8]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+5]}], 1);
          const uint32_t vidx${ABC[N+2]} = (uint32_t) _mm_extract_epi32(vidx${ABC[N:N+4]}, 2);
          const uint32_t vidx${ABC[N+6]} = (uint32_t) _mm_extract_epi32(vidx${ABC[N+4:N+8]}, 2);

        $for N in range(0, BATCH_TILE, 8):
          vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+2]}], 2);
          vl${ABC[N+4:N+8]} = _mm_insert_epi32(vl${ABC[N+4:N+8]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+6]}], 2);
          const uint32_t vidx${ABC[N+3]} = (uint32_t) _mm_extract_epi32(vidx${ABC[N:N+4]}, 3);
          const uint32_t vidx${ABC[N+7]} = (uint32_t) _mm_extract_epi32(vidx${ABC[N+4:N+8]}, 3);

        $for N in range(0, BATCH_TILE, 8):
          vl${ABC[N:N+4]} = _mm_insert_epi32(vl${ABC[N:N+4]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+3]}], 3);
          vl${ABC[N+4:N+8]} = _mm_insert_epi32(vl${ABC[N+4:N+8]}, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx${ABC[N+7]}], 3);
      #endif

      $for N in range(0, BATCH_TILE, 8):
        const __m128 vs${ABC[N:N+4]} = _mm_castsi128_ps(_mm_add_epi32(vl${ABC[N:N+4]}, ve${ABC[N:N+4]}));
        const __m128 vs${ABC[N+4:N+8]} = _mm_castsi128_ps(_mm_add_epi32(vl${ABC[N+4:N+8]}, ve${ABC[N+4:N+8]}));

      $for N in range(0, BATCH_TILE, 8):
        const __m256 vs${ABC[N:N+8]} = _mm256_insertf128_ps(_mm256_castps128_ps256(vs${ABC[N:N+4]}), vs${ABC[N+4:N+8]}, 1);
        vn${ABC[N:N+8]} = _mm256_sub_ps(vn${ABC[N:N+8]}, vmagic_bias);

      $for N in range(0, BATCH_TILE, 8):
        $if FMA == 3:
          const __m256 vt${ABC[N:N+8]} = _mm256_fmadd_ps(vn${ABC[N:N+8]}, vminus_ln2, vz${ABC[N:N+8]});
        $else:
          const __m256 vt${ABC[N:N+8]} = _mm256_add_ps(_mm256_mul_ps(vn${ABC[N:N+8]}, vminus_ln2), vz${ABC[N:N+8]});

      $if FMA == 3:
        $for N in range(0, BATCH_TILE, 8):
          __m256 vp${ABC[N:N+8]} = vc${P};
        $for i in reversed(range(2, P)):
          $for N in range(0, BATCH_TILE, 8):
            vp${ABC[N:N+8]} = _mm256_fmadd_ps(vp${ABC[N:N+8]}, vt${ABC[N:N+8]}, vc${i});
      $else:
        $for N in range(0, BATCH_TILE, 8):
          __m256 vp${ABC[N:N+8]} = _mm256_add_ps(_mm256_mul_ps(vc${P}, vt${ABC[N:N+8]}), vc${P-1});
        $for i in reversed(range(2, P-1)):
          $for N in range(0, BATCH_TILE, 8):
            vp${ABC[N:N+8]} = _mm256_add_ps(_mm256_mul_ps(vp${ABC[N:N+8]}, vt${ABC[N:N+8]}), vc${i});
      $for N in range(0, BATCH_TILE, 8):
        $if FMA == 3:
          vp${ABC[N:N+8]} = _mm256_fmadd_ps(vp${ABC[N:N+8]}, vt${ABC[N:N+8]}, vtwo);
        $else:
          vp${ABC[N:N+8]} = _mm256_add_ps(_mm256_mul_ps(vp${ABC[N:N+8]}, vt${ABC[N:N+8]}), vtwo);

      $for N in range(0, BATCH_TILE, 8):
        const __m256 vts${ABC[N:N+8]} = _mm256_mul_ps(vt${ABC[N:N+8]}, vs${ABC[N:N+8]});
        const __m256 vsmo${ABC[N:N+8]} = _mm256_add_ps(vs${ABC[N:N+8]}, vminus_one);
      $for N in range(0, BATCH_TILE, 8):
        $if FMA == 3:
          const __m256 vemo${ABC[N:N+8]} = _mm256_fmadd_ps(vp${ABC[N:N+8]}, vts${ABC[N:N+8]}, vsmo${ABC[N:N+8]});
        $else:
          const __m256 vemo${ABC[N:N+8]} = _mm256_add_ps(_mm256_mul_ps(vp${ABC[N:N+8]}, vts${ABC[N:N+8]}), vsmo${ABC[N:N+8]});
      $for N in range(0, BATCH_TILE, 8):
        const __m256 vepo${ABC[N:N+8]} = _mm256_add_ps(vemo${ABC[N:N+8]}, vtwo);

      $if DIV == "DIV":
        $for N in range(0, BATCH_TILE, 8):
          __m256 vy${ABC[N:N+8]} = _mm256_div_ps(vemo${ABC[N:N+8]}, vepo${ABC[N:N+8]});
      $else:
        $for N in range(0, BATCH_TILE, 8):
          __m256 vrepo${ABC[N:N+8]} = _mm256_rcp_ps(vepo${ABC[N:N+8]});

        $for N in range(0, BATCH_TILE, 8):
          const __m256 verepo${ABC[N:N+8]} = _mm256_fnmsub_ps(vrepo${ABC[N:N+8]}, vepo${ABC[N:N+8]}, vminus_one);
        $for N in range(0, BATCH_TILE, 8):
          vrepo${ABC[N:N+8]} = _mm256_fmadd_ps(verepo${ABC[N:N+8]}, vrepo${ABC[N:N+8]}, vrepo${ABC[N:N+8]});

        $for N in range(0, BATCH_TILE, 8):
          __m256 vy${ABC[N:N+8]} = _mm256_mul_ps(vemo${ABC[N:N+8]}, vrepo${ABC[N:N+8]});

        $if DIV.endswith("ADJ"):
          $for N in range(0, BATCH_TILE, 8):
            const __m256 vey${ABC[N:N+8]} = _mm256_fnmadd_ps(vy${ABC[N:N+8]}, vepo${ABC[N:N+8]}, vemo${ABC[N:N+8]});
          $for N in range(0, BATCH_TILE, 8):
            vy${ABC[N:N+8]} = _mm256_fmadd_ps(vey${ABC[N:N+8]}, vrepo${ABC[N:N+8]}, vy${ABC[N:N+8]});

      $for N in range(0, BATCH_TILE, 8):
        vy${ABC[N:N+8]} = _mm256_xor_ps(vy${ABC[N:N+8]}, vinvsignx${ABC[N:N+8]});

      _mm256_storeu_ps(output, vy${ABC[0:8]});
      $for N in range(8, BATCH_TILE, 8):
        _mm256_storeu_ps(output + ${N}, vy${ABC[N:N+8]});
      output += ${BATCH_TILE};
    }
  for (; batch >= 8 * sizeof(float); batch -= 8 * sizeof(float)) {
    const __m256 vx = _mm256_loadu_ps(input);
    input += 8;

    __m256 vz = _mm256_or_ps(vx, vsign_mask);
    const __m256 vinvsignx = _mm256_xor_ps(vx, vz);
    vz = _mm256_max_ps(vsat_cutoff, vz);

    $if FMA == 3:
      __m256 vn = _mm256_fmadd_ps(vz, vlog2e, vmagic_bias);
    $else:
      __m256 vn = _mm256_add_ps(_mm256_mul_ps(vz, vlog2e), vmagic_bias);

    const __m128 vn_hi = _mm256_extractf128_ps(vn, 1);
    const __m128i ve_lo = _mm_slli_epi32(_mm_castps_si128(_mm256_castps256_ps128(vn)), ${23-LOG2LUT});
    const __m128i ve_hi = _mm_slli_epi32(_mm_castps_si128(vn_hi), ${23-LOG2LUT});

    const __m128i vidx_lo = _mm_and_si128(_mm_castps_si128(_mm256_castps256_ps128(vn)), vindex_mask);
    const __m128i vidx_hi = _mm_and_si128(_mm_castps_si128(vn_hi), vindex_mask);
    #if XNN_ARCH_X86_64
      const uint64_t vidx01 = (uint64_t) _mm_cvtsi128_si64(vidx_lo);
      const uint64_t vidx45 = (uint64_t) _mm_cvtsi128_si64(vidx_hi);
      __m128i vl_lo = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx01]);
      __m128i vl_hi = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx45]);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx01 >> 32)], 1);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx45 >> 32)], 1);
      const uint64_t vidx23 = (uint64_t) _mm_extract_epi64(vidx_lo, 1);
      const uint64_t vidx67 = (uint64_t) _mm_extract_epi64(vidx_hi, 1);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx23], 2);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx67], 2);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx23 >> 32)], 3);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx67 >> 32)], 3);
    #else
      const uint32_t vidx0 = (uint32_t) _mm_cvtsi128_si32(vidx_lo);
      const uint32_t vidx4 = (uint32_t) _mm_cvtsi128_si32(vidx_hi);
      __m128i vl_lo = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx0]);
      __m128i vl_hi = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx4]);
      const uint32_t vidx1 = (uint32_t) _mm_extract_epi32(vidx_lo, 1);
      const uint32_t vidx5 = (uint32_t) _mm_extract_epi32(vidx_hi, 1);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx1], 1);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx5], 1);
      const uint32_t vidx2 = (uint32_t) _mm_extract_epi32(vidx_lo, 2);
      const uint32_t vidx6 = (uint32_t) _mm_extract_epi32(vidx_hi, 2);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx2], 2);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx6], 2);
      const uint32_t vidx3 = (uint32_t) _mm_extract_epi32(vidx_lo, 3);
      const uint32_t vidx7 = (uint32_t) _mm_extract_epi32(vidx_hi, 3);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx3], 3);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx7], 3);
    #endif

    const __m128 vs_lo = _mm_castsi128_ps(_mm_add_epi32(vl_lo, ve_lo));
    const __m128 vs_hi = _mm_castsi128_ps(_mm_add_epi32(vl_hi, ve_hi));
    const __m256 vs = _mm256_insertf128_ps(_mm256_castps128_ps256(vs_lo), vs_hi, 1);

    vn = _mm256_sub_ps(vn, vmagic_bias);

    $if FMA == 3:
      const __m256 vt = _mm256_fmadd_ps(vn, vminus_ln2, vz);
    $else:
      const __m256 vt = _mm256_add_ps(_mm256_mul_ps(vn, vminus_ln2), vz);

    $if FMA == 3:
      __m256 vp = vc${P};
      $for i in reversed(range(2, P)):
        vp = _mm256_fmadd_ps(vp, vt, vc${i});
    $else:
      __m256 vp = _mm256_add_ps(_mm256_mul_ps(vc${P}, vt), vc${P-1});
      $for i in reversed(range(2, P-1)):
        vp = _mm256_add_ps(_mm256_mul_ps(vp, vt), vc${i});
    $if FMA == 3:
      vp = _mm256_fmadd_ps(vp, vt, vtwo);
    $else:
      vp = _mm256_add_ps(_mm256_mul_ps(vp, vt), vtwo);

    const __m256 vts = _mm256_mul_ps(vt, vs);
    const __m256 vsmo = _mm256_add_ps(vs, vminus_one);
    $if FMA == 3:
      const __m256 vemo = _mm256_fmadd_ps(vp, vts, vsmo);
    $else:
      const __m256 vemo = _mm256_add_ps(_mm256_mul_ps(vp, vts), vsmo);
    const __m256 vepo = _mm256_add_ps(vemo, vtwo);

    $if DIV == "DIV":
      __m256 vy = _mm256_div_ps(vemo, vepo);
    $else:
      __m256 vrepo = _mm256_rcp_ps(vepo);
      const __m256 verepo = _mm256_fnmsub_ps(vrepo, vepo, vminus_one);
      vrepo = _mm256_fmadd_ps(verepo, vrepo, vrepo);
      __m256 vy = _mm256_mul_ps(vemo, vrepo);
      $if DIV.endswith("ADJ"):
        const __m256 vey = _mm256_fnmadd_ps(vy, vepo, vemo);
        vy = _mm256_fmadd_ps(vey, vrepo, vy);

    vy = _mm256_xor_ps(vy, vinvsignx);

    _mm256_storeu_ps(output, vy);
    output += 8;
  }
  if XNN_UNLIKELY(batch != 0) {
    assert(batch >= 1 * sizeof(float));
    assert(batch <= 7 * sizeof(float));
    const __m256i vmask = _mm256_loadu_si256((const __m256i*) ((uintptr_t) &params->${PARAMS_STRUCT}.mask_table[7] - batch));

    const __m256 vx = _mm256_maskload_ps(input, vmask);

    __m256 vz = _mm256_or_ps(vx, vsign_mask);
    const __m256 vinvsignx = _mm256_xor_ps(vx, vz);
    vz = _mm256_max_ps(vsat_cutoff, vz);

    $if FMA == 3:
      __m256 vn = _mm256_fmadd_ps(vz, vlog2e, vmagic_bias);
    $else:
      __m256 vn = _mm256_add_ps(_mm256_mul_ps(vz, vlog2e), vmagic_bias);

    const __m128 vn_hi = _mm256_extractf128_ps(vn, 1);
    const __m128i ve_lo = _mm_slli_epi32(_mm_castps_si128(_mm256_castps256_ps128(vn)), ${23-LOG2LUT});
    const __m128i ve_hi = _mm_slli_epi32(_mm_castps_si128(vn_hi), ${23-LOG2LUT});

    const __m128i vidx_lo = _mm_and_si128(_mm_castps_si128(_mm256_castps256_ps128(vn)), vindex_mask);
    const __m128i vidx_hi = _mm_and_si128(_mm_castps_si128(vn_hi), vindex_mask);
    #if XNN_ARCH_X86_64
      const uint64_t vidx01 = (uint64_t) _mm_cvtsi128_si64(vidx_lo);
      const uint64_t vidx45 = (uint64_t) _mm_cvtsi128_si64(vidx_hi);
      __m128i vl_lo = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx01]);
      __m128i vl_hi = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx45]);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx01 >> 32)], 1);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx45 >> 32)], 1);
      const uint64_t vidx23 = (uint64_t) _mm_extract_epi64(vidx_lo, 1);
      const uint64_t vidx67 = (uint64_t) _mm_extract_epi64(vidx_hi, 1);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx23], 2);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx67], 2);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx23 >> 32)], 3);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx67 >> 32)], 3);
    #else
      const uint32_t vidx0 = (uint32_t) _mm_cvtsi128_si32(vidx_lo);
      const uint32_t vidx4 = (uint32_t) _mm_cvtsi128_si32(vidx_hi);
      __m128i vl_lo = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx0]);
      __m128i vl_hi = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx4]);
      const uint32_t vidx1 = (uint32_t) _mm_extract_epi32(vidx_lo, 1);
      const uint32_t vidx5 = (uint32_t) _mm_extract_epi32(vidx_hi, 1);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx1], 1);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx5], 1);
      const uint32_t vidx2 = (uint32_t) _mm_extract_epi32(vidx_lo, 2);
      const uint32_t vidx6 = (uint32_t) _mm_extract_epi32(vidx_hi, 2);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx2], 2);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx6], 2);
      const uint32_t vidx3 = (uint32_t) _mm_extract_epi32(vidx_lo, 3);
      const uint32_t vidx7 = (uint32_t) _mm_extract_epi32(vidx_hi, 3);
      vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx3], 3);
      vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx7], 3);
    #endif

    const __m128 vs_lo = _mm_castsi128_ps(_mm_add_epi32(vl_lo, ve_lo));
    const __m128 vs_hi = _mm_castsi128_ps(_mm_add_epi32(vl_hi, ve_hi));
    const __m256 vs = _mm256_insertf128_ps(_mm256_castps128_ps256(vs_lo), vs_hi, 1);

    vn = _mm256_sub_ps(vn, vmagic_bias);

    $if FMA == 3:
      const __m256 vt = _mm256_fmadd_ps(vn, vminus_ln2, vz);
    $else:
      const __m256 vt = _mm256_add_ps(_mm256_mul_ps(vn, vminus_ln2), vz);

    $if FMA == 3:
      __m256 vp = vc${P};
      $for i in reversed(range(2, P)):
        vp = _mm256_fmadd_ps(vp, vt, vc${i});
    $else:
      __m256 vp = _mm256_add_ps(_mm256_mul_ps(vc${P}, vt), vc${P-1});
      $for i in reversed(range(2, P-1)):
        vp = _mm256_add_ps(_mm256_mul_ps(vp, vt), vc${i});
    $if FMA == 3:
      vp = _mm256_fmadd_ps(vp, vt, vtwo);
    $else:
      vp = _mm256_add_ps(_mm256_mul_ps(vp, vt), vtwo);

    const __m256 vts = _mm256_mul_ps(vt, vs);
    const __m256 vsmo = _mm256_add_ps(vs, vminus_one);
    $if FMA == 3:
      const __m256 vemo = _mm256_fmadd_ps(vp, vts, vsmo);
    $else:
      const __m256 vemo = _mm256_add_ps(_mm256_mul_ps(vp, vts), vsmo);
    const __m256 vepo = _mm256_add_ps(vemo, vtwo);

    $if DIV == "DIV":
      __m256 vy = _mm256_div_ps(vemo, vepo);
    $else:
      __m256 vrepo = _mm256_rcp_ps(vepo);
      const __m256 verepo = _mm256_fnmsub_ps(vrepo, vepo, vminus_one);
      vrepo = _mm256_fmadd_ps(verepo, vrepo, vrepo);
      __m256 vy = _mm256_mul_ps(vemo, vrepo);
      $if DIV.endswith("ADJ"):
        const __m256 vey = _mm256_fnmadd_ps(vy, vepo, vemo);
        vy = _mm256_fmadd_ps(vey, vrepo, vy);

    vy = _mm256_xor_ps(vy, vinvsignx);

    __m128 vy_lo = _mm256_castps256_ps128(vy);
    if (batch & (4 * sizeof(float))) {
      _mm_storeu_ps(output, vy_lo);
      vy_lo = _mm256_extractf128_ps(vy, 1);
      output += 4;
    }
    if (batch & (2 * sizeof(float))) {
      _mm_storel_pi((__m64*) output, vy_lo);
      vy_lo = _mm_movehl_ps(vy_lo, vy_lo);
      output += 2;
    }
    if (batch & (1 * sizeof(float))) {
      _mm_store_ss(output, vy_lo);
    }
  }
}
