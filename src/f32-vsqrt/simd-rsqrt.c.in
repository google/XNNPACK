// Copyright 2025 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$BATCH_TILES = tuple(int(bt) for bt in BATCH_TILES.split(","))
$SIMD_SIZE = BATCH_TILES[0]
$ABC = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
#include <assert.h>
#include <stddef.h>
#include <stdint.h>
    
// Arch-specific SIMD wrapper.
#include "src/xnnpack/simd/f32-${ARCH}.h"

#include "src/xnnpack/common.h"
#include "src/xnnpack/microparams.h"
#include "src/xnnpack/vunary.h"


// In the following, we first compute the _reciprocal_ square root of an input
// `a` and then multiply it with `a` to produce the square root.
//
// We compute the reciprocal square root using a single Newton-Raphson step on
// the equation $x^{-2} - a$, which expands to:
//
//  $$x_{k+1} = x_k - 0.5 * x_k * (a * x_k^2 - 1.0)$$
//
// Note that we don't further simplify this expression, e.g. by factoring out
// `x_k`, so that the iteration _updates_ `x_k` in a numerically consistent way.
//
// So we do the following steps:
//
//  1. t0 = x_k
//  2. t1 = t0 * t0   (x_k^2)
//  3. t2 = a * t1    (a * x_k^2)
//  4. t3 = t2 - 1.0  (a * x_k^2 - 1.0)
//  5. t4 = 0.5 * t0  (0.5 * x_k)
//  6. t5 = t3 * t4   (0.5 * x_k * (a * x_k^2 - 1.0))
//  7. t6 = t0 - t5   (x_k - (0.5 * x_k * (a * x_k^2 - 1.0)))
//  7. y = a * t6     (a * a^{-1/2})
//
// Where $x_k$ is the original 12-bit (or 14-bit on AVX512f) approximation and
// `t6` contains the final 24-bit approximation $x_{k+1}$.
//
// In the implementation below, steps 3+4 and 6+7 could be merged into a single
// fused multiply-add/sub operation, but we chose not to as they are not
// performance critical and thus numerical consistency across microarchitectures
// is prefered.

$for BATCH_TILE in BATCH_TILES:
  $assert BATCH_TILE % SIMD_SIZE == 0
  $assert BATCH_TILE >= SIMD_SIZE
  $SIMD_TILE = BATCH_TILE // SIMD_SIZE

  void xnn_f32_vsqrt_ukernel__${ARCH}_rsqrt_u${BATCH_TILE}(
      size_t batch, const float* input, float* output,
      const struct xnn_f32_default_params* unused_params) {
    assert(batch != 0);
    assert(batch % sizeof(float) == 0);
    assert(input != NULL);
    assert(output != NULL);

    // Constants for the Newton-Raphson iteration.
    const xnn_simd_f32_t kOne = xnn_set1_f32(1.0f);
    const xnn_simd_f32_t kHalf = xnn_set1_f32(0.5f);

    $if BATCH_TILE > SIMD_SIZE:
      for (; batch >= ${BATCH_TILE} * sizeof(float); batch -= ${BATCH_TILE} * sizeof(float)) {
        const xnn_simd_f32_t vx${ABC[0]} = xnn_loadu_f32(input);
        $for N in range(1, SIMD_TILE):
          const xnn_simd_f32_t vx${ABC[N]} = xnn_loadu_f32(input + ${N * SIMD_SIZE});
        input += ${BATCH_TILE};

        // Create a mask of the +/-0 inputs, which will be flushed to zero later.
        $for N in range(SIMD_TILE):
          const xnn_simd_f32_t vinf_mask_${ABC[N]} = xnn_cmpneq_f32(vx${ABC[N]}, xnn_zero_f32());

        // Generate the initial 12-bit approximation.
        $for N in range(SIMD_TILE):
          xnn_simd_f32_t vt0_${ABC[N]} = xnn_rsqrt_f32(vx${ABC[N]});

        // Do a fixed number of Newton-Raphson steps as described above.
        for (size_t i = 0; i < XNN_SIMD_NUM_RSQRT_ITER_F32; i++) {
          $for N in range(SIMD_TILE):
            const xnn_simd_f32_t vt1_${ABC[N]} = xnn_mul_f32(vt0_${ABC[N]}, vt0_${ABC[N]});
          $for N in range(SIMD_TILE):
            const xnn_simd_f32_t vt2_${ABC[N]} = xnn_mul_f32(vx${ABC[N]}, vt1_${ABC[N]});
          $for N in range(SIMD_TILE):
            const xnn_simd_f32_t vt3_${ABC[N]} = xnn_sub_f32(vt2_${ABC[N]}, kOne);
          $for N in range(SIMD_TILE):
            const xnn_simd_f32_t vt4_${ABC[N]} = xnn_mul_f32(kHalf, vt0_${ABC[N]});
          $for N in range(SIMD_TILE):
            const xnn_simd_f32_t vt5_${ABC[N]} = xnn_mul_f32(vt3_${ABC[N]}, vt4_${ABC[N]});
          $for N in range(SIMD_TILE):
            vt0_${ABC[N]} = xnn_sub_f32(vt0_${ABC[N]}, vt5_${ABC[N]});
        }

        // Mask out the inputs that were zero.
        $for N in range(SIMD_TILE):
          const xnn_simd_f32_t vt7_${ABC[N]} = xnn_and_f32(vinf_mask_${ABC[N]}, vt0_${ABC[N]});

        // Extract the square root by multiplying the original value with the
        // inverse square root.
        $for N in range(SIMD_TILE):
          const xnn_simd_f32_t vy${ABC[N]} = xnn_mul_f32(vx${ABC[N]}, vt7_${ABC[N]});

        // Store the results.
        xnn_storeu_f32(output, vy${ABC[0]});
        $for N in range(1, SIMD_TILE):
          xnn_storeu_f32(output + ${N * SIMD_SIZE}, vy${ABC[N]});
        output += ${BATCH_TILE};
      }

    for (; batch >= xnn_simd_bytes_f32; batch -= xnn_simd_bytes_f32) {
      const xnn_simd_f32_t vx = xnn_loadu_f32(input);
      input += xnn_simd_size_f32;

      // Create a mask of the +/-0 inputs, which will be flushed to zero later.
      const xnn_simd_f32_t vinf_mask = xnn_cmpneq_f32(vx, xnn_zero_f32());

      // Generate the initial 12-bit approximation.
      xnn_simd_f32_t vt0 = xnn_rsqrt_f32(vx);

      // Do a fixed number of Newton-Raphson steps as described above.
      for (size_t i = 0; i < XNN_SIMD_NUM_RSQRT_ITER_F32; i++) {
        const xnn_simd_f32_t vt1 = xnn_mul_f32(vt0, vt0);
        const xnn_simd_f32_t vt2 = xnn_mul_f32(vx, vt1);
        const xnn_simd_f32_t vt3 = xnn_sub_f32(vt2, kOne);
        const xnn_simd_f32_t vt4 = xnn_mul_f32(kHalf, vt0);
        const xnn_simd_f32_t vt5 = xnn_mul_f32(vt3, vt4);
        vt0 = xnn_sub_f32(vt0, vt5);
      }

      // Mask out the inputs that were zero.
      const xnn_simd_f32_t vt7 = xnn_and_f32(vinf_mask, vt0);

      // Extract the square root by multiplying the original value with the
      // inverse square root.
      const xnn_simd_f32_t vy = xnn_mul_f32(vx, vt7);

      xnn_storeu_f32(output, vy);
      output += xnn_simd_size_f32;
    }

    if XNN_UNLIKELY(batch != 0) {
      assert(batch >= 1 * sizeof(float));
      assert(batch <= ${SIMD_SIZE - 1} * sizeof(float));
      const xnn_simd_f32_t vx = xnn_load_tail_f32(input, batch >> XNN_LOG2_SIZEOF_FLOAT);

      // Create a mask of the +/-0 inputs, which will be flushed to zero later.
      const xnn_simd_f32_t vinf_mask = xnn_cmpneq_f32(vx, xnn_zero_f32());

      // Generate the initial 12-bit approximation.
      xnn_simd_f32_t vt0 = xnn_rsqrt_f32(vx);

      // Do a fixed number of Newton-Raphson steps as described above.
      for (size_t i = 0; i < XNN_SIMD_NUM_RSQRT_ITER_F32; i++) {
        const xnn_simd_f32_t vt1 = xnn_mul_f32(vt0, vt0);
        const xnn_simd_f32_t vt2 = xnn_mul_f32(vx, vt1);
        const xnn_simd_f32_t vt3 = xnn_sub_f32(vt2, kOne);
        const xnn_simd_f32_t vt4 = xnn_mul_f32(kHalf, vt0);
        const xnn_simd_f32_t vt5 = xnn_mul_f32(vt3, vt4);
        vt0 = xnn_sub_f32(vt0, vt5);
      }

      // Mask out the inputs that were zero.
      const xnn_simd_f32_t vt7 = xnn_and_f32(vinf_mask, vt0);

      // Extract the square root by multiplying the original value with the
      // inverse square root.
      const xnn_simd_f32_t vy = xnn_mul_f32(vx, vt7);

      xnn_store_tail_f32(output, vy, batch >> XNN_LOG2_SIZEOF_FLOAT);
    }
  }
