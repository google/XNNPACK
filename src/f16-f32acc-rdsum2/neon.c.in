// Copyright 2025 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

#include <arm_neon.h>
#include <assert.h>
#include <stddef.h>
#include <stdint.h>

#include "src/xnnpack/common.h"
#include "src/xnnpack/math.h"
#include "src/xnnpack/microparams.h"
#include "src/xnnpack/reduce.h"

$CHANNELS_BATCHES = tuple(int(cb) for cb in CHANNELS_BATCHES.split(","))
$for CHANNELS_BATCH in CHANNELS_BATCHES:
  $UNROLL = CHANNELS_BATCH >> 2

  void xnn_f16_f32acc_rdsum2_ukernel_${ACCUMULATORS}p${ACCUMULATORS}x__neonfp16arith_u${CHANNELS_BATCH}(
      size_t channels,
      size_t k1,
      size_t k2,
      size_t k3,
      const xnn_float16* input,
      size_t input_stride1,
      size_t input_stride2,
      size_t input_stride3,
      const xnn_float16* zero,
      float* output,
      const struct xnn_f16_f32acc_scale_params* restrict params)
  {
    assert(k1 != 0);
    assert(channels != 0);
    assert(input != NULL);
    assert(output != NULL);

    const float32x4_t vscale = vdupq_n_f32(params->scalar.scale);
    float* original_output = output;
    size_t original_channels = channels;

    for (size_t k = 0; k < k3; ++k) {
      for (size_t j = 0; j < k2; ++j) {
        const xnn_float16* input_row = (const xnn_float16*)((uintptr_t)input + j * input_stride2 + k * input_stride3);
        output = original_output;
        channels = original_channels;

        assert(input_row != NULL);

        size_t input_increment = ${ACCUMULATORS} * input_stride1;
        for (; channels >= ${CHANNELS_BATCH}; channels -= ${CHANNELS_BATCH}) {
          const uint16_t* i0 = (const uint16_t*) input_row;
          $for i in range(1, ACCUMULATORS):
            const uint16_t* i${i} = (const uint16_t*) ((uintptr_t) input_row + ${i} * input_stride1);

          $for i in range(UNROLL):
            float32x4_t vacc${i} = vdupq_n_f32(0.f);

          for (int r = k1; r > 0; r -= ${ACCUMULATORS}) {
            $for N in range(1, ACCUMULATORS, 2):
              if XNN_UNPREDICTABLE(r < ${N+1}) {
                i${N} = (const uint16_t*) zero;
              }
              if XNN_UNPREDICTABLE(r <= ${N+1}) {
                i${N+1} = (const uint16_t*) zero;
              }
            $for c in range(UNROLL):
              float32x4_t vin${c};
            $for j in range(ACCUMULATORS):
              $for c in range(UNROLL):
                vin${c} = vcvt_f32_f16(vreinterpret_f16_u16(vld1_u16(&i${j}[${c*4}])));
              $for c in range(UNROLL):
                vacc${c} = vmlaq_f32(vacc${c}, vin${c}, vin${c});
            $for N in range(0, ACCUMULATORS):
              i${N} = (const uint16_t*) ((uintptr_t) i${N} + input_increment);
          }
          $for i in range(UNROLL):
            vacc${i} = vmulq_f32(vacc${i}, vscale);

          const float* o = (const float*) output;
          $for i in range(0, UNROLL):
            float32x4_t vo${i} = vld1q_f32(o); o += 4;
          $for i in range(0, UNROLL):
            float32x4_t v_out${i} = vaddq_f32(vo${i}, vacc${i});
          $for i in range(0, UNROLL):
            vst1q_f32(output, v_out${i}); output = (void*) ((uintptr_t) output + 4 * sizeof(float));

          input_row = (const xnn_float16*) ((uintptr_t) input_row + ${CHANNELS_BATCH} * sizeof(uint16_t));
        }
        if (channels != 0) {
          input_increment = ${ACCUMULATORS} * input_stride1;
          const uint16_t* i0 = (const uint16_t*) input_row;
          $for i in range(1, ACCUMULATORS):
            const uint16_t* i${i} = (const uint16_t*) ((uintptr_t) input_row + ${i} * input_stride1);
          float32x4_t vacc[${UNROLL}];
          $for i in range(UNROLL):
            vacc[${i}] = vdupq_n_f32(0.f);

          const size_t num_chunks = round_up_po2(channels, 4) >> 2;
          const size_t num_full_chunks = channels >> 2;
          for (int r = k1; r > 0; r -= ${ACCUMULATORS}) {
            $for N in range(1, ACCUMULATORS, 2):
              if XNN_UNPREDICTABLE(r < ${N+1}) {
                i${N} = (const uint16_t*) zero;
              }
              if XNN_UNPREDICTABLE(r <= ${N+1}) {
                i${N+1} = (const uint16_t*) zero;
              }
            for (int i = 0; i < num_chunks; ++i) {
              $for c in range(ACCUMULATORS):
                float32x4_t vin${c} = vcvt_f32_f16(vreinterpret_f16_u16(vld1_u16(&i${c}[i*4])));
              $for c in range(ACCUMULATORS):
                vacc[i] = vmlaq_f32(vacc[i], vin${c}, vin${c});
            }
            $for N in range(ACCUMULATORS):
              i${N} = (const uint16_t*) ((uintptr_t) i${N} + input_increment);
          }
          for (int i = 0; i < (channels + 4) >> 2; ++i) {
            vacc[i] = vmulq_f32(vacc[i], vscale);
          }

          float32x4_t vo[${UNROLL}];
          const float* o = (const float*) output;
          for (int i = 0; i < num_full_chunks; ++i) {
            vo[i] = vld1q_f32(o); o += 4;
          }
          float32x4_t v_out[${UNROLL}];
          for (int i = 0; i < num_full_chunks; ++i) {
            v_out[i] = vaddq_f32(vo[i], vacc[i]);
          }
          for (int i = 0; i < num_full_chunks; ++i) {
            vst1q_f32(output, v_out[i]); output = (void*) ((uintptr_t) output + 4 * sizeof(float));
          }

          const size_t pos = channels >> 2;
          channels &= 0x3;
          float32x2_t vacc_low = vget_low_f32(vacc[pos]);
          if (channels & 2) {
            vst1_f32(output, vadd_f32(vacc_low, vld1_f32(output))); output = (void*) ((uintptr_t) output + 2 * sizeof(float));
            vacc_low = vget_high_f32(vacc[pos]);
          }
          if (channels & 1) {
            vst1_lane_f32(output, vadd_f32(vacc_low, vld1_dup_f32(output)), 0);
          }
        }
      }
    }
  }
