// Copyright 2020 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert REQUANTIZATION in ["FP32", "RNDNU"]

#include <xnnpack/assembly.h>

$REWIND_DECREMENT = {"RNDNU": 15, "FP32": 7}[REQUANTIZATION]
# void xnn_qu8_gemm_minmax_${REQUANTIZATION.lower()}_ukernel_4x8c4__asm_aarch64_neondot_cortex_a55(
#     size_t mr,                 x0
#     size_t nc,                 x1
#     size_t kc,                 x2 / x0
#     const int8_t* restrict a,  x3
#     size_t a_stride,           x4
#     const void* restrict w,    x5
#     int8_t* restrict c,        x6
#     size_t cm_stride,          x7
#     size_t cn_stride,          [sp] -> x12
#     const union xnn_qu8_conv_minmax_params)  [sp + 8] -> x11

# d8-d15, x19-x30 need to be preserved if used. x18 is reserved by the OS.

// Register usage
// A0  x3  v0  v4
// A1 x15  v1  v5
// A2 x13  v2  v6
// A3  x4  v3  v7
// B   x5  v28  v29 v30 v31
// C0  x6 v16 v20
// C1  x8 v17 v21
// C2  x9 v18 v22
// C3  x7 v19 v23
// zero_point v24 v25 v26 v27 v8
// unused v12 v13 v14 v15 v29 v30 v31
// temp x14 for Cortex-A55 loads

BEGIN_FUNCTION xnn_qu8_gemm_minmax_${REQUANTIZATION.lower()}_ukernel_4x8c4__asm_aarch64_neondot_cortex_a55

        # Clamp A and C pointers
        CMP         x0, 2                   // if mr < 2
        ADD         x2, x2, 3               // kc = (kc + 3) & ~3
        ADD         x15, x3, x4             // a1 = a0 + a_stride
        ADD         x8, x6, x7              // c1 = c0 + cm_stride
        CSEL        x15, x3, x15, LO        //   a1 = a0
        CSEL        x8, x6,  x8, LO         //   c1 = c0
        BIC         x2, x2, 3

        LDP         x12, x11, [sp]          // cn_stride, params

        ADD         x13, x15, x4            // a2 = a1 + a_stride
        ADD         x9,  x8, x7             // c2 = c1 + cm_stride
        STR         d8, [sp, -16]!          // Save d8 on stack
                                            // if mr <= 2
        CSEL        x13, x15, x13, LS       //   a2 = a1
        CSEL        x9,  x8,  x9, LS        //   c2 = c1

        LD1R        {v8.4s}, [x11], 4       // kernel_zero_point

        CMP         x0, 4                   // if mr < 4
        ADD         x4, x13, x4             // a3 = a2 + a_stride
        ADD         x7,  x9, x7             // c3 = c2 + cm_stride
        CSEL        x4, x13, x4, LO         //   a3 = a2
        CSEL        x7,  x9, x7, LO         //   c3 = c2

        .p2align    3
0:
        # Load initial bias from w into accumulators
        LDP         q16, q20, [x5], 32
        MOV         v17.16b, v16.16b
        MOV         v18.16b, v16.16b
        MOV         v19.16b, v16.16b
        MOV         v21.16b, v20.16b
        MOV         v22.16b, v20.16b
        MOV         v23.16b, v20.16b
        SUBS        x0, x2, 16              // k = kc - 16
        MOVI        v24.16b, 0
        MOVI        v25.16b, 0
        MOVI        v26.16b, 0
        MOVI        v27.16b, 0

        # Is there at least 16 bytes for prologue/epilogue?
        B.LO        4f

        # prologue - read A and B values for block 0 and 1
        LDR         d0,  [x3], 8
        LDR         q28, [x5], 16
        LDR         d1, [x15], 8
        LDR         d2, [x13], 8
        LDR         d3,  [x4], 8
        SUBS        x0, x0, 16              // is there 16 for main loop?
        LDR         d29, [x5], 8
        LDR         x14, [x5], 8
        # Is there at least 16 bytes for main loop?
        B.LO        2f

        # Main loop - 16 bytes of A in 4 groups of 2 blocks
        # 4 row of 2 vectors wide = 8 UDOT instructions for 4 channels
        # 4 LD64 for A
        # 4 LD128 for W. = 2 LD64 + INS.
        # for each 4 UDOT, 1 LD64 for A, 2 LD64 for W + INS.

        .p2align    3
1:
        # BLOCK 0
        UDOT        v16.4s,  v28.16b, v0.4b[0]
        LDR         d30,  [x5], 8
        UDOT        v17.4s,  v28.16b, v1.4b[0]
        INS         v29.d[1], x14
        UDOT        v18.4s,  v28.16b, v2.4b[0]
        LDR         x14,  [x5], 8
        UDOT        v19.4s,  v28.16b, v3.4b[0]
        LDR         d4,  [x3], 8

        # BLOCK 1
        UDOT        v20.4s,  v29.16b, v0.4b[0]
        LDR         d31,  [x5], 8
        UDOT        v21.4s,  v29.16b, v1.4b[0]
        INS         v30.d[1], x14
        UDOT        v22.4s,  v29.16b, v2.4b[0]
        LDR         x14,  [x5], 8
        UDOT        v23.4s,  v29.16b, v3.4b[0]
        LDR         d5, [x15], 8

        # BLOCK 0
        UDOT        v16.4s, v30.16b, v0.4b[1]
        LDR         d28,  [x5], 8
        UDOT        v17.4s, v30.16b, v1.4b[1]
        INS         v31.d[1], x14
        UDOT        v18.4s, v30.16b, v2.4b[1]
        LDR         x14,  [x5], 8
        UDOT        v19.4s, v30.16b, v3.4b[1]
        LDR         d6, [x13], 8

        # BLOCK 1
        UDOT        v20.4s, v31.16b, v0.4b[1]
        LDR         d29,  [x5], 8
        UDOT        v21.4s, v31.16b, v1.4b[1]
        INS         v28.d[1], x14
        UDOT        v22.4s, v31.16b, v2.4b[1]
        LDR         x14,  [x5], 8
        UDOT        v23.4s, v31.16b, v3.4b[1]
        LDR         d7,  [x4], 8

        UDOT        v24.2s, v8.8b, v0.8b
        UDOT        v25.2s, v8.8b, v1.8b
        UDOT        v26.2s, v8.8b, v2.8b
        UDOT        v27.2s, v8.8b, v3.8b

        # BLOCK 0
        UDOT        v16.4s,  v28.16b, v4.4b[0]
        LDR         d30,  [x5], 8
        UDOT        v17.4s,  v28.16b, v5.4b[0]
        INS         v29.d[1], x14
        UDOT        v18.4s,  v28.16b, v6.4b[0]
        LDR         x14,  [x5], 8
        UDOT        v19.4s,  v28.16b, v7.4b[0]
        LDR         d0,  [x3], 8

        # BLOCK 1
        UDOT        v20.4s,  v29.16b, v4.4b[0]
        LDR         d31,  [x5], 8
        UDOT        v21.4s,  v29.16b, v5.4b[0]
        INS         v30.d[1], x14
        UDOT        v22.4s,  v29.16b, v6.4b[0]
        LDR         x14,  [x5], 8
        UDOT        v23.4s,  v29.16b, v7.4b[0]
        LDR         d1, [x15], 8

        # BLOCK 0
        UDOT        v16.4s, v30.16b, v4.4b[1]
        LDR         d28,  [x5], 8
        UDOT        v17.4s, v30.16b, v5.4b[1]
        INS         v31.d[1], x14
        UDOT        v18.4s, v30.16b, v6.4b[1]
        LDR         x14,  [x5], 8
        UDOT        v19.4s, v30.16b, v7.4b[1]
        LDR         d2, [x13], 8

        # BLOCK 1
        UDOT        v20.4s, v31.16b, v4.4b[1]
        LDR         d29,  [x5], 8
        UDOT        v21.4s, v31.16b, v5.4b[1]
        INS         v28.d[1], x14
        UDOT        v22.4s, v31.16b, v6.4b[1]
        LDR         x14,  [x5], 8
        UDOT        v23.4s, v31.16b, v7.4b[1]
        LDR         d3,  [x4], 8

        UDOT        v24.2s, v8.8b, v4.8b
        UDOT        v25.2s, v8.8b, v5.8b
        SUBS        x0, x0, 16
        UDOT        v26.2s, v8.8b, v6.8b
        UDOT        v27.2s, v8.8b, v7.8b

        B.HS        1b

        # Epilogue.  Same as main loop but no preloads in final group
2:
        # BLOCK 0
        UDOT        v16.4s,  v28.16b, v0.4b[0]
        LDR         d30,  [x5], 8
        UDOT        v17.4s,  v28.16b, v1.4b[0]
        INS         v29.d[1], x14
        UDOT        v18.4s,  v28.16b, v2.4b[0]
        LDR         x14,  [x5], 8
        UDOT        v19.4s,  v28.16b, v3.4b[0]
        LDR         d4,  [x3], 8

        # BLOCK 1
        UDOT        v20.4s,  v29.16b, v0.4b[0]
        LDR         d31,  [x5], 8
        UDOT        v21.4s,  v29.16b, v1.4b[0]
        INS         v30.d[1], x14
        UDOT        v22.4s,  v29.16b, v2.4b[0]
        LDR         x14,  [x5], 8
        UDOT        v23.4s,  v29.16b, v3.4b[0]
        LDR         d5, [x15], 8

        # BLOCK 0
        UDOT        v16.4s, v30.16b, v0.4b[1]
        LDR         d28,  [x5], 8
        UDOT        v17.4s, v30.16b, v1.4b[1]
        INS         v31.d[1], x14
        UDOT        v18.4s, v30.16b, v2.4b[1]
        LDR         x14,  [x5], 8
        UDOT        v19.4s, v30.16b, v3.4b[1]
        LDR         d6, [x13], 8

        # BLOCK 1
        UDOT        v20.4s, v31.16b, v0.4b[1]
        LDR         d29,  [x5], 8
        UDOT        v21.4s, v31.16b, v1.4b[1]
        INS         v28.d[1], x14
        UDOT        v22.4s, v31.16b, v2.4b[1]
        LDR         x14,  [x5], 8
        UDOT        v23.4s, v31.16b, v3.4b[1]
        LDR         d7,  [x4], 8

        UDOT        v24.2s, v8.8b, v0.8b
        UDOT        v25.2s, v8.8b, v1.8b
        UDOT        v26.2s, v8.8b, v2.8b
        UDOT        v27.2s, v8.8b, v3.8b

        # BLOCK 0
        UDOT        v16.4s,  v28.16b, v4.4b[0]
        LDR         d30,  [x5], 8
        UDOT        v17.4s,  v28.16b, v5.4b[0]
        INS         v29.d[1], x14
        UDOT        v18.4s,  v28.16b, v6.4b[0]
        LDR         x14,  [x5], 8
        UDOT        v19.4s,  v28.16b, v7.4b[0]

        # BLOCK 1
        UDOT        v20.4s,  v29.16b, v4.4b[0]
        LDR         d31,  [x5], 8
        UDOT        v21.4s,  v29.16b, v5.4b[0]
        INS         v30.d[1], x14
        UDOT        v22.4s,  v29.16b, v6.4b[0]
        LDR         x14,  [x5], 8
        UDOT        v23.4s,  v29.16b, v7.4b[0]

        # BLOCK 0
        UDOT        v16.4s, v30.16b, v4.4b[1]
        UDOT        v17.4s, v30.16b, v5.4b[1]
        INS         v31.d[1], x14
        UDOT        v18.4s, v30.16b, v6.4b[1]
        UDOT        v19.4s, v30.16b, v7.4b[1]

        # BLOCK 1
        UDOT        v20.4s, v31.16b, v4.4b[1]
        UDOT        v21.4s, v31.16b, v5.4b[1]
        UDOT        v22.4s, v31.16b, v6.4b[1]
        UDOT        v23.4s, v31.16b, v7.4b[1]

        AND         x0, x2, 15              // kc remainder 0 to 12

        UDOT        v24.2s, v8.8b, v4.8b
        UDOT        v25.2s, v8.8b, v5.8b
        UDOT        v26.2s, v8.8b, v6.8b
        UDOT        v27.2s, v8.8b, v7.8b

        # Is there a remainder?- 4 to 12 bytes of A
        CBNZ        x0, 4f

        .p2align    3
3:
        ADDP        v0.2s, v24.2s, v25.2s
        ADDP        v1.2s, v26.2s, v27.2s
        DUP         v24.4s, v0.s[0]
        DUP         v25.4s, v0.s[1]
        DUP         v26.4s, v1.s[0]
        DUP         v27.4s, v1.s[1]

        # Subtract zero point from accumulators
        SUB         v16.4s, v16.4s, v24.4s
        SUB         v17.4s, v17.4s, v25.4s
        SUB         v18.4s, v18.4s, v26.4s
        SUB         v19.4s, v19.4s, v27.4s
        SUB         v20.4s, v20.4s, v24.4s
        SUB         v21.4s, v21.4s, v25.4s
        SUB         v22.4s, v22.4s, v26.4s
        SUB         v23.4s, v23.4s, v27.4s

        $if REQUANTIZATION == "RNDNU":
          # Apply params - preshift, scale, postshift, bias and clamp
          LD1R        {v4.4s}, [x11], 4
          SSHL        v16.4s, v16.4s, v4.4s   // shift to upper bits
          SSHL        v17.4s, v17.4s, v4.4s
          SSHL        v18.4s, v18.4s, v4.4s
          SSHL        v19.4s, v19.4s, v4.4s
          LD1R        {v5.4s}, [x11], 4
          SSHL        v20.4s, v20.4s, v4.4s
          SSHL        v21.4s, v21.4s, v4.4s
          SSHL        v22.4s, v22.4s, v4.4s
          SSHL        v23.4s, v23.4s, v4.4s
          LD1R        {v6.4s}, [x11], 4
          SQDMULH     v16.4s, v16.4s, v5.4s   // scale without rounding
          SQDMULH     v17.4s, v17.4s, v5.4s
          SQDMULH     v18.4s, v18.4s, v5.4s
          SQDMULH     v19.4s, v19.4s, v5.4s
          SQDMULH     v20.4s, v20.4s, v5.4s
          SQDMULH     v21.4s, v21.4s, v5.4s
          SQDMULH     v22.4s, v22.4s, v5.4s
          SQDMULH     v23.4s, v23.4s, v5.4s
          SRSHL       v16.4s, v16.4s, v6.4s   // signed rounding shift left
          SRSHL       v17.4s, v17.4s, v6.4s
          SRSHL       v18.4s, v18.4s, v6.4s
          SRSHL       v19.4s, v19.4s, v6.4s
          SRSHL       v20.4s, v20.4s, v6.4s
          SRSHL       v21.4s, v21.4s, v6.4s
          SRSHL       v22.4s, v22.4s, v6.4s
          SRSHL       v23.4s, v23.4s, v6.4s
        $elif REQUANTIZATION == "FP32":
          # Apply params - scale, bias and clamp
          SCVTF       v16.4s, v16.4s
          SCVTF       v17.4s, v17.4s
          LD1R        {v4.4s}, [x11], 4
          SCVTF       v18.4s, v18.4s
          SCVTF       v19.4s, v19.4s
          SCVTF       v20.4s, v20.4s
          SCVTF       v21.4s, v21.4s
          SCVTF       v22.4s, v22.4s
          SCVTF       v23.4s, v23.4s

          FMUL        v16.4s, v16.4s, v4.4s
          FMUL        v17.4s, v17.4s, v4.4s
          FMUL        v18.4s, v18.4s, v4.4s
          FMUL        v19.4s, v19.4s, v4.4s
          FMUL        v20.4s, v20.4s, v4.4s
          FMUL        v21.4s, v21.4s, v4.4s
          FMUL        v22.4s, v22.4s, v4.4s
          FMUL        v23.4s, v23.4s, v4.4s

          FCVTNS      v16.4s, v16.4s
          FCVTNS      v17.4s, v17.4s
          FCVTNS      v18.4s, v18.4s
          FCVTNS      v19.4s, v19.4s
          FCVTNS      v20.4s, v20.4s
          FCVTNS      v21.4s, v21.4s
          FCVTNS      v22.4s, v22.4s
          FCVTNS      v23.4s, v23.4s

        SQXTN       v16.4h, v16.4s
        SQXTN       v17.4h, v17.4s
        SQXTN       v18.4h, v18.4s
        SQXTN       v19.4h, v19.4s
        LD1R        {v6.8h}, [x11], 2       // add bias

        SQXTN2      v16.8h, v20.4s
        SQXTN2      v17.8h, v21.4s
        SQXTN2      v18.8h, v22.4s
        SQXTN2      v19.8h, v23.4s

        SQADD       v16.8h, v16.8h, v6.8h
        SQADD       v17.8h, v17.8h, v6.8h
        SQADD       v18.8h, v18.8h, v6.8h
        SQADD       v19.8h, v19.8h, v6.8h
        LD1R        {v4.16b}, [x11], 1      // clamp min value

        SQXTUN      v0.8b, v16.8h
        SQXTUN      v1.8b, v18.8h
        LD1R        {v5.16b}, [x11]         // clamp max value
        SQXTUN2     v0.16b, v17.8h
        SQXTUN2     v1.16b, v19.8h
        SUB         x11, x11, ${REWIND_DECREMENT}               // rewind params pointer

        UMAX        v0.16b, v0.16b, v4.16b
        UMAX        v1.16b, v1.16b, v4.16b
        SUBS        x1, x1, 8
        UMIN        v0.16b, v0.16b, v5.16b
        UMIN        v1.16b, v1.16b, v5.16b
        B.LO        6f

        # Store full 4 x 8
        ST1         {v0.8b}, [x6], x12
        SUB         x3,  x3, x2             // a0 -= kc
        ST1         {v0.d}[1], [x8], x12
        SUB         x15, x15, x2            // a1 -= kc
        ST1         {v1.8b}, [x9], x12
        SUB         x13, x13, x2            // a2 -= kc
        ST1         {v1.d}[1], [x7], x12
        SUB         x4,  x4, x2             // a3 -= kc
        B.NE        0b

        # Restore d8 from stack
        LDR         d8, [sp], 16
        RET

        # Remainder- 4 to 12 bytes of A
        # Although C4, its safe to read 16 bytes.
        .p2align    3
4:
        TBZ         x0, 3, 5f

        LDR         d0,  [x3], 8
        LDR         q4, [x5], 16
        LDR         d1, [x15], 8
        LDR         d2, [x13], 8
        LDR         d3,  [x4], 8
        LDR         q5, [x5], 16
        UDOT        v24.2s, v8.8b, v0.8b
        UDOT        v25.2s, v8.8b, v1.8b
        UDOT        v26.2s, v8.8b, v2.8b
        UDOT        v27.2s, v8.8b, v3.8b
        UDOT        v16.4s, v4.16b, v0.4b[0]
        UDOT        v17.4s, v4.16b, v1.4b[0]
        UDOT        v18.4s, v4.16b, v2.4b[0]
        UDOT        v19.4s, v4.16b, v3.4b[0]
        LDR         q6, [x5], 16
        UDOT        v20.4s, v5.16b, v0.4b[0]
        UDOT        v21.4s, v5.16b, v1.4b[0]
        UDOT        v22.4s, v5.16b, v2.4b[0]
        UDOT        v23.4s, v5.16b, v3.4b[0]
        LDR         q4, [x5], 16
        UDOT        v16.4s, v6.16b, v0.4b[1]
        UDOT        v17.4s, v6.16b, v1.4b[1]
        UDOT        v18.4s, v6.16b, v2.4b[1]
        UDOT        v19.4s, v6.16b, v3.4b[1]
        UDOT        v20.4s, v4.16b, v0.4b[1]
        UDOT        v21.4s, v4.16b, v1.4b[1]
        UDOT        v22.4s, v4.16b, v2.4b[1]
        UDOT        v23.4s, v4.16b, v3.4b[1]
        TBZ         x0, 2, 3b
5:
        LDR         s0,  [x3], 4
        LDR         q4, [x5], 16
        LDR         s1, [x15], 4
        LDR         s2, [x13], 4
        LDR         s3,  [x4], 4
        LDR         q5, [x5], 16
        UDOT        v24.2s, v8.8b, v0.8b
        UDOT        v25.2s, v8.8b, v1.8b
        UDOT        v26.2s, v8.8b, v2.8b
        UDOT        v27.2s, v8.8b, v3.8b
        UDOT        v16.4s, v4.16b, v0.4b[0]
        UDOT        v17.4s, v4.16b, v1.4b[0]
        UDOT        v18.4s, v4.16b, v2.4b[0]
        UDOT        v19.4s, v4.16b, v3.4b[0]
        UDOT        v20.4s, v5.16b, v0.4b[0]
        UDOT        v21.4s, v5.16b, v1.4b[0]
        UDOT        v22.4s, v5.16b, v2.4b[0]
        UDOT        v23.4s, v5.16b, v3.4b[0]
        B           3b

        # Store odd width
        .p2align    3
6:
        TBZ         x1, 2, 7f
        STR         s0, [x6], 4
        ST1         {v0.s}[2], [x8], 4
        STR         s1, [x9], 4
        ST1         {v1.s}[2], [x7], 4
        EXT         v0.16b, v0.16b, v0.16b, 4
        EXT         v1.16b, v1.16b, v1.16b, 4
7:
        TBZ         x1, 1, 8f
        STR         h0, [x6], 2
        ST1         {v0.h}[4], [x8], 2
        STR         h1, [x9], 2
        ST1         {v1.h}[4], [x7], 2
        EXT         v0.16b, v0.16b, v0.16b, 2
        EXT         v1.16b, v1.16b, v1.16b, 2
8:
        TBZ         x1, 0, 9f
        STR         b0, [x6]
        ST1         {v0.b}[8], [x8]
        STR         b1, [x9]
        ST1         {v1.b}[8], [x7]
9:
        # Restore d8 from stack
        LDR         d8, [sp], 16
        RET

END_FUNCTION xnn_qu8_gemm_minmax_${REQUANTIZATION.lower()}_ukernel_4x8c4__asm_aarch64_neondot_cortex_a55

#ifdef __ELF__
.section ".note.GNU-stack","",%progbits
#endif
