// Copyright 2025 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

#include <assert.h>
#include <stddef.h>
#include <stdint.h>

#include "src/xnnpack/common.h"
#include "src/xnnpack/math.h"
#include "src/xnnpack/microparams.h"
#include "src/xnnpack/reduce.h"
#include "src/xnnpack/simd/f32-${ARCH}.h"

$import math
$LOG2_SIMD_SIZE = int(math.log2(SIMD_SIZE))
$CHANNELS_LIST = tuple(int(c) for c in str(CHANNELS).split(","))
$for CHANNELS in $CHANNELS_LIST:
  $UNROLL = CHANNELS >> LOG2_SIMD_SIZE

  void xnn_f32_rdsum2_ukernel_${ACCUMULATORS}p${ACCUMULATORS}x__${ARCH}_u${CHANNELS}(
    size_t channels, size_t k1, size_t k2, size_t k3, const float* input,
    size_t input_stride1, size_t input_stride2, size_t input_stride3,
    const float* zero, float* output,
    const struct xnn_f32_scale_params* restrict params) {
    assert(k1 != 0);
    assert(channels != 0);
    assert(input != NULL);
    assert(output != NULL);

    const xnn_simd_f32_t vscale = xnn_set1_f32(params->scalar.scale);
    float* original_output = output;
    size_t original_channels = channels;

    for (size_t k = 0; k < k3; ++k) {
      for (size_t j = 0; j < k2; ++j) {
        const float* input_row =
            (const float*)((uintptr_t)input + j * input_stride2
                           + k * input_stride3);
        output = original_output;
        channels = original_channels;

        assert(input_row != NULL);

        size_t input_increment = ${ACCUMULATORS} * input_stride1;
        for (; channels >= ${CHANNELS}; channels -= ${CHANNELS}) {
          const float* i0 = input_row;
          $for i in range(1, ACCUMULATORS):
            const float* i${i} = 
                (const float*) ((uintptr_t) input_row + ${i} * input_stride1);

          $for i in range(UNROLL):
            xnn_simd_f32_t vacc${i} = xnn_zero_f32();

          for (int r = k1; r > 0; r -= ${ACCUMULATORS}) {
            $for N in range(1, ACCUMULATORS, 2):
              if XNN_UNPREDICTABLE(r < ${N+1}) {
                i${N} = zero;
              }
              if XNN_UNPREDICTABLE(r <= ${N+1}) {
                i${N+1} = zero;
              }
            $for c in range(UNROLL):
              xnn_simd_f32_t vin${c};
            $for j in range(ACCUMULATORS):
              $for c in range(UNROLL):
                vin${c} = xnn_loadu_f32(&i${j}[${c*SIMD_SIZE}]);
              $for c in range(UNROLL):
                vin${c} = xnn_mul_f32(vin${c}, vin${c});
              $for c in range(UNROLL):
                vacc${c} = xnn_add_f32(vin${c}, vacc${c});
            $for N in range(0, ACCUMULATORS):
              i${N} = (const float*) ((uintptr_t) i${N} + input_increment);
          }
          $for i in range(UNROLL):
            vacc${i} = xnn_mul_f32(vacc${i}, vscale);

          const float* o = output;
          $for i in range(0, UNROLL):
            xnn_simd_f32_t vo${i} = xnn_loadu_f32(o); o += ${SIMD_SIZE};
          $for i in range(0, UNROLL):
            vacc${i} = xnn_add_f32(vo${i}, vacc${i});
          $for i in range(0, UNROLL):
            xnn_storeu_f32(output, vacc${i}); output += ${SIMD_SIZE};

          input_row = (const float*) ((uintptr_t) input_row + ${CHANNELS} * sizeof(float));
        }
        if (channels != 0) {
          input_increment = ${ACCUMULATORS} * input_stride1;
          const float* i0 = input_row;
          $for i in range(1, ACCUMULATORS):
            const float* i${i} =
                (const float*) ((uintptr_t) input_row + ${i} * input_stride1);
          xnn_simd_f32_t vacc[${UNROLL}];
          $for i in range(UNROLL):
            vacc[${i}] = xnn_zero_f32();

          const size_t num_full_chunks = channels >> ${LOG2_SIMD_SIZE};
          const size_t num_chunks = round_up_po2(channels, ${SIMD_SIZE}) >> ${LOG2_SIMD_SIZE};
          const size_t remainder = channels & ${SIMD_SIZE - 1};
          for (int r = k1; r > 0; r -= ${ACCUMULATORS}) {
            $for N in range(1, ACCUMULATORS, 2):
              if XNN_UNPREDICTABLE(r < ${N+1}) {
                i${N} = zero;
              }
              if XNN_UNPREDICTABLE(r <= ${N+1}) {
                i${N+1} = zero;
              }
            for (int i = 0; i < num_full_chunks; ++i) {
              $for c in range(ACCUMULATORS):
                xnn_simd_f32_t vin${c} = xnn_loadu_f32(&i${c}[i*${SIMD_SIZE}]);
              $for c in range(ACCUMULATORS):
                vin${c} = xnn_mul_f32(vin${c}, vin${c});
              $for c in range(ACCUMULATORS):
                vacc[i] = xnn_add_f32(vin${c}, vacc[i]);
            }

            if (remainder) {
              $for c in range(ACCUMULATORS):
                xnn_simd_f32_t vin${c} = xnn_load_tail_f32(&i${c}[num_full_chunks*${SIMD_SIZE}], remainder);
              $for c in range(ACCUMULATORS):
                vin${c} = xnn_mul_f32(vin${c}, vin${c});
              $for c in range(ACCUMULATORS):
                vacc[num_full_chunks] = xnn_add_f32(vin${c}, vacc[num_full_chunks]);
            }
            $for N in range(ACCUMULATORS):
              i${N} = (const float*) ((uintptr_t) i${N} + input_increment);
          }
          for (size_t i = 0; i < num_chunks; ++i) {
            vacc[i] = xnn_mul_f32(vacc[i], vscale);
          }

          xnn_simd_f32_t vo[${UNROLL}];
          const float* o = output;
          for (int i = 0; i < channels >> ${LOG2_SIMD_SIZE}; ++i) {
            vo[i] = xnn_loadu_f32(o); o += ${SIMD_SIZE};
          }
          for (int i = 0; i < channels >> ${LOG2_SIMD_SIZE}; ++i) {
            vacc[i] = xnn_add_f32(vo[i], vacc[i]);
          }
          for (int i = 0; i < channels >> ${LOG2_SIMD_SIZE}; ++i) {
            xnn_storeu_f32(output, vacc[i]); output += ${SIMD_SIZE};
          }
          if (remainder) {
            const size_t pos = num_full_chunks;
            xnn_simd_f32_t vout = vacc[pos];
            const xnn_simd_f32_t vdata = xnn_load_tail_safe_f32(output, remainder);
            vout = xnn_add_f32(vout, vdata);
            xnn_store_tail_f32(output, vout, remainder);
          }
        }
      }
    }
  }
