// Copyright 2023 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$import math
$assert LOG2LUT in [0, 3]
$assert P == H + 1 or P == H + 2
$assert P == H + 1 or not PS
$LUT = 1 << LOG2LUT
$assert MINMAX in ["WASM", "PSEUDO", "RELAXED"]
#include <assert.h>
#include <stddef.h>
#include <stdint.h>

#include <wasm_simd128.h>

#include <xnnpack/common.h>
#include <xnnpack/math.h>
#include <xnnpack/math-stubs.h>

$if LOG2LUT != 0:

  // Table of exp2(k / ${LUT}) values decremented (as integer) by (k << ${23-LOG2LUT}), k = 0..${LUT-1}
  extern XNN_INTERNAL const uint32_t xnn_table_exp2minus_k_over_${LUT}[${LUT}];

$if LOG2LUT == 3:
  $COEFFS = ["0x1.000000p+1", "-0x1.555C20p+0", "0x1.5558ECp-1"]
$elif LOG2LUT == 0:
  $COEFFS = ["0x1.FFFFFEp+0", "-0x1.5554B0p+0", "0x1.555716p-1", "-0x1.12278Ep-2", "0x1.6B7338p-4"]
$POLYNOMIAL = "c%d" % P
$for i in reversed(range(2, P)):
$  POLYNOMIAL = "(c%d + t * %s)" % (i, POLYNOMIAL)
$POLYNOMIAL = "(-2 + t * %s)" % POLYNOMIAL
$WASM_F32X4_MIN = {"WASM": "wasm_f32x4_min", "PSEUDO": "wasm_f32x4_pmin"}[MINMAX]
$LUT_SUFFIX = "" if LOG2LUT == 0 else "lut%d_" % LUT
$POLY_SUFFIX = "p%dh%d%s_" % (P, H, "ps" if PS else "ts")
$MINMAX_SUFFIX = {"WASM": "min", "PSEUDO": "pmin"}[MINMAX]
void xnn_math_f32_tanh__wasmsimd_expm1minus_rr1_${LUT_SUFFIX}${POLY_SUFFIX}div_abs_${MINMAX_SUFFIX}(
    size_t n,
    const float* input,
    float* output)
{
  assert(n % sizeof(v128_t) == 0);

  // The smallest z for which tanhf(-z) is saturated at -1.0f.
  const v128_t vsat_cutoff = wasm_f32x4_const_splat(0x1.205968p+3f);
  const v128_t vminus_log2e = wasm_f32x4_const_splat(-0x1.715476p+0f);
  $if LOG2LUT == 0:
    // Large number such that ulp(magic bias) == 0.5 and magic bias === 63.5 mod 2**21.
    const v128_t vmagic_bias = wasm_f32x4_const_splat(0x1.8000FEp+22f);
  $else:
    // Large number such that ulp(magic bias) == exp2(${-1-LOG2LUT})
    const v128_t vmagic_bias = wasm_f32x4_const_splat(0x1.800000p+${22-LOG2LUT}f);
  $if LOG2LUT != 0:
    // Mask for the lowest ${LOG2LUT} bits
    const v128_t vindex_mask = wasm_u32x4_const_splat(UINT32_C(0x${"%X" % (LUT-1)}));
  const v128_t vln2 = wasm_f32x4_const_splat(0x1.62E430p-1f);
  // Coefficients of polynomial approximation
  //   exp(-2t) - 1 ~ t * ${POLYNOMIAL}
  // on [-log(2)/${4*LUT}, log(2)/${4*LUT}]
  $for i in reversed(range(len(COEFFS))):
    const v128_t vc${i+P-len(COEFFS)+1} = wasm_f32x4_const_splat(${COEFFS[i]}f);
  const v128_t vminus_two = wasm_f32x4_const_splat(-2.0f);
  const v128_t vone = wasm_f32x4_const_splat(1.0f);
  // Mask for the sign bit.
  const v128_t vsign_mask = wasm_f32x4_const_splat(-0.0f);

  for (; n != 0; n -= 4 * sizeof(float)) {
    const v128_t vx = wasm_v128_load(input);
    input += 4;

    // General structure of the algorithm:
    //
    //           / -expm1(-2x) / (2 + expm1(-2x)) if x >= 0
    //   f(x) :=
    //           \ -f(-x) if x <= 0
    //
    // First we compute y := expm1(-2z) / (2 + expm1(-2z)) where z = abs(x),
    // then set its sign according to the sign of x: f(x) := sign(x) * abs(y).
    v128_t vz = wasm_f32x4_abs(vx);

    // The function saturates at -1 for large positive inputs: tanhf(-z) == -1.0f for z >= sat_cutoff ~= 9.010913.
    // To guarantee this behaviour, we clip input z at sat_cutoff, and leverage the fact that for our implementation
    // tanhf(-sat_cutoff) == -1.0f. NaN inputs are passed unchanged.
    vz = ${WASM_F32X4_MIN}(vz, vsat_cutoff);

    // Compute reduced argument n := round(-z / log(2), ${1+LOG2LUT}).
    // We do it by adding a large number (magic bias), which cause rounding of the result to ${1+LOG2LUT} fractional ${"bit" if LOG2LUT == 0 else "bits"},
    // then subtracing the large number back. The trick with adding large number is valid only within certain bounds
    // (|-z / log(2)| <= 2**${21-LOG2LUT}, i.e. |z| <= 0x1.62E43p+${20-LOG2LUT} = ${math.ldexp(float.fromhex("0x1.62E43p+20"), -LOG2LUT)}), but that is acceptable, because inputs x
    // outside of [-9.010913, 9.010913] (i.e. z outsize [0, 9.010913]) saturate tanhf(x).
    $if LOG2LUT == 0:
      // Additionally, we fuse addition of the floating-point exponent bias (127) into the magic bias.
    // Note that addition-subtraction of the large number doesn't cause overflow for inputs in this range.
    v128_t vn = wasm_f32x4_add(wasm_f32x4_mul(vz, vminus_log2e), vmagic_bias);

    $if LOG2LUT == 0:
      // Create a floating-point number s (scale) such that s == 2**(2n) for inputs which don't cause underflow, i.e.
      // 0 <= z <= 9.010913, and -13 <= n <= 0 accordingly.
      const v128_t vs = wasm_i32x4_shl(vn, 23);
    $else:
      // Create a floating-point number s (scale) such that s := 2**(2n) for valid inputs, i.e. 0 <= z <= 9.010913. As
      // n has ${1+LOG2LUT} fractional bits, we split s == 2**(2n) = 2**int(2n) * 2**frac(2n). We create s in two steps:
      // 1. Fetch 2**frac(2n) from the table using the ${LOG2LUT} low bits of n, as integer. Note that the fetched values are in
      //    the [1.0, 2.0) range, i.e. their unbiased floating-point exponent is 0.
      // 2. Adjust fetched value by addition of int(2n) to its floating-point exponent. The result is always a normalized
      //    number, because for 0 <= z <= 9.010913 we have -13 <= int(n) <= 0, and thus the adjusted exponent is not
      //    lower than -13.
      //
      // Shift bits ${LOG2LUT}:${LOG2LUT+8} into 23:31 (position of floating-point exponent).
      const v128_t ve = wasm_i32x4_shl(vn, ${23-LOG2LUT});

      // Use bits 0:${LOG2LUT} bits of n, as integer, as an index for table lookup of l := 2**frac(n).
      const v128_t vidx = wasm_i32x4_shl(wasm_v128_and(vn, vindex_mask), 2);
      const uint64_t vidx_lo = wasm_u64x2_extract_lane(vidx, 0);
      v128_t vl = wasm_v128_load32_zero((const void*) ((uintptr_t) xnn_table_exp2minus_k_over_${LUT} + (uint32_t) vidx_lo));
      vl = wasm_v128_load32_lane((const void*) ((uintptr_t) xnn_table_exp2minus_k_over_${LUT} + (uint32_t) (vidx_lo >> 32)), vl, 1);
      const uint64_t vidx_hi = wasm_u64x2_extract_lane(vidx, 1);
      vl = wasm_v128_load32_lane((const void*) ((uintptr_t) xnn_table_exp2minus_k_over_${LUT} + (uint32_t) vidx_hi), vl, 2);
      vl = wasm_v128_load32_lane((const void*) ((uintptr_t) xnn_table_exp2minus_k_over_${LUT} + (uint32_t) (vidx_hi >> 32)), vl, 3);

      // Adjust exponent of the value l fetched from the table to get the final s value.
      const v128_t vs = wasm_i32x4_add(vl, ve);

    // Subtract the large number back to get final n := round(-z / log(2), ${1+LOG2LUT}) as a floating-point number.
    vn = wasm_f32x4_sub(vn, vmagic_bias);

    // Compute reduced argument t := z + n * log(2). Note that -t = -z - n * log(2).
    const v128_t vt = wasm_f32x4_add(wasm_f32x4_mul(vn, vln2), vz);

    // Compute degree-${P} polynomial approximation for exp(-2t) - 1 on [-log(2)/${4*LUT}, log(2)/${4*LUT}].
    //   P(t) = t * ${POLYNOMIAL}
    //        = t * p
    v128_t vp = wasm_f32x4_add(wasm_f32x4_mul(vc${P}, vt), vc${P-1});
    $for i in reversed(range(2, P-1)):
      vp = wasm_f32x4_add(wasm_f32x4_mul(vp, vt), vc${i});
    vp = wasm_f32x4_add(wasm_f32x4_mul(vp, vt), vminus_two);

    // Reconstruct the exp(-2z) - 1 value:
    //   exp(-2z) - 1 = s * (t * ${POLYNOMIAL} + 1) - 1
    //                = s * t * p + (s - 1)
    $if PS:
      //                = (s - 1) + (p * s) * t
      const v128_t vps = wasm_f32x4_mul(vp, vs);
    $else:
      //                = (s - 1) + (t * s) * p
      const v128_t vts = wasm_f32x4_mul(vt, vs);
    const v128_t vsmo = wasm_f32x4_sub(vs, vone);
    $if PS:
      const v128_t vemo = wasm_f32x4_add(wasm_f32x4_mul(vt, vps), vsmo);
    $else:
      const v128_t vemo = wasm_f32x4_add(wasm_f32x4_mul(vp, vts), vsmo);

    // Reconstruct y = expm1(-2z) / (expm1(-2z) + 2)
    const v128_t vepo = wasm_f32x4_sub(vemo, vminus_two);
    v128_t vy = wasm_f32x4_div(vemo, vepo);

    // Reconstruct tanh(x) = copysign(y, x)
    vy = wasm_v128_bitselect(vx, vy, vsign_mask);

    wasm_v128_store(output, vy);
    output += 4;
  }
}
