// Copyright 2023 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$import math
$assert RR == 1 or RR == 2
$assert LOG2LUT in [0, 2, 3]
$assert P == H + 1 or P == H + 2
$assert P == H + 1 or not PS
$assert DIV in ["DIV", "NR1", "NR2", "NR1ADJ"]
$assert SAT in ["MINMAX", "SELECT"]
$assert FMA in [0, 3]
$assert FMA == 0 or RR == 1
$assert not PERM or LOG2LUT == 2
$LUT = 1 << LOG2LUT
#include <assert.h>
#include <stddef.h>
#include <stdint.h>
#include <math.h>

#include <immintrin.h>

#include <xnnpack/common.h>
#include <xnnpack/math.h>
#include <xnnpack/math-stubs.h>

$if LOG2LUT != 0 and not PERM:

  // Table of exp2(k / ${LUT}) values decremented (as integer) by (k << ${23-LOG2LUT}), k = 0..${LUT-1}
  extern XNN_INTERNAL const uint32_t xnn_table_exp2minus_k_over_${LUT}[${LUT}];

$if LOG2LUT == 0:
  $COEFFS = {6: ["0x1.FFFFFEp+0", "0x1.5554B0p+0", "0x1.555716p-1", "0x1.12278Ep-2", "0x1.6B7338p-4"]}[P]
  $MINUS_LN2 = ("-0x1.62E420p-1", "-0x1.FDF474p-22")
$elif LOG2LUT == 2:
  $COEFFS = {4: ["0x1.000002p+1", "0x1.557082p+0", "0x1.554F9Ap-1"]}[P]
  $MINUS_LN2 = ("-0x1.62E400p-1", "-0x1.7F7D1Cp-20")
$elif LOG2LUT == 3:
  $COEFFS = {4: ["0x1.000000p+1", "0x1.555C20p+0", "0x1.5558ECp-1"]}[P]
  $MINUS_LN2 = ("-0x1.62E400p-1", "-0x1.7F7D1Cp-20")
$POLYNOMIAL = "c%d" % P
$for i in reversed(range(2, P)):
$  POLYNOMIAL = "(c%d + t * %s)" % (i, POLYNOMIAL)
$if P == H + 1:
  $POLYNOMIAL = "(2 + t * %s)" % POLYNOMIAL
$else:
  $COEFFS = [float.hex(0.5 * float.fromhex(c)).upper().replace("0X", "0x").replace("0000000P", "p") for c in COEFFS]
$LUT_SUFFIX = "" if LOG2LUT == 0 else "lut%d_" % LUT
$POLY_SUFFIX = "p%dh%d%s_" % (P, H, "ps" if PS else "ts")
$PERM_SUFFIX = "perm_" if PERM else ""
$DIV_SUFFIX = DIV.lower()
$ISA = {0: "avx", 3: "fma3"}[FMA]
void xnn_math_f32_tanh__${ISA}_expm1minus_rr${RR}_${LUT_SUFFIX}${POLY_SUFFIX}${PERM_SUFFIX}${DIV_SUFFIX}(
    size_t n,
    const float* input,
    float* output)
{
  assert(n % sizeof(__m256) == 0);

  // Mask for the sign bit.
  const __m256 vsign_mask = _mm256_set1_ps(-0.0f);
  // The largest z for which tanhf(z) is saturated at -1.0f.
  const __m256 vsat_cutoff = _mm256_set1_ps(-0x1.205968p+3f);
  const __m256 vlog2e = _mm256_set1_ps(0x1.715476p+0f);
  $if LOG2LUT == 0:
    // Large number such that ulp(magic bias) == 0.5 and magic bias === 63.5 mod 2**21.
    const __m256 vmagic_bias = _mm256_set1_ps(0x1.8000FEp+22f);
  $else:
    // Large number such that ulp(magic bias) == exp2(${-1-LOG2LUT})
    const __m256 vmagic_bias = _mm256_set1_ps(0x1.800000p+${22-LOG2LUT}f);
  $if LOG2LUT != 0:
    $if PERM:
      // Table of exp2(k / 4) values decremented (as integer) by (k << 21), k = 0..3
      const __m128 vtable = _mm_set_ps(
        0x1.EE89FAp-1f, 0x1.EA09E6p-1f, 0x1.F06FE0p-1f, 0x1.000000p+0f);
    $else:
      // Mask for the lowest ${LOG2LUT} bits
      const __m128i vindex_mask = _mm_set1_epi32(0x${"%X" % (LUT-1)});
  $if RR == 1:
    const __m256 vminus_ln2 = _mm256_set1_ps(-0x1.62E430p-1f);
  $else:
    // Last ${4+LOG2LUT} bits are zeroes
    const __m256 vminus_ln2_hi = _mm256_set1_ps(${MINUS_LN2[0]}f);
    const __m256 vminus_ln2_lo = _mm256_set1_ps(${MINUS_LN2[1]}f);
  // Coefficients of polynomial approximation
  $if P == H + 1:
    //   exp(2t) - 1 ~ t * ${POLYNOMIAL}
  $else:
    //   exp(2t) - 1 ~ 2 * (t + t * (t * ${POLYNOMIAL}))
  // on [-log(2)/${4*LUT}, log(2)/${4*LUT}]
  $for i in reversed(range(len(COEFFS))):
    const __m256 vc${i+P-len(COEFFS)+1} = _mm256_set1_ps(${COEFFS[i]}f);
  $if P != H + 1:
    const __m256 vminus_one = _mm256_set1_ps(-1.0f);
  const __m256 vtwo = _mm256_set1_ps(2.0f);
  $if P == H + 1:
    const __m256 vminus_one = _mm256_set1_ps(-1.0f);

  for (; n != 0; n -= sizeof(__m256)) {
    const __m256 vx = _mm256_load_ps(input);
    input += 8;

    // General structure of the algorithm:
    //
    //           / expm1(2x) / (2 + expm1(2x)) if x <= 0
    //   f(x) :=
    //           \ -f(-x) if x >= 0
    //
    // First we compute f(z) := expm1(2z) / (2 + expm1(2z)) where z = -abs(x), then negate the result if x >= 0.
    __m256 vz = _mm256_or_ps(vx, vsign_mask);

    // Inverted mask for the sign of input: 0x00000000 for negative x, 0x80000000 for positive x.
    const __m256 vinvsignx = _mm256_xor_ps(vx, vz);

    $if SAT == "MINMAX":
      // The function saturates at -1 for large negative inputs: tanhf(z) == -1.0f for z <= sat_cutoff ~= -9.010913.
      // To guarantee this behaviour, we clip input z at sat_cutoff, and leverage the fact that for our implementation
      // tanhf(sat_cutoff) == -1.0f. NaN inputs are passed unchanged.
      vz = _mm256_max_ps(vsat_cutoff, vz);
    $elif SAT == "SELECT":
      // The function saturates at -1 for large negative inputs: tanhf(z) == -1.0f for z <= sat_cutoff ~= -9.010913.
      // To guarantee this behaviour, we compute the saturation mask here, and later use it to replace computed outputs
      // with the saturation value (-1). Note that for NaN inputs the saturation mask is inactive.
      const __m256 vm = _mm256_cmp_ps(vz, vsat_cutoff, _CMP_LE_OS);

    // Compute reduced argument n := round(z / log(2), ${1+LOG2LUT}).
    // We do it by adding a large number (magic bias), which cause rounding of the result to ${1+LOG2LUT} fractional ${"bit" if LOG2LUT == 0 else "bits"},
    // then subtracing the large number back. The trick with adding large number is valid only within certain bounds
    // (|z / log(2)| <= 2**${21-LOG2LUT}, i.e. |z| <= 0x1.62E43p+${20-LOG2LUT} = ${math.ldexp(float.fromhex("0x1.62E43p+20"), -LOG2LUT)}), but that is acceptable, because inputs x
    // outside of [-9.010913, 9.010913] (i.e. z outsize [-9.010913, 0]) saturate tanhf(x).
    $if LOG2LUT == 0:
      // Additionally, we fuse addition of the floating-point exponent bias (127) into the magic bias.
    // Note that addition-subtraction of the large number doesn't cause overflow for inputs in this range.
    $if FMA == 3:
      __m256 vn = _mm256_fmadd_ps(vz, vlog2e, vmagic_bias);
    $else:
      __m256 vn = _mm256_add_ps(_mm256_mul_ps(vz, vlog2e), vmagic_bias);

    $if LOG2LUT == 0:
      // Create a floating-point number s (scale) such that s == 2**(2n) for inputs which don't cause underflow, i.e.
      // -9.010913 <= z <= 0, and -13 <= n <= 0 accordingly.
      const __m128 vn_hi = _mm256_extractf128_ps(vn, 1);
      __m256 vs = _mm256_castps128_ps256(_mm_castsi128_ps(_mm_slli_epi32(_mm_castps_si128(_mm256_castps256_ps128(vn)), 23)));
      const __m128 vs_hi = _mm_castsi128_ps(_mm_slli_epi32(_mm_castps_si128(vn_hi), 23));
      vs = _mm256_insertf128_ps(vs, vs_hi, 1);
    $elif PERM:
      // Create a floating-point number s (scale) such that s := 2**(2n) for valid inputs, i.e. -9.010913 <= z <= 0. As
      // n has ${1+LOG2LUT} fractional bits, we split s == 2**(2n) = 2**int(2n) * 2**frac(2n). We create s in two steps:
      // 1. Fetch 2**frac(2n) from the table using the ${LOG2LUT} low bits of n, as integer. Note that the fetched values are in
      //    the [1.0, 2.0) range, i.e. their unbiased floating-point exponent is 0.
      // 2. Adjust fetched value by addition of int(2n) to its floating-point exponent. The result is always a normalized
      //    number, because for -9.010913 <= z <= 0 we have -13 <= int(n) <= 0, and thus the adjusted exponent is not
      //    lower than -13.
      //
      // Shift bits ${LOG2LUT}:${LOG2LUT+8} into 23:31 (position of floating-point exponent).
      const __m128 vn_hi = _mm256_extractf128_ps(vn, 1);
      __m128i ve_lo = _mm_slli_epi32(_mm_castps_si128(_mm256_castps256_ps128(vn)), ${23-LOG2LUT});
      __m128i ve_hi = _mm_slli_epi32(_mm_castps_si128(vn_hi), ${23-LOG2LUT});

      // Use bits 0:${LOG2LUT} bits of n, as integer, as an index for table lookup of l := 2**frac(2n).
      const __m128i vl_lo = _mm_castps_si128(_mm_permutevar_ps(vtable, _mm_castps_si128(_mm256_castps256_ps128(vn))));
      const __m128i vl_hi = _mm_castps_si128(_mm_permutevar_ps(vtable, _mm_castps_si128(vn_hi)));

      // Adjust exponent of the value l fetched from the table to get the final s value.
      const __m128 vs_lo = _mm_castsi128_ps(_mm_add_epi32(ve_lo, vl_lo));
      const __m128 vs_hi = _mm_castsi128_ps(_mm_add_epi32(ve_hi, vl_hi));
      const __m256 vs = _mm256_insertf128_ps(_mm256_castps128_ps256(vs_lo), vs_hi, 1);
    $else:
      // Create a floating-point number s (scale) such that s := 2**(2n) for valid inputs, i.e. -9.010913 <= z <= 0. As
      // n has ${1+LOG2LUT} fractional bits, we split s == 2**(2n) = 2**int(2n) * 2**frac(2n). We create s in two steps:
      // 1. Fetch 2**frac(2n) from the table using the ${LOG2LUT} low bits of n, as integer. Note that the fetched values are in
      //    the [1.0, 2.0) range, i.e. their unbiased floating-point exponent is 0.
      // 2. Adjust fetched value by addition of int(2n) to its floating-point exponent. The result is always a normalized
      //    number, because for -9.010913 <= z <= 0 we have -13 <= int(n) <= 0, and thus the adjusted exponent is not
      //    lower than -13.
      //
      // Shift bits ${LOG2LUT}:${LOG2LUT+8} into 23:31 (position of floating-point exponent).
      const __m128 vn_hi = _mm256_extractf128_ps(vn, 1);
      const __m128i ve_lo = _mm_slli_epi32(_mm_castps_si128(_mm256_castps256_ps128(vn)), ${23-LOG2LUT});
      const __m128i ve_hi = _mm_slli_epi32(_mm_castps_si128(vn_hi), ${23-LOG2LUT});

      // Use bits 0:${LOG2LUT} bits of n, as integer, as an index for table lookup of l := 2**frac(n).
      const __m128i vidx_lo = _mm_and_si128(_mm_castps_si128(_mm256_castps256_ps128(vn)), vindex_mask);
      const __m128i vidx_hi = _mm_and_si128(_mm_castps_si128(vn_hi), vindex_mask);
      #if XNN_ARCH_X86_64
        const uint64_t vidx01 = (uint64_t) _mm_cvtsi128_si64(vidx_lo);
        const uint64_t vidx45 = (uint64_t) _mm_cvtsi128_si64(vidx_hi);
        __m128i vl_lo = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx01]);
        __m128i vl_hi = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx45]);
        vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx01 >> 32)], 1);
        vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx45 >> 32)], 1);
        const uint64_t vidx23 = (uint64_t) _mm_extract_epi64(vidx_lo, 1);
        const uint64_t vidx67 = (uint64_t) _mm_extract_epi64(vidx_hi, 1);
        vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx23], 2);
        vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx67], 2);
        vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx23 >> 32)], 3);
        vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) (vidx67 >> 32)], 3);
      #else
        const uint32_t vidx0 = (uint32_t) _mm_cvtsi128_si32(vidx_lo);
        const uint32_t vidx4 = (uint32_t) _mm_cvtsi128_si32(vidx_hi);
        __m128i vl_lo = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx0]);
        __m128i vl_hi = _mm_cvtsi32_si128((int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx4]);
        const uint32_t vidx1 = (uint32_t) _mm_extract_epi32(vidx_lo, 1);
        const uint32_t vidx5 = (uint32_t) _mm_extract_epi32(vidx_hi, 1);
        vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx1], 1);
        vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx5], 1);
        const uint32_t vidx2 = (uint32_t) _mm_extract_epi32(vidx_lo, 2);
        const uint32_t vidx6 = (uint32_t) _mm_extract_epi32(vidx_hi, 2);
        vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx2], 2);
        vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx6], 2);
        const uint32_t vidx3 = (uint32_t) _mm_extract_epi32(vidx_lo, 3);
        const uint32_t vidx7 = (uint32_t) _mm_extract_epi32(vidx_hi, 3);
        vl_lo = _mm_insert_epi32(vl_lo, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx3], 3);
        vl_hi = _mm_insert_epi32(vl_hi, (int) xnn_table_exp2minus_k_over_${LUT}[(uint32_t) vidx7], 3);
      #endif

      // Adjust exponent of the value l fetched from the table to get the final s value.
      const __m128 vs_lo = _mm_castsi128_ps(_mm_add_epi32(vl_lo, ve_lo));
      const __m128 vs_hi = _mm_castsi128_ps(_mm_add_epi32(vl_hi, ve_hi));
      const __m256 vs = _mm256_insertf128_ps(_mm256_castps128_ps256(vs_lo), vs_hi, 1);

    // Subtract the large number back to get final n := round(z / log(2), ${1+LOG2LUT}) as a floating-point number.
    vn = _mm256_sub_ps(vn, vmagic_bias);

    // Compute reduced argument t := z - n * log(2).
    $if RR == 1:
      $if FMA == 3:
        const __m256 vt = _mm256_fmadd_ps(vn, vminus_ln2, vz);
      $else:
        const __m256 vt = _mm256_add_ps(_mm256_mul_ps(vn, vminus_ln2), vz);
    $else:
      // Use Cody-Waite range reduction method (note two constants to represent log(2)) to improve accuracy.
      __m256 vt = _mm256_add_ps(_mm256_mul_ps(vn, vminus_ln2_hi), vz);
      vt = _mm256_add_ps(_mm256_mul_ps(vn, vminus_ln2_lo), vt);

    // Compute degree-${P} polynomial approximation for exp(2t) - 1 on [-log(2)/${4*LUT}, log(2)/${4*LUT}].
    $if P == H + 1:
      //   P(t) = t * ${POLYNOMIAL}
      //        = t * p
    $else:
      //   P(t) = 2 * (t + t * (t * ${POLYNOMIAL}))
      //        = 2 * (t + t * p)
    $if FMA == 3:
      __m256 vp = vc${P};
      $for i in reversed(range(2, P)):
        vp = _mm256_fmadd_ps(vp, vt, vc${i});
    $else:
      __m256 vp = _mm256_add_ps(_mm256_mul_ps(vc${P}, vt), vc${P-1});
      $for i in reversed(range(2, P-1)):
        vp = _mm256_add_ps(_mm256_mul_ps(vp, vt), vc${i});
    $if P == H + 1:
      $if FMA == 3:
        vp = _mm256_fmadd_ps(vp, vt, vtwo);
      $else:
        vp = _mm256_add_ps(_mm256_mul_ps(vp, vt), vtwo);
    $else:
      vp = _mm256_mul_ps(vp, vt);

    // Reconstruct the exp(2z) - 1 value:
    $if P == H + 1:
      //   exp(2z) - 1 = s * (t * ${POLYNOMIAL} + 1) - 1
      //               = s * t * p + (s - 1)
      $if PS:
        //               = (s - 1) + (p * s) * t
      $else:
        //               = (s - 1) + (t * s) * p
    $else:
      //   exp(2z) - 1 = s * (2 * (t + t * (t * ${POLYNOMIAL})) + 1) - 1
      //               = s * (2 * (t + t * p) + 1) - 1
      //               = (s - 1) + 2 * ((t * s) + (t * s) * p)
    $if PS:
      const __m256 vps = _mm256_mul_ps(vp, vs);
    $else:
      const __m256 vts = _mm256_mul_ps(vt, vs);
    const __m256 vsmo = _mm256_add_ps(vs, vminus_one);
    $if P == H + 1:
      $if PS:
        $if FMA == 3:
          const __m256 vemo = _mm256_fmadd_ps(vt, vps, vsmo);
        $else:
          const __m256 vemo = _mm256_add_ps(_mm256_mul_ps(vt, vps), vsmo);
      $else:
        $if FMA == 3:
          const __m256 vemo = _mm256_fmadd_ps(vp, vts, vsmo);
        $else:
          const __m256 vemo = _mm256_add_ps(_mm256_mul_ps(vp, vts), vsmo);
    $else:
      $if FMA == 3:
        vp = _mm256_fmadd_ps(vp, vts, vts);
        const __m256 vemo = _mm256_fmadd_ps(vp, vtwo, vsmo);
      $else:
        vp = _mm256_add_ps(_mm256_mul_ps(vp, vts), vts);
        const __m256 vemo = _mm256_add_ps(_mm256_mul_ps(vp, vtwo), vsmo);

    // Denominator of the tanh fraction: exp(2z) + 1 = expm1(2z) + 2
    const __m256 vepo = _mm256_add_ps(vemo, vtwo);

    $if DIV == "DIV":
      // Reconstruct tanh(z) = expm1(2z) / (expm1(2z) + 2)
      __m256 vy = _mm256_div_ps(vemo, vepo);
    $else:
      // Use Newton-Raphson method (${"1 iteration" if DIV.startswith("NR1") else "2 iterations"}) to compute reciprocal of the denominator.
      // Note: 2 < exp(2z) + 1 <= 3, because z <= 0 and 0 < exp(2z) <= 1.
      // Thus the reciprocal of the denominator never overflows.
      __m256 vrepo = _mm256_rcp_ps(vepo);
      $if FMA:
        const __m256 verepo = _mm256_fnmsub_ps(vrepo, vepo, vminus_one);
        vrepo = _mm256_fmadd_ps(verepo, vrepo, vrepo);
      $else:
        vrepo = _mm256_mul_ps(vrepo, _mm256_sub_ps(vtwo, _mm256_mul_ps(vrepo, vepo)));
        $if DIV == "NR2":
          vrepo = _mm256_mul_ps(vrepo, _mm256_sub_ps(vtwo, _mm256_mul_ps(vrepo, vepo)));

      // Reconstruct tanh(z) := expm1(2z) / (2 + expm1(2z))
      __m256 vy = _mm256_mul_ps(vemo, vrepo);

      $if DIV.endswith("ADJ"):
        // Adjust reconstructred expm1(2z) / (2 + expm1(2z)) to match the correctly rounded division result
        const __m256 vey = _mm256_fnmadd_ps(vy, vepo, vemo);
        vy = _mm256_fmadd_ps(vey, vrepo, vy);

    $if SAT == "SELECT":
      // Saturate tanh(z) at -1 for large inputs.
      vy = _mm256_blendv_ps(vy, vminus_one, vm);

    // Reconstruct tanh(x):
    //
    //             / tanh(z) if x <= 0
    //   tanh(x) =
    //             \ -tanh(z) if x >= 0
    vy = _mm256_xor_ps(vy, vinvsignx);

    _mm256_store_ps(output, vy);
    output += 8;
  }
}
