// Copyright 2023 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert BATCH_TILE % 4 == 0
$assert BATCH_TILE >= 4
$SIMD_TILE = BATCH_TILE // 4
$assert ACCUMULATORS <= SIMD_TILE
$ABC = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
#include <assert.h>

#include <wasm_simd128.h>

#include "xnnpack/common.h"
#include "xnnpack/reduce.h"


$ACC_SUFFIX = "" if ACCUMULATORS == 1 else "_acc%d" % ACCUMULATORS
void xnn_f32_rsum_ukernel__wasmsimd_u${BATCH_TILE}${ACC_SUFFIX}(
    size_t batch,
    const float* input,
    float* output,
    const union xnn_f32_scaleminmax_params params[restrict XNN_MIN_ELEMENTS(1)])
{
  assert(batch != 0);
  assert(batch % sizeof(float) == 0);
  assert(input != NULL);
  assert(output != NULL);

  $for A in range(ACCUMULATORS):
    v128_t vacc${A} = wasm_f32x4_const_splat(0.0f);
  $if BATCH_TILE > 4:
    for (; batch >= ${BATCH_TILE} * sizeof(float); batch -= ${BATCH_TILE} * sizeof(float)) {
      const v128_t vt0 = wasm_v128_load(input);
      $for N in range(1, SIMD_TILE):
        const v128_t vt${N} = wasm_v128_load(input + ${N * 4});
      input += ${BATCH_TILE};

      $for N in range(SIMD_TILE):
        vacc${N % ACCUMULATORS} = wasm_f32x4_add(vacc${N % ACCUMULATORS}, vt${N});
    }
    $if ACCUMULATORS > 1:
      $ACC_SLICE = 1
      $while ACC_SLICE < ACCUMULATORS:
        $for A in range(0, ACCUMULATORS, ACC_SLICE * 2):
          $if A + ACC_SLICE < ACCUMULATORS:
            vacc${A} = wasm_f32x4_add(vacc${A}, vacc${A + ACC_SLICE});
        $ACC_SLICE *= 2
  for (; batch >= 4 * sizeof(float); batch -= 4 * sizeof(float)) {
    const v128_t vt = wasm_v128_load(input);
    input += 4;

    vacc0 = wasm_f32x4_add(vacc0, vt);
  }
  vacc0 = wasm_f32x4_add(vacc0, wasm_v64x2_shuffle(vacc0, vacc0, 1, 1));
  if XNN_UNLIKELY(batch & (2 * sizeof(float))) {
    const v128_t vt = wasm_v128_load64_zero(input);
    input += 2;
    vacc0 = wasm_f32x4_add(vacc0, vt);
  }
  vacc0 = wasm_f32x4_add(vacc0, wasm_v32x4_shuffle(vacc0, vacc0, 1, 1, 1, 1));
  if XNN_UNLIKELY(batch & (1 * sizeof(float))) {
    const v128_t vt = wasm_v128_load32_zero(input);
    vacc0 = wasm_f32x4_add(vacc0, vt);
  }
  const v128_t vscale = wasm_v128_load32_zero(&params->scalar.scale);
  const v128_t vmin = wasm_v128_load32_zero(&params->scalar.min);
  const v128_t vmax = wasm_v128_load32_zero(&params->scalar.max);
  vacc0 = wasm_f32x4_mul(vacc0, vscale);
  vacc0 = wasm_f32x4_max(vacc0, vmin);
  vacc0 = wasm_f32x4_min(vacc0, vmax);
  *output += wasm_f32x4_extract_lane(vacc0, 0);
}
