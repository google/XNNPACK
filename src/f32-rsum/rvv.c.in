// Copyright 2024 Imagination Technologies, Inc.
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert LMUL in [1, 2, 4, 8]
#include <assert.h>

#include <riscv_vector.h>
#include "xnnpack/common.h"
#include "xnnpack/reduce.h"

void xnn_f32_rsum_ukernel__rvv_u${LMUL}v(
    size_t batch,
    const float* input,
    float* output,
    const struct xnn_f32_scaleminmax_params params[restrict XNN_MIN_ELEMENTS(1)])
{
  assert(batch != 0);
  assert(batch % sizeof(float) == 0);
  assert(input != NULL);
  assert(output != NULL);

  const float scale = params->scalar.scale;
  const float min = params->scalar.min;
  const float max = params->scalar.max;

  batch >>= XNN_LOG2_SIZEOF_FLOAT;
  vfloat32m1_t acc_f32v = __riscv_vfmv_s_f_f32m1(0.f, __riscv_vsetvl_e32m1(batch));
  $if LMUL == 1 or LMUL == 2:
    size_t n = __riscv_vsetvl_e32m${LMUL}(batch);
    vfloat32m${LMUL}_t sum0_f32v = __riscv_vfmv_v_f_f32m${LMUL}(0.f, n);
    vfloat32m${LMUL}_t sum1_f32v = __riscv_vfmv_v_f_f32m${LMUL}(0.f, n);
    for (; batch >= n * 8; batch -= n * 8) {
      vfloat32m${LMUL}_t in0_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in1_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in2_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in3_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in4_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in5_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in6_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in7_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t sum01_f32v = __riscv_vfadd_vv_f32m${LMUL}(in0_f32v, in1_f32v, n);
      vfloat32m${LMUL}_t sum23_f32v = __riscv_vfadd_vv_f32m${LMUL}(in2_f32v, in3_f32v, n);
      vfloat32m${LMUL}_t sum45_f32v = __riscv_vfadd_vv_f32m${LMUL}(in4_f32v, in5_f32v, n);
      vfloat32m${LMUL}_t sum67_f32v = __riscv_vfadd_vv_f32m${LMUL}(in6_f32v, in7_f32v, n);
      vfloat32m${LMUL}_t sum0123_f32v = __riscv_vfadd_vv_f32m${LMUL}(sum01_f32v, sum23_f32v, n);
      vfloat32m${LMUL}_t sum4567_f32v = __riscv_vfadd_vv_f32m${LMUL}(sum45_f32v, sum67_f32v, n);
      sum0_f32v = __riscv_vfadd_vv_f32m${LMUL}(sum0_f32v, sum0123_f32v, n);
      sum1_f32v = __riscv_vfadd_vv_f32m${LMUL}(sum1_f32v, sum4567_f32v, n);
    }
    for (; batch >= n * 4; batch -= n * 4) {
      vfloat32m${LMUL}_t in0_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in1_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in2_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t in3_f32v = __riscv_vle32_v_f32m${LMUL}(input, n); input += n;
      vfloat32m${LMUL}_t sum01_f32v = __riscv_vfadd_vv_f32m${LMUL}(in0_f32v, in1_f32v, n);
      vfloat32m${LMUL}_t sum23_f32v = __riscv_vfadd_vv_f32m${LMUL}(in2_f32v, in3_f32v, n);
      sum0_f32v = __riscv_vfadd_vv_f32m${LMUL}(sum0_f32v, sum01_f32v, n);
      sum1_f32v = __riscv_vfadd_vv_f32m${LMUL}(sum1_f32v, sum23_f32v, n);
    }
    vfloat32m${LMUL}_t sum_f32v = __riscv_vfadd_vv_f32m${LMUL}(sum0_f32v, sum1_f32v, n);
    for (; batch > 0;) {
      size_t n1 = __riscv_vsetvl_e32m${LMUL}(batch);
      vfloat32m${LMUL}_t in_f32v = __riscv_vle32_v_f32m${LMUL}(input, n1); input += n1;
      sum_f32v = __riscv_vfadd_vv_f32m${LMUL}_tu(sum_f32v, sum_f32v, in_f32v, n1);
      batch -= n1;
    }
    acc_f32v = __riscv_vfredusum_vs_f32m${LMUL}_f32m1(sum_f32v, acc_f32v, n);
  $elif LMUL == 4:
    size_t n = __riscv_vsetvl_e32m4(batch);
    vfloat32m4_t sum0_f32v = __riscv_vfmv_v_f_f32m4(0.f, n);
    vfloat32m4_t sum1_f32v = __riscv_vfmv_v_f_f32m4(0.f, n);
    for (; batch >= n * 4; batch -= n * 4) {
      vfloat32m4_t in0_f32v = __riscv_vle32_v_f32m4(input, n); input += n;
      vfloat32m4_t in1_f32v = __riscv_vle32_v_f32m4(input, n); input += n;
      vfloat32m4_t in2_f32v = __riscv_vle32_v_f32m4(input, n); input += n;
      vfloat32m4_t in3_f32v = __riscv_vle32_v_f32m4(input, n); input += n;
      vfloat32m4_t sum01_f32v = __riscv_vfadd_vv_f32m4(in0_f32v, in1_f32v, n);
      vfloat32m4_t sum23_f32v = __riscv_vfadd_vv_f32m4(in2_f32v, in3_f32v, n);
      sum0_f32v = __riscv_vfadd_vv_f32m4(sum0_f32v, sum01_f32v, n);
      sum1_f32v = __riscv_vfadd_vv_f32m4(sum1_f32v, sum23_f32v, n);
    }
    vfloat32m4_t sum_f32v = __riscv_vfadd_vv_f32m4(sum0_f32v, sum1_f32v, n);
    for (; batch > 0;) {
      size_t n1 = __riscv_vsetvl_e32m4(batch);
      vfloat32m4_t in_f32v = __riscv_vle32_v_f32m4(input, n1); input += n1;
      sum_f32v = __riscv_vfadd_vv_f32m4_tu(sum_f32v, sum_f32v, in_f32v, n1);
      batch -= n1;
    }
    acc_f32v = __riscv_vfredusum_vs_f32m4_f32m1(sum_f32v, acc_f32v, n);
  $elif LMUL == 8:
    size_t n = __riscv_vsetvl_e32m8(batch);
    vfloat32m8_t sum_f32v = __riscv_vfmv_v_f_f32m8(0.f, n);
    for (; batch > 0;) {
      size_t n1 = __riscv_vsetvl_e32m8(batch);
      vfloat32m8_t in_f32v = __riscv_vle32_v_f32m8(input, n1); input += n1;
      sum_f32v = __riscv_vfadd_vv_f32m8_tu(sum_f32v, sum_f32v, in_f32v, n1);
      batch -= n1;
    }
    acc_f32v = __riscv_vfredusum_vs_f32m8_f32m1(sum_f32v, acc_f32v, n);
  vfloat32m1_t out_f32v = __riscv_vfmul_vf_f32m1(acc_f32v, scale, 1);
  out_f32v = __riscv_vfmin_vf_f32m1(__riscv_vfmax_vf_f32m1(out_f32v, min, n), max, n);
  *output += __riscv_vfmv_f_s_f32m1_f32(out_f32v);
}
