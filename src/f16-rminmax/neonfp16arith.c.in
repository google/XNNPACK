// Copyright 2023 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert BATCH_TILE % 8 == 0
$assert BATCH_TILE >= 8
$SIMD_TILE = BATCH_TILE // 8
$assert ACCUMULATORS <= SIMD_TILE
$assert OP in ["MAX", "MIN", "MINMAX"]
#include <assert.h>

#include <arm_neon.h>

#include <xnnpack/common.h>
#include <xnnpack/reduce.h>


$ACC_SUFFIX = "" if ACCUMULATORS == 1 else "_acc%d" % ACCUMULATORS
$EMIT_MIN = "MIN" in OP
$EMIT_MAX = "MAX" in OP
void xnn_f16_r${OP.lower()}_ukernel__neonfp16arith_u${BATCH_TILE}${ACC_SUFFIX}(
    size_t batch,
    const void* input,
    void* output,
    const union xnn_f16_default_params params[restrict XNN_MIN_ELEMENTS(1)]) XNN_OOB_READS
{
  assert(batch != 0);
  assert(batch % sizeof(uint16_t) == 0);
  assert(input != NULL);
  assert(output != NULL);

  const uint16_t* i = (const uint16_t*) input;
  uint16_t* o = (uint16_t*) output;
  $if EMIT_MIN:
    float16x8_t vmin0 = vreinterpretq_f16_u16(vld1q_dup_u16(i));
    $if EMIT_MAX:
      float16x8_t vmax0 = vmin0;
  $elif EMIT_MAX:
    float16x8_t vmax0 = vreinterpretq_f16_u16(vld1q_dup_u16(i));
  $if BATCH_TILE > 8:
    $for A in range(1, ACCUMULATORS):
      $if EMIT_MIN:
        float16x8_t vmin${A} = vmin0;
      $if EMIT_MAX:
        float16x8_t vmax${A} = vmax0;
    for (; batch >= ${BATCH_TILE} * sizeof(uint16_t); batch -= ${BATCH_TILE} * sizeof(uint16_t)) {
      $for N in range(SIMD_TILE):
        const float16x8_t vt${N} = vreinterpretq_f16_u16(vld1q_u16(i)); i += 8;

      $for N in range(SIMD_TILE):
        $if $EMIT_MIN:
          vmin${N % ACCUMULATORS} = vminq_f16(vmin${N % ACCUMULATORS}, vt${N});
        $if $EMIT_MAX:
          vmax${N % ACCUMULATORS} = vmaxq_f16(vmax${N % ACCUMULATORS}, vt${N});
    }
    $if ACCUMULATORS > 1:
      $ACC_SLICE = 1
      $while ACC_SLICE < ACCUMULATORS:
        $for A in range(0, ACCUMULATORS, ACC_SLICE * 2):
          $if A + ACC_SLICE < ACCUMULATORS:
            $if $EMIT_MIN:
              vmin${A} = vminq_f16(vmin${A}, vmin${A + ACC_SLICE});
            $if $EMIT_MAX:
              vmax${A} = vmaxq_f16(vmax${A}, vmax${A + ACC_SLICE});
        $ACC_SLICE *= 2
  for (; batch >= 8 * sizeof(uint16_t); batch -= 8 * sizeof(uint16_t)) {
    const float16x8_t vt = vreinterpretq_f16_u16(vld1q_u16(i)); i += 8;
    $if $EMIT_MIN:
      vmin0 = vminq_f16(vmin0, vt);
    $if $EMIT_MAX:
      vmax0 = vmaxq_f16(vmax0, vt);
  }
  $if $EMIT_MIN:
    float16x4_t vmin_lo = vmin_f16(vget_low_f16(vmin0), vget_high_f16(vmin0));
  $if $EMIT_MAX:
    float16x4_t vmax_lo = vmax_f16(vget_low_f16(vmax0), vget_high_f16(vmax0));

  if (XNN_UNLIKELY(batch != 0)) {
    const float16x8_t vt = vreinterpretq_f16_u16(vld1q_u16(i));
    float16x4_t vt_lo = vget_low_f16(vt);
    if (batch & (4 * sizeof(uint16_t))) {
      $if $EMIT_MIN:
        vmin_lo = vmin_f16(vmin_lo, vt_lo);
      $if $EMIT_MAX:
        vmax_lo = vmax_f16(vmax_lo, vt_lo);
      vt_lo = vget_high_f16(vt);
    }
    if (batch & (2 * sizeof(uint16_t))) {
      $if $EMIT_MIN:
        vmin_lo = vmin_f16(vmin_lo, vext_f16(vmin_lo, vt_lo, 2));
      $if $EMIT_MAX:
        vmax_lo = vmax_f16(vmax_lo, vext_f16(vmax_lo, vt_lo, 2));
      vt_lo = vext_f16(vt_lo, vt_lo, 2);
    }
    if (batch & (1 * sizeof(uint16_t))) {
      $if $EMIT_MIN:
        vmin_lo = vmin_f16(vmin_lo, vext_f16(vmin_lo, vt_lo, 1));
      $if $EMIT_MAX:
        vmax_lo = vmax_f16(vmax_lo, vext_f16(vmax_lo, vt_lo, 1));
    }
  }
  $if $EMIT_MIN:
    #if XNN_ARCH_ARM64 && defined(__GNUC__)
      *((__fp16*) o) = vminv_f16(vmin_lo);
    #else
      vmin_lo = vpmax_f16(vmin_lo, vmin_lo);
      vmin_lo = vpmax_f16(vmin_lo, vmin_lo);
      vst1_lane_u16(o, vreinterpret_u16_f16(vmin_lo), 0);
    #endif
    $if $EMIT_MAX:
      #if XNN_ARCH_ARM64 && defined(__GNUC__)
        *((__fp16*) o + 1) = vmaxv_f16(vmax_lo);
      #else
        vmax_lo = vpmax_f16(vmax_lo, vmax_lo);
        vmax_lo = vpmax_f16(vmax_lo, vmax_lo);
        vst1_lane_u16(o + 1, vreinterpret_u16_f16(vmax_lo), 0);
      #endif
  $elif $EMIT_MAX:
    #if XNN_ARCH_ARM64 && defined(__GNUC__)
      *((__fp16*) o) = vmaxv_f16(vmax_lo);
    #else
      vmax_lo = vpmax_f16(vmax_lo, vmax_lo);
      vmax_lo = vpmax_f16(vmax_lo, vmax_lo);
      vst1_lane_u16(o, vreinterpret_u16_f16(vmax_lo), 0);
    #endif
}
