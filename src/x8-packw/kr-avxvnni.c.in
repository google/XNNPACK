// Copyright 2024 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert NR in [8, 16]
$assert KR == 8
$assert DATATYPE in ["QS8", "X8", "QS4"]
$assert TYPE in ["int8_t"]
$assert IZP in [0, 128]

#include <assert.h>
#include <stddef.h>
#include <stdint.h>

#include <immintrin.h>

#include "src/xnnpack/packw.h"
#include "src/xnnpack/unaligned.h"
$if PREFETCH:
  #include "src/xnnpack/prefetch.h"
$if VARIANT == "MADD":
  // AVXVNNI replacement that uses vpmaddubsw.
  // u7 is vone.  s8 is int8 weights.
  static XNN_INTRINSIC
  __m256i mm256_dpbusd_epi32_madd(__m256i i32, const __m256i u7, const __m256i s8) {
    const __m256i vone = _mm256_set1_epi16(1);
    const __m256i i16 = _mm256_maddubs_epi16(u7, s8);  // u7 * s8 = s16
    const __m256i v = _mm256_madd_epi16(i16, vone);  // convert 16 bits to 32 bits
    return _mm256_add_epi32(i32, v);
  }

XNN_INLINE static uint64_t safe_load_u64(const void* address, size_t n) {
  uint64_t value = 0;
  assert(n <= sizeof(uint64_t));
  const uint8_t* bytes = (const uint8_t*) address;
  for (size_t i = 0; i < n; ++i) {
    value |= (uint64_t) bytes[i] << (i * 8);
  }
  return value;
}

$BTYPE = {"QS8": "int32_t", "QS4": "int32_t", "X8": "uint32_t"}[DATATYPE]
$WTYPE = {"QS8": "int8_t", "QS4": "uint8_t", "X8": "int8_t"}[DATATYPE]
$PACKEDWTYPE = {"QS8": "int8_t", "QS4": "void", "X8": "int8_t"}[DATATYPE]
$SCALETYPE = {"QS8": "void", "QS4": "float", "X8": "void"}[DATATYPE]
$PARAMTYPE = {"QS8": "void", "QS4": "struct xnn_qs8_qc4w_packing_params", "X8": "void"}[DATATYPE]
$if DATATYPE in ["QS8", "QS4"]:
  $_MM256_DPBUSD_EPI32 = "mm256_dpbusd_epi32_madd" if VARIANT == "MADD" else "_mm256_dpbusd_avx_epi32" if AVX == 2 else "_mm256_dpbusd_epi32"
  $ISA = "avx2" if VARIANT == "MADD" else "avxvnni" if AVX == 2 else "avx256vnni"
$else:
  $ISA = "avx2" if AVX == 2 else "avx256skx"
$DATATYPE_SPEC = "qs8_to_qu8" if IZP == 128 else {"QS8": "qs8", "QS4": "qs8_qc4w", "X8": "x8"}[DATATYPE]
$if DATATYPE in ["QS4"]:
  // Convert a vector from packed nibbles to planar, and accumulate sum
  static XNN_INTRINSIC
  __m256i xnn_packed2planar(__m256i* vacc, const __m256i v, const __m256i vmask, const __m256i vone) {
     const __m256i v0213 = _mm256_shuffle_epi32(v, _MM_SHUFFLE(3, 1, 2, 0));
     const __m256i vt = _mm256_slli_epi32(v0213, 4);     // isolate lower int4
     const __m256i vh = _mm256_and_si256(v0213, vmask);  // isolate upper int4
     const __m256i vl = _mm256_and_si256(vt, vmask);
     const __m256i v01 = _mm256_unpacklo_epi8(vl, vh);
     const __m256i v23 = _mm256_unpackhi_epi8(vl, vh);
     *vacc = ${_MM256_DPBUSD_EPI32}(*vacc, vone, v01);
     *vacc = ${_MM256_DPBUSD_EPI32}(*vacc, vone, v23);
     const __m256i vl01 = _mm256_srli_epi32(v01, 4);
     return _mm256_or_si256(vl01, v23);
  }

void xnn_${DATATYPE_SPEC}_packw_gemm_goi_ukernel_x${NR}c${KR}__${ISA}${"_madd" if VARIANT == "MADD" else ""}${"_prfm" if PREFETCH else ""}(
  size_t g,
  size_t nc,
  size_t kc,
  size_t nr,
  size_t kr,
  size_t sr,
  size_t n_stride,
  const ${WTYPE}* weights,
  const ${BTYPE}* bias,
  const ${SCALETYPE}* scale,
  ${PACKEDWTYPE}* packed_weights,
  size_t extra_bytes,
  const ${PARAMTYPE}* params)
{
  assert(g != 0);
  assert(nc != 0);
  assert(kc != 0);
  assert(nr == ${NR});
  assert(kr == ${KR});
  assert(sr == 1);
  assert(weights != NULL);
  assert(packed_weights != NULL);
  assert(params != NULL);
  $if DATATYPE == "QS4":
    assert(kc % 2 == 0);  // This kernel does not support odd KC
    kc >>= 1;  // KR=8 4 bit with 2 planes is 8 bytes.  Measure in bytes

  ${TYPE}* out = (${TYPE}*) packed_weights;
  const ${BTYPE}* b = (const ${BTYPE}*) bias;

  $if DATATYPE in ["QS8"]:
    const __m256i vone = _mm256_set1_epi8(1);
    const __m256i vzeropoint = _mm256_set1_epi32((int32_t) (params ? (((const struct xnn_qs8_packw_params*) params)->input_zero_point + ${IZP}): ${IZP}));
  $elif DATATYPE in ["QS4"]:
    const __m256i vone = _mm256_set1_epi8(1);
    const __m256i vmask = _mm256_set1_epi8(0xF0);
    const __m256i vzeropoint = _mm256_set1_epi32((int32_t) params->input_zero_point + ${IZP});
    const __m256i vkernel_zero_point = _mm256_set1_epi32((uint32_t) params->kernel_zero_point * 0x11111111);
    assert(params->kernel_zero_point == 8 || params->kernel_zero_point == 0);

  do {
    // NC main loop multiple of ${NR}
    const ${TYPE}* w0 = (const ${TYPE}*) weights;
    size_t n = nc;
    for (;n >= ${NR}; n -= ${NR}) {
      $for N in range(1, NR):
        const ${TYPE}* w${N} = w${N-1} + n_stride;

      $if DATATYPE in ["QS8", "QS4"]:
        ${BTYPE}* packed_b = (${BTYPE}*) out;
      if XNN_LIKELY(b != NULL) {
        $for N in range(0, NR, 8):
          const __m256i vb${N} = _mm256_loadu_si256((const __m256i*) (b + ${N}));
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i*) (out + ${N*4}), vb${N});
        b += ${NR};
      } else {
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i*) (out + ${N*4}), _mm256_setzero_si256());
      }
      out += ${NR} * sizeof(${BTYPE});

      $if PREFETCH:
        $for N in range(0, NR):
          $for OFFSET in range(0, 448, 64):
            xnn_prefetch_to_l1((const int8_t*) w${N} + ${OFFSET});

      $if DATATYPE in ["QS8", "QS4"]:
        $for N in range(0, NR, 4):
          __m256i vacc${N} = _mm256_setzero_si256();

      size_t k = kc;
      // KC main loop multiple of ${NR}x${4 * KR}
      for (; k >= ${4 * KR}; k -= ${4 * KR}) {
        $for N in range(NR):
          $if DATATYPE in ["QS4"]:
            const __m256i v${N}_0123 = _mm256_xor_si256(_mm256_loadu_si256((const __m256i*) w${N}), vkernel_zero_point);  // uint4 -> int4
          $else:
            const __m256i v${N}_0123 = _mm256_loadu_si256((const __m256i*) w${N});

        $for N in range(0, NR, 2):
          const __m256i v${N}${N+1}_02 = _mm256_unpacklo_epi64(v${N}_0123, v${N+1}_0123);
          const __m256i v${N}${N+1}_13 = _mm256_unpackhi_epi64(v${N}_0123, v${N+1}_0123);

        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $for N in range(0, NR, 4):
          $for I in range(0, 4):
            __m256i v${N}_${I} = _mm256_permute2f128_si256(v${N}${N+1}_${I%2}${I%2+2}, v${N+2}${N+3}_${I%2}${I%2+2}, _MM_SHUFFLE(0, ${I//2+2}, 0, ${I//2}));

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 4):
            $for I in range(0, 4):
              vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N}_${I});
        $elif DATATYPE in ["QS4"]:
          $for N in range(0, NR, 4):
            $for I in range(0, 4):
              v${N}_${I} = xnn_packed2planar(&vacc${N}, v${N}_${I}, vmask, vone);

        $for I in range(0, 4):
          $for N in range(0, NR, 4):
            _mm256_storeu_si256((__m256i *)&out[${(I*NR + N)*KR}],  v${N}_${I});

        $for N in range(NR):
          w${N} += ${4 * KR};
        out += ${4*NR*KR};
      }

      // KC main loop multiple of ${NR}x${KR}
      for (; k >= ${KR}; k -= ${KR}) {
        $for N in range(0, NR, 4):
          __m256i v${N} = _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N}));
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+1})), 0x0C);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+2})), 0x30);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+3})), 0xC0);
        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 4):
            vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});
        $elif DATATYPE in ["QS4"]:
          $for N in range(0, NR, 4):
            v${N} = _mm256_xor_si256(v${N}, vkernel_zero_point);    // uint4 -> int4
            v${N} = xnn_packed2planar(&vacc${N}, v${N}, vmask, vone);

        $for N in range(0, NR, 4):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        $for N in range(NR):
          w${N} += ${KR};
        out += ${NR*KR};
      }

      // KC remainder of 1..${KR-1}
      if (k != 0) {
        assert(k >= 1 && k <= ${KR-1});

        $for N in range(0, NR, 4):
          __m256i v${N} = _mm256_set1_epi64x((int64_t) safe_load_u64(w${N}, k));
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) safe_load_u64(w${N+1}, k)), 0x0C);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) safe_load_u64(w${N+2}, k)), 0x30);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) safe_load_u64(w${N+3}, k)), 0xC0);

        $for N in range(NR):
          w${N} += k;

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 4):
            vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});
        $elif DATATYPE in ["QS4"]:
          $for N in range(0, NR, 4):
            v${N} = _mm256_xor_si256(v${N}, vkernel_zero_point);    // uint4 -> int4
            v${N} = xnn_packed2planar(&vacc${N}, v${N}, vmask, vone);

        $for N in range(0, NR, 4):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        out += ${NR*KR};
      }

      $if DATATYPE in ["QS8", "QS4"]:
        $for N in range(0, NR, 8):
          __m256i vksum${N} = _mm256_hadd_epi32(vacc${N}, vacc${N+4});
          vksum${N} = _mm256_permute4x64_epi64(vksum${N}, _MM_SHUFFLE(3, 1, 2, 0));
        $for N in range(0, NR, 8):
          vksum${N} = _mm256_mullo_epi32(vksum${N}, vzeropoint);
        $for N in range(0, NR, 8):
          __m256i vpack${N} =  _mm256_loadu_si256((const __m256i*) (packed_b + ${N}));
        $for N in range(0, NR, 8):
          vpack${N} = _mm256_sub_epi32(vpack${N}, vksum${N});
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i *) (packed_b + ${N}), vpack${N});
      out = (${TYPE}*) ((uintptr_t) out + extra_bytes);
      w0 = w${NR-1};
    }

    // NC remainder (1..${NR-1})
    // Same as main loop except bias is copied and w pointers are clamped
    if XNN_UNLIKELY(n != 0) {
      assert(n >= 1 && n <= ${NR-1});
      // Clamp weight pointers for NC remainder
      $for N in range(1, NR):
        const ${TYPE}* w${N} = w${N-1} + n_stride;
        $if N % 2 == 0:
          if XNN_UNPREDICTABLE(n <= ${N}) {
            w${N} = w${N-1};
          }
        $else:
          if XNN_UNPREDICTABLE(n < ${N+1}) {
            w${N} = w${N-1};
          }

      $if DATATYPE in ["QS8", "QS4"]:
        ${BTYPE}* packed_b = (${BTYPE}*) out;
      if XNN_LIKELY(b != NULL) {
        size_t nb = n;
        for (nb = 0; nb < n; ++nb) {
          ((${BTYPE}*) out)[nb] = b[nb];
        }
        b += n;
      } else {
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i*) (out + ${N*4}), _mm256_setzero_si256());
      }
      out += ${NR} * sizeof(${BTYPE});

      $if PREFETCH:
        $for N in range(0, NR):
          $for OFFSET in range(0, 448, 64):
            xnn_prefetch_to_l1((const int8_t*) w${N} + ${OFFSET});

      $if DATATYPE in ["QS8", "QS4"]:
        $for N in range(0, NR, 4):
          __m256i vacc${N} = _mm256_setzero_si256();

      size_t k = kc;
      // KC main loop multiple of ${NR}x${4 * KR}
      for (; k >= ${4 * KR}; k -= ${4 * KR}) {
        $for N in range(NR):
          $if DATATYPE in ["QS4"]:
            const __m256i v${N}_0123 = _mm256_xor_si256(_mm256_loadu_si256((const __m256i*) w${N}), vkernel_zero_point);  // uint4 -> int4
          $else:
            const __m256i v${N}_0123 = _mm256_loadu_si256((const __m256i*) w${N});

        $for N in range(0, NR, 2):
          const __m256i v${N}${N+1}_02 = _mm256_unpacklo_epi64(v${N}_0123, v${N+1}_0123);
          const __m256i v${N}${N+1}_13 = _mm256_unpackhi_epi64(v${N}_0123, v${N+1}_0123);

        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $for N in range(0, NR, 4):
          $for I in range(0, 4):
            __m256i v${N}_${I} = _mm256_permute2f128_si256(v${N}${N+1}_${I%2}${I%2+2}, v${N+2}${N+3}_${I%2}${I%2+2}, _MM_SHUFFLE(0, ${I//2+2}, 0, ${I//2}));

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 4):
            $for I in range(0, 4):
              vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N}_${I});
        $elif DATATYPE in ["QS4"]:
          $for N in range(0, NR, 4):
            $for I in range(0, 4):
              v${N}_${I} = xnn_packed2planar(&vacc${N}, v${N}_${I}, vmask, vone);

        $for I in range(0, 4):
          $for N in range(0, NR, 4):
            _mm256_storeu_si256((__m256i *)&out[${(I*NR + N)*KR}],  v${N}_${I});

        $for N in range(NR):
          w${N} += ${4 * KR};
        out += ${4*NR*KR};
      }

      // KC main loop multiple of ${NR}x${KR}
      for (; k >= ${KR}; k -= ${KR}) {
        $for N in range(0, NR, 4):
          __m256i v${N} = _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N}));
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+1})), 0x0C);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+2})), 0x30);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+3})), 0xC0);
        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 4):
            vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});
        $elif DATATYPE in ["QS4"]:
          $for N in range(0, NR, 4):
            v${N} = _mm256_xor_si256(v${N}, vkernel_zero_point);    // uint4 -> int4
            v${N} = xnn_packed2planar(&vacc${N}, v${N}, vmask, vone);

        $for N in range(0, NR, 4):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        $for N in range(NR):
          w${N} += ${KR};
        out += ${NR*KR};
      }

      // KC remainder of 1..${KR-1}
      if (k != 0) {
        assert(k >= 1 && k <= ${KR-1});

        $for N in range(0, NR, 4):
          __m256i v${N} = _mm256_set1_epi64x((int64_t) safe_load_u64(w${N}, k));
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) safe_load_u64(w${N+1}, k)), 0x0C);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) safe_load_u64(w${N+2}, k)), 0x30);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) safe_load_u64(w${N+3}, k)), 0xC0);

        $for N in range(NR):
          w${N} += k;

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 4):
            vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});
        $elif DATATYPE in ["QS4"]:
          $for N in range(0, NR, 4):
            v${N} = _mm256_xor_si256(v${N}, vkernel_zero_point);    // uint4 -> int4
            v${N} = xnn_packed2planar(&vacc${N}, v${N}, vmask, vone);

        $for N in range(0, NR, 4):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        out += ${NR*KR};
      }

      $if DATATYPE in ["QS8", "QS4"]:
        $for N in range(0, NR, 8):
          __m256i vksum${N} = _mm256_hadd_epi32(vacc${N}, vacc${N+4});
          vksum${N} = _mm256_permute4x64_epi64(vksum${N}, _MM_SHUFFLE(3, 1, 2, 0));
        $for N in range(0, NR, 8):
          vksum${N} = _mm256_mullo_epi32(vksum${N}, vzeropoint);
        $for N in range(0, NR, 8):
          __m256i vpack${N} =  _mm256_loadu_si256((const __m256i*) (packed_b + ${N}));
        $for N in range(0, NR, 8):
          vpack${N} = _mm256_sub_epi32(vpack${N}, vksum${N});
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i *) (packed_b + ${N}), vpack${N});

      out = (${TYPE}*) ((uintptr_t) out + extra_bytes);
    }

    weights = (const ${WTYPE}*)((intptr_t) weights + nc * kc);
  } while (--g != 0);
}