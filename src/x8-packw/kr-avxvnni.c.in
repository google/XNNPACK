// Copyright 2024 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert NR in [8, 16]
$assert KR == 8
$assert TYPE in ["int8_t"]
$assert IZP in [0, 128]

#include <assert.h>
#include <stddef.h>
#include <stdint.h>

#include <immintrin.h>

#include "xnnpack/packw.h"
#include "xnnpack/unaligned.h"
$if PREFETCH:
  #include "xnnpack/prefetch.h"


$BTYPE = {"int8_t": "uint32_t"}[TYPE]
$WTYPE = {"int8_t": "int8_t"}[TYPE]
$_MM256_DPBUSD_EPI32 = "_mm256_dpbusd_avx_epi32" if AVX == 2 else "_mm256_dpbusd_epi32"
$ISA = "avxvnni" if AVX == 2 else "avx256vnni"
void xnn_qs8${"_to_qu8" if IZP == 128 else ""}_packw_gemm_goi_ukernel_x${NR}c${KR}__${ISA}${"_prfm" if PREFETCH else ""}(
  size_t g,
  size_t nc,
  size_t kc,
  size_t nr,
  size_t kr,
  size_t sr,
  const ${WTYPE}* weights,
  const int32_t* bias,
  const void* scale,
  ${WTYPE}* packed_weights,
  size_t extra_bytes,
  const void* params) XNN_OOB_READS
{
  assert(g != 0);
  assert(nc != 0);
  assert(kc != 0);
  assert(nr == ${NR});
  assert(kr == ${KR});
  assert(sr == 1);
  assert(weights != NULL);
  assert(packed_weights != NULL);

  const __m256i vone = _mm256_set1_epi8(1);
  ${TYPE}* out = (${TYPE}*) packed_weights;
  const ${BTYPE}* b = (const ${BTYPE}*) bias;
  const uint32_t izp = (uint32_t) (params ? (((const struct xnn_qs8_packw_params*) params)->input_zero_point + ${IZP}): ${IZP});
  __m256i vzeropoint = _mm256_set1_epi32((int32_t) izp);

  do {
    // NC main loop multiple of ${NR}
    const ${TYPE}* w0 = (const ${TYPE}*) weights;
    size_t n = nc;
    for (;n >= ${NR}; n -= ${NR}) {
      int32_t* packed_b = (int32_t*) out;
      if XNN_LIKELY(b != NULL) {
        $for N in range(0, NR, 8):
          const __m256i vb${N} = _mm256_loadu_si256((const __m256i*) (b + ${N}));
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i*) (out + ${N*4}), vb${N});
        b += ${NR};
      } else {
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i*) (out + ${N*4}), _mm256_setzero_si256());
      }
      out += ${NR} * sizeof(${BTYPE});

      $for N in range(1, NR):
        const ${TYPE}* w${N} = w${N-1} + kc;
      $if PREFETCH:
        $for N in range(0, NR):
          $for OFFSET in range(0, 448, 64):
            xnn_prefetch_to_l1((const int8_t*) w${N} + ${OFFSET});

      $for N in range(0, NR, 4):
        __m256i vacc${N} = _mm256_setzero_si256();

      size_t k = kc;
      // KC main loop multiple of ${NR}x${4 * KR}
      for (; k >= ${4 * KR}; k -= ${4 * KR}) {
        $for N in range(NR):
          const __m256i v${N}_0123 = _mm256_loadu_si256((const __m256i*) w${N});

        $for N in range(0, NR, 2):
          const __m256i v${N}${N+1}_02 = _mm256_unpacklo_epi64(v${N}_0123, v${N+1}_0123);
          const __m256i v${N}${N+1}_13 = _mm256_unpackhi_epi64(v${N}_0123, v${N+1}_0123);

        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $for N in range(0, NR, 4):
          $for I in range(0, 4):
            const __m256i v${N}_${I} = _mm256_permute2f128_si256(v${N}${N+1}_${I%2}${I%2+2}, v${N+2}${N+3}_${I%2}${I%2+2}, _MM_SHUFFLE(0, ${I//2+2}, 0, ${I//2}));

        $for N in range(0, NR, 4):
          $for I in range(0, 4):
            vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N}_${I});

        $for I in range(0, 4):
          $for N in range(0, NR, 4):
            _mm256_storeu_si256((__m256i *)&out[${(I*NR + N)*KR}],  v${N}_${I});

        $for N in range(NR):
          w${N} += ${4 * KR};
        out += ${4*NR*KR};
      }

      // KC main loop multiple of ${NR}x${KR}
      for (; k >= ${KR}; k -= ${KR}) {
        $for N in range(0, NR, 4):
          __m256i v${N} = _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N}));
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+1})), 0x0C);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+2})), 0x30);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+3})), 0xC0);
        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $for N in range(0, NR, 4):
          vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});

        $for N in range(0, NR, 4):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        $for N in range(NR):
          w${N} += ${KR};
        out += ${NR*KR};
      }

      // KC remainder of 1..${KR-1}
      if (k != 0) {
        assert(k >= 1 && k <= ${KR-1});

        const __m256i vmask = _mm256_srli_epi64(_mm256_set1_epi32(-1), (8 - k) * sizeof(int8_t) * 8);

        $for N in range(0, NR, 4):
          __m256i v${N} = _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N}));
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+1})), 0x0C);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+2})), 0x30);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+3})), 0xC0);
          v${N} = _mm256_and_si256(v${N}, vmask);

        $for N in range(NR):
          w${N} += k;

        $for N in range(0, NR, 4):
          vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});

        $for N in range(0, NR, 4):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        out += ${NR*KR};
      }

      $for N in range(0, NR, 8):
        __m256i vksum${N} = _mm256_hadd_epi32(vacc${N}, vacc${N+4});
        vksum${N} = _mm256_permute4x64_epi64(vksum${N}, _MM_SHUFFLE(3, 1, 2, 0));
      $for N in range(0, NR, 8):
        vksum${N} = _mm256_mullo_epi32(vksum${N}, vzeropoint);
      $for N in range(0, NR, 8):
        __m256i vpack${N} =  _mm256_loadu_si256((const __m256i*) (packed_b + ${N}));
      $for N in range(0, NR, 8):
        vpack${N} = _mm256_sub_epi32(vpack${N}, vksum${N});
      $for N in range(0, NR, 8):
        _mm256_storeu_si256((__m256i *) (packed_b + ${N}), vpack${N});
      out = (${TYPE}*) ((uintptr_t) out + extra_bytes);
      w0 = w${NR-1};
    }

    // NC remainder (1..${NR-1})
    if XNN_UNLIKELY(n != 0) {
      assert(n >= 1 && n <= ${NR-1});

      int32_t* packed_b = (int32_t*) out;
      if XNN_LIKELY(b != NULL) {
        size_t nb = n;
        do {
          *((${BTYPE}*) out) = *b++;
          out += sizeof(${BTYPE});
        } while (--nb != 0);
      } else {
        size_t nb = n;
        do {
          *((${BTYPE}*) out) = 0;
          out += sizeof(${BTYPE});
        } while (--nb != 0);
      }
      out += (${NR} - n) * sizeof(${BTYPE});

      $for N in range(1, NR):
        const ${TYPE}* w${N} = w${N-1} + kc;
        $if N % 2 == 0:
          if XNN_UNPREDICTABLE(n <= ${N}) {
            w${N} = w${N-1};
          }
        $else:
          if XNN_UNPREDICTABLE(n < ${N+1}) {
            w${N} = w${N-1};
          }
      $if PREFETCH:
        $for N in range(0, NR):
          xnn_prefetch_to_l1((const int8_t*) w${N});
          xnn_prefetch_to_l1((const int8_t*) w${N} + 64);

      $for N in range(0, NR, 4):
        __m256i vacc${N} = _mm256_setzero_si256();

      // KC main loop multiple of ${NR}x${KR}
      size_t k = kc;
      for (; k >= ${KR}; k -= ${KR}) {
        $for N in range(0, NR, 4):
          __m256i v${N} = _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N}));
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+1})), 0x0C);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+2})), 0x30);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+3})), 0xC0);
        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $for N in range(0, NR, 4):
          vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});

        $for N in range(0, NR, 4):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        $for N in range(NR):
          w${N} += ${KR};
        out += ${NR*KR};
      }

      // KC remainder of 1..${KR-1}
      if (k != 0) {
        assert(k >= 1 && k <= ${KR-1});

        const __m256i vmask = _mm256_srli_epi64(_mm256_set1_epi32(-1), (8 - k) * sizeof(int8_t) * 8);

        $for N in range(0, NR, 4):
          __m256i v${N} = _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N}));
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+1})), 0x0C);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+2})), 0x30);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi64x((int64_t) unaligned_load_u64(w${N+3})), 0xC0);
          v${N} = _mm256_and_si256(v${N}, vmask);

        $for N in range(NR):
          w${N} += k;

        $for N in range(0, NR, 4):
          vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});

        $for N in range(0, NR, 4):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        out += ${NR*KR};
      }

      $for N in range(0, NR, 8):
        __m256i vksum${N} = _mm256_hadd_epi32(vacc${N}, vacc${N+4});
        vksum${N} = _mm256_permute4x64_epi64(vksum${N}, _MM_SHUFFLE(3, 1, 2, 0));
      $for N in range(0, NR, 8):
        vksum${N} = _mm256_mullo_epi32(vksum${N}, vzeropoint);
      $for N in range(0, NR, 8):
        __m256i vpack${N} =  _mm256_loadu_si256((const __m256i*) (packed_b + ${N}));
      $for N in range(0, NR, 8):
        vpack${N} = _mm256_sub_epi32(vpack${N}, vksum${N});
      $for N in range(0, NR, 8):
        _mm256_storeu_si256((__m256i *) (packed_b + ${N}), vpack${N});
      out = (${TYPE}*) ((uintptr_t) out + extra_bytes);
    }

    weights += nc * kc;
  } while (--g != 0);
}
