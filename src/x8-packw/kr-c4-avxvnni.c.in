// Copyright 2024 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert NR == 8
$assert KR == 4
$assert DATATYPE in ["QS8", "X8"]
$assert TYPE in ["int8_t"]
$assert IZP in [0, 128]

#include <assert.h>
#include <stddef.h>
#include <stdint.h>

#include <immintrin.h>

#include "xnnpack/packw.h"
#include "xnnpack/unaligned.h"
$if PREFETCH:
  #include "xnnpack/prefetch.h"


$BTYPE = {"QS8": "int32_t", "X8": "uint32_t"}[DATATYPE]
$WTYPE = "int8_t"
$if DATATYPE in ["QS8"]:
  $_MM256_DPBUSD_EPI32 = "_mm256_dpbusd_avx_epi32" if AVX == 2 else "_mm256_dpbusd_epi32"
  $ISA = "avxvnni" if AVX == 2 else "avx256vnni"
$else:
  $ISA = "avx2" if AVX == 2 else "avx256skx"
XNN_INLINE static uint32_t safe_load_u32(const void* src, size_t k) {
  uint32_t value = 0;
  const uint8_t* bytes = (const uint8_t*)src;
  for (size_t i = 0; i < k; ++i) {
    value |= (uint32_t) bytes[i] << (i * 8);
  }
  return value;
}

void xnn_${DATATYPE.lower()}${"_to_qu8" if IZP == 128 else ""}_packw_gemm_goi_ukernel_x${NR}c${KR}__${ISA}${"_prfm" if PREFETCH else ""}(
  size_t g,
  size_t nc,
  size_t kc,
  size_t nr,
  size_t kr,
  size_t sr,
  const ${WTYPE}* weights,
  const ${BTYPE}* bias,
  const void* scale,
  ${WTYPE}* packed_weights,
  size_t extra_bytes,
  const void* params)
{
  assert(g != 0);
  assert(nc != 0);
  assert(kc != 0);
  assert(nr == ${NR});
  assert(kr == ${KR});
  assert(sr == 1);
  assert(weights != NULL);
  assert(packed_weights != NULL);

  ${TYPE}* out = (${TYPE}*) packed_weights;
  const ${BTYPE}* b = (const ${BTYPE}*) bias;

  $if DATATYPE in ["QS8"]:
    const __m256i vone = _mm256_set1_epi8(1);
    const uint32_t izp = (uint32_t) (params ? (((const struct xnn_qs8_packw_params*) params)->input_zero_point + ${IZP}): ${IZP});
    __m256i vzeropoint = _mm256_set1_epi32((int32_t) izp);

  do {
    // NC main loop multiple of ${NR}
    const ${TYPE}* w0 = (const ${TYPE}*) weights;
    size_t n = nc;
    for (; n >= ${NR}; n -= ${NR}) {
      $if DATATYPE in ["QS8"]:
        ${BTYPE}* packed_b = (${BTYPE}*) out;
      if XNN_LIKELY(b != NULL) {
        $for N in range(0, NR, 8):
          const __m256i vb${N} = _mm256_loadu_si256((const __m256i*) (b + ${N}));
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i*) (out + ${N*4}), vb${N});
        b += ${NR};
      } else {
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i*) (out + ${N*4}), _mm256_setzero_si256());
      }
      out += ${NR} * sizeof(${BTYPE});

      $for N in range(1, NR):
        const ${TYPE}* w${N} = w${N-1} + kc;
      $if PREFETCH:
        $for N in range(0, NR):
          $for OFFSET in range(0, 256, 64):
            xnn_prefetch_to_l1((const int8_t*) w${N} + ${OFFSET});

      $if DATATYPE in ["QS8"]:
        $for N in range(0, NR, 8):
          __m256i vacc${N} = _mm256_setzero_si256();

      size_t k = kc;
      // KC main loop multiple of ${NR}x${8 * KR}
      for (; k >= ${8 * KR}; k -= ${8 * KR}) {
        $for N in range(NR):
          const __m256i v${N}_01234567 = _mm256_loadu_si256((const __m256i*) w${N});

        $for N in range(0, NR, 2):
          const __m256i v${N}${N+1}_0145 = _mm256_unpacklo_epi32(v${N}_01234567, v${N+1}_01234567);
          const __m256i v${N}${N+1}_2367 = _mm256_unpackhi_epi32(v${N}_01234567, v${N+1}_01234567);

        $for N in range(0, NR, 4):
          const __m256i v${N}${N+2}_02 = _mm256_unpacklo_epi64(v${N}${N+1}_0145, v${N+2}${N+3}_0145);
          const __m256i v${N}${N+2}_13 = _mm256_unpackhi_epi64(v${N}${N+1}_0145, v${N+2}${N+3}_0145);
          const __m256i v${N+1}${N+3}_02 = _mm256_unpacklo_epi64(v${N}${N+1}_2367, v${N+2}${N+3}_2367);
          const __m256i v${N+1}${N+3}_13 = _mm256_unpackhi_epi64(v${N}${N+1}_2367, v${N+2}${N+3}_2367);

       $for N in range(0, NR // 4):
         $for I in range(0, 2):
           $C = N*2+I
           const __m256i v${C}${C+4}_0 = _mm256_permute2f128_si256(v${N}${N+2}_${I}${I+2}, v${N+4}${N+6}_${I}${I+2}, _MM_SHUFFLE(0, 2, 0, 0));
           const __m256i v${C}${C+4}_1 = _mm256_permute2f128_si256(v${N}${N+2}_${I}${I+2}, v${N+4}${N+6}_${I}${I+2}, _MM_SHUFFLE(0, 3, 0, 1));

        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 8):
            $for I in range(0, 2):
              $for J in range(0, 4):
                vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${J}${J+4}_${I});

        $for I in range(0, 2):
          $for N in range(0, KR):
            _mm256_storeu_si256((__m256i *)&out[${(I*KR + N)*8*KR}],  v${N}${N+4}_${I});

        $for N in range(NR):
          w${N} += ${8 * KR};
        out += ${8*NR*KR};
      }

      // KC main loop multiple of ${NR}x${KR}
      for (; k >= ${KR}; k -= ${KR}) {
        __m256i v0 = _mm256_set1_epi32((int32_t) unaligned_load_u32(w0));
        v0 = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) unaligned_load_u32(w1)), 0x02);
        v0 = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) unaligned_load_u32(w2)), 0x04);
        v0 = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) unaligned_load_u32(w3)), 0x08);
        v0 = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) unaligned_load_u32(w4)), 0x10);
        v0 = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) unaligned_load_u32(w5)), 0x20);
        v0 = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) unaligned_load_u32(w6)), 0x40);
        v0 = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) unaligned_load_u32(w7)), 0x80);
        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 8):
            vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});

        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        $for N in range(NR):
          w${N} += ${KR};
        out += ${NR*KR};
      }

      // KC remainder of 1..${KR-1}
      if (k != 0) {
        assert(k >= 1 && k <= ${KR-1});

        const __m256i vmask = _mm256_set1_epi32((1u << (k * sizeof(int8_t) * 8)) - 1);

        $for N in range(0, NR, 8):
          __m256i v${N} = _mm256_set1_epi32((int32_t) safe_load_u32(w${N}, k));
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+1}, k)), 0x02);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+2}, k)), 0x04);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+3}, k)), 0x08);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+4}, k)), 0x10);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+5}, k)), 0x20);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+6}, k)), 0x40);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+7}, k)), 0x80);
          v${N} = _mm256_and_si256(v${N}, vmask);

        $for N in range(NR):
          w${N} += k;

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 8):
            vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});

        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        out += ${NR*KR};
      }

      $if DATATYPE in ["QS8"]:
        $for N in range(0, NR, 8):
          __m256i vksum${N} = _mm256_mullo_epi32(vacc${N}, vzeropoint);
        $for N in range(0, NR, 8):
          __m256i vpack${N} =  _mm256_loadu_si256((const __m256i*) (packed_b + ${N}));
        $for N in range(0, NR, 8):
          vpack${N} = _mm256_sub_epi32(vpack${N}, vksum${N});
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i *) (packed_b + ${N}), vpack${N});
      out = (${TYPE}*) ((uintptr_t) out + extra_bytes);
      w0 = w${NR-1};
    }

    // NC remainder (1..${NR-1})
    if XNN_UNLIKELY(n != 0) {
      assert(n >= 1 && n <= ${NR-1});

      $if DATATYPE in ["QS8"]:
        ${BTYPE}* packed_b = (${BTYPE}*) out;
      if XNN_LIKELY(b != NULL) {
        size_t nb = n;
        do {
          *((${BTYPE}*) out) = *b++;
          out += sizeof(${BTYPE});
        } while (--nb != 0);
      } else {
        size_t nb = n;
        do {
          *((${BTYPE}*) out) = 0;
          out += sizeof(${BTYPE});
        } while (--nb != 0);
      }
      out += (${NR} - n) * sizeof(${BTYPE});

      $for N in range(1, NR):
        const ${TYPE}* w${N} = w${N-1} + kc;
        $if N % 2 == 0:
          if XNN_UNPREDICTABLE(n <= ${N}) {
            w${N} = w${N-1};
          }
        $else:
          if XNN_UNPREDICTABLE(n < ${N+1}) {
            w${N} = w${N-1};
          }
      $if PREFETCH:
        $for N in range(0, NR):
          xnn_prefetch_to_l1((const int8_t*) w${N});
          xnn_prefetch_to_l1((const int8_t*) w${N} + 64);

      $if DATATYPE in ["QS8"]:
        $for N in range(0, NR, 8):
          __m256i vacc${N} = _mm256_setzero_si256();

      // KC main loop multiple of ${NR}x${KR}
      size_t k = kc;
      for (; k >= ${KR}; k -= ${KR}) {
        $for N in range(0, NR, 8):
          __m256i v${N} = _mm256_set1_epi32((int32_t) unaligned_load_u32(w${N}));
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi32((int32_t) unaligned_load_u32(w${N+1})), 0x02);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi32((int32_t) unaligned_load_u32(w${N+2})), 0x04);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi32((int32_t) unaligned_load_u32(w${N+3})), 0x08);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi32((int32_t) unaligned_load_u32(w${N+4})), 0x10);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi32((int32_t) unaligned_load_u32(w${N+5})), 0x20);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi32((int32_t) unaligned_load_u32(w${N+6})), 0x40);
          v${N} = _mm256_blend_epi32(v${N}, _mm256_set1_epi32((int32_t) unaligned_load_u32(w${N+7})), 0x80);
        $if PREFETCH:
          $for N in range(0, NR):
            xnn_prefetch_to_l1((const int8_t*) w${N} + 448);

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 8):
            vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});

        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        $for N in range(NR):
          w${N} += ${KR};
        out += ${NR*KR};
      }

      // KC remainder of 1..${KR-1}
      if (k != 0) {
        assert(k >= 1 && k <= ${KR-1});

        const __m256i vmask = _mm256_set1_epi32((1u << (k * sizeof(int8_t) * 8)) - 1);

        $for N in range(0, NR, 8):
          __m256i v${N} = _mm256_set1_epi32((int32_t) safe_load_u32(w${N}, k));
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+1}, k)), 0x02);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+2}, k)), 0x04);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+3}, k)), 0x08);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+4}, k)), 0x10);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+5}, k)), 0x20);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+6}, k)), 0x40);
          v${N} = _mm256_blend_epi32(v0, _mm256_set1_epi32((int32_t) safe_load_u32(w${N+7}, k)), 0x80);
          v${N} = _mm256_and_si256(v${N}, vmask);

        $for N in range(NR):
          w${N} += k;

        $if DATATYPE in ["QS8"]:
          $for N in range(0, NR, 8):
            vacc${N} = ${_MM256_DPBUSD_EPI32}(vacc${N}, vone, v${N});

        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i *)&out[${N * KR}],  v${N});

        out += ${NR*KR};
      }

      $if DATATYPE in ["QS8"]:
        $for N in range(0, NR, 8):
          __m256i vksum${N} = _mm256_mullo_epi32(vacc${N}, vzeropoint);
        $for N in range(0, NR, 8):
          __m256i vpack${N} =  _mm256_loadu_si256((const __m256i*) (packed_b + ${N}));
        $for N in range(0, NR, 8):
          vpack${N} = _mm256_sub_epi32(vpack${N}, vksum${N});
        $for N in range(0, NR, 8):
          _mm256_storeu_si256((__m256i *) (packed_b + ${N}), vpack${N});
      out = (${TYPE}*) ((uintptr_t) out + extra_bytes);
    }

    weights += nc * kc;
  } while (--g != 0);
}
